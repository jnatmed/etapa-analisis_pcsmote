{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "482b3476",
   "metadata": {},
   "source": [
    "# Evaluaci√≥n de PC-SMOTE con Grid Search en el dataset Shuttle (Generaci√≥n de caso base y datasets aumentados)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27267283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../scripts\")\n",
    "sys.path.append(\"../datasets\")\n",
    "\n",
    "import os\n",
    "\n",
    "# Rutas de datasets y resultados\n",
    "RUTA_DATASETS_BASE = \"../datasets/datasets_aumentados/base/\"\n",
    "RUTA_DATASETS_AUMENTADOS = \"../datasets/datasets_aumentados/\"\n",
    "RUTA_DATASETS_CLASICOS = \"../datasets/datasets_aumentados/resampler_clasicos/\"\n",
    "DIRECTORIO_SALIDA = \"../resultados\"\n",
    "\n",
    "os.makedirs(DIRECTORIO_SALIDA, exist_ok=True)\n",
    "os.makedirs(RUTA_DATASETS_CLASICOS, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "054fb149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, time  # gc: liberaci√≥n expl√≠cita de memoria entre ejecuciones; time: medici√≥n de duraci√≥n de b√∫squedas\n",
    "from dataclasses import dataclass, asdict  # dataclass: estructura limpia para registrar resultados y metadatos de cada combinaci√≥n\n",
    "\n",
    "import numpy as np  # operaciones num√©ricas y manipulaci√≥n de vectores/matrices\n",
    "import pandas as pd  # manejo de estructuras tabulares (dataframes) para consolidar resultados\n",
    "\n",
    "\n",
    "# Utilizamos validaci√≥n estratificada + b√∫squeda aleatoria de hiperpar√°metros\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "\n",
    "# M√©tricas utilizadas en CV y test (todas macro para evitar sesgos por clase mayoritaria)\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    balanced_accuracy_score,\n",
    "    recall_score,\n",
    "    make_scorer\n",
    ")\n",
    "\n",
    "# Cada modelo se ejecuta dentro de un Pipeline para permitir transformaciones futuras\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Modelo principal evaluado (Random Forest)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Suprimir warnings de convergencia innecesarios (SVM no se usa en esta fase)\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Controlar comportamiento en entornos con m√∫ltiples n√∫cleos\n",
    "# (evita paralelismo interno conflictivo con n_jobs de sklearn)\n",
    "import os\n",
    "\n",
    "# Estado aleatorio fijo para reproducibilidad entre ejecuciones\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# En Shuttle aumentado omitimos SVM por inestabilidad del ROC-AUC y l√≠mites computacionales\n",
    "OMITIR_SVM_EN_SHUTTLE_AUMENTADO = True\n",
    "\n",
    "# Archivo Excel consolidado con resultados CV y Test para todas las t√©cnicas\n",
    "NOMBRE_ARCHIVO_EXCEL = os.path.join(DIRECTORIO_SALIDA, \"resultados_RS_cv_vs_test.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "919cc6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Enumerando combinaciones base y aumentadas...\n",
      "üìÇ Explorando carpeta base: ../datasets/datasets_aumentados/base/\n",
      "#1  ‚úÖ Agregado base: ecoli_I0_tm268_train.csv combinado con ecoli_tdataset336_tm68_test.csv\n",
      "#2  ‚úÖ Agregado base: ecoli_I1_tm262_train.csv combinado con ecoli_tdataset336_tm68_test.csv\n",
      "#3  ‚úÖ Agregado base: ecoli_I3_tm258_train.csv combinado con ecoli_tdataset336_tm68_test.csv\n",
      "#4  ‚úÖ Agregado base: ecoli_I5_tm254_train.csv combinado con ecoli_tdataset336_tm68_test.csv\n",
      "  ‚ö™ Omitido (no es *_train.csv): ecoli_tdataset336_tm68_test.csv\n",
      "#5  ‚úÖ Agregado base: gear_vibration_I0_tm72_train.csv combinado con gear_vibration_tdataset90_tm18_test.csv\n",
      "#6  ‚úÖ Agregado base: gear_vibration_I1_tm68_train.csv combinado con gear_vibration_tdataset90_tm18_test.csv\n",
      "#7  ‚úÖ Agregado base: gear_vibration_I3_tm68_train.csv combinado con gear_vibration_tdataset90_tm18_test.csv\n",
      "#8  ‚úÖ Agregado base: gear_vibration_I5_tm67_train.csv combinado con gear_vibration_tdataset90_tm18_test.csv\n",
      "  ‚ö™ Omitido (no es *_train.csv): gear_vibration_tdataset90_tm18_test.csv\n",
      "#9  ‚úÖ Agregado base: glass_I0_tm171_train.csv combinado con glass_tdataset214_tm43_test.csv\n",
      "#10  ‚úÖ Agregado base: glass_I1_tm166_train.csv combinado con glass_tdataset214_tm43_test.csv\n",
      "#11  ‚úÖ Agregado base: glass_I3_tm164_train.csv combinado con glass_tdataset214_tm43_test.csv\n",
      "#12  ‚úÖ Agregado base: glass_I5_tm161_train.csv combinado con glass_tdataset214_tm43_test.csv\n",
      "  ‚ö™ Omitido (no es *_train.csv): glass_tdataset214_tm43_test.csv\n",
      "#13  ‚úÖ Agregado base: heart_I0_tm242_train.csv combinado con heart_tdataset303_tm61_test.csv\n",
      "#14  ‚úÖ Agregado base: heart_I1_tm236_train.csv combinado con heart_tdataset303_tm61_test.csv\n",
      "#15  ‚úÖ Agregado base: heart_I3_tm233_train.csv combinado con heart_tdataset303_tm61_test.csv\n",
      "#16  ‚úÖ Agregado base: heart_I5_tm227_train.csv combinado con heart_tdataset303_tm61_test.csv\n",
      "  ‚ö™ Omitido (no es *_train.csv): heart_tdataset303_tm61_test.csv\n",
      "#17  ‚úÖ Agregado base: predict_faults_I0_tm8000_train.csv combinado con predict_faults_tdataset10000_tm2000_test.csv\n",
      "#18  ‚úÖ Agregado base: predict_faults_I1_tm7917_train.csv combinado con predict_faults_tdataset10000_tm2000_test.csv\n",
      "#19  ‚úÖ Agregado base: predict_faults_I3_tm7757_train.csv combinado con predict_faults_tdataset10000_tm2000_test.csv\n",
      "#20  ‚úÖ Agregado base: predict_faults_I5_tm7597_train.csv combinado con predict_faults_tdataset10000_tm2000_test.csv\n",
      "  ‚ö™ Omitido (no es *_train.csv): predict_faults_tdataset10000_tm2000_test.csv\n",
      "#21  ‚úÖ Agregado base: telco_churn_I0_tm5634_train.csv combinado con telco_churn_tdataset7043_tm1409_test.csv\n",
      "#22  ‚úÖ Agregado base: telco_churn_I1_tm5577_train.csv combinado con telco_churn_tdataset7043_tm1409_test.csv\n",
      "#23  ‚úÖ Agregado base: telco_churn_I3_tm5464_train.csv combinado con telco_churn_tdataset7043_tm1409_test.csv\n",
      "#24  ‚úÖ Agregado base: telco_churn_I5_tm5352_train.csv combinado con telco_churn_tdataset7043_tm1409_test.csv\n",
      "  ‚ö™ Omitido (no es *_train.csv): telco_churn_tdataset7043_tm1409_test.csv\n",
      "#25  ‚úÖ Agregado base: us_crime_I0_tm1595_train.csv combinado con us_crime_tdataset1994_tm399_test.csv\n",
      "#26  ‚úÖ Agregado base: us_crime_I1_tm1578_train.csv combinado con us_crime_tdataset1994_tm399_test.csv\n",
      "#27  ‚úÖ Agregado base: us_crime_I3_tm1546_train.csv combinado con us_crime_tdataset1994_tm399_test.csv\n",
      "#28  ‚úÖ Agregado base: us_crime_I5_tm1515_train.csv combinado con us_crime_tdataset1994_tm399_test.csv\n",
      "  ‚ö™ Omitido (no es *_train.csv): us_crime_tdataset1994_tm399_test.csv\n",
      "#29  ‚úÖ Agregado base: wdbc_I0_tm455_train.csv combinado con wdbc_tdataset569_tm114_test.csv\n",
      "#30  ‚úÖ Agregado base: wdbc_I1_tm450_train.csv combinado con wdbc_tdataset569_tm114_test.csv\n",
      "#31  ‚úÖ Agregado base: wdbc_I3_tm440_train.csv combinado con wdbc_tdataset569_tm114_test.csv\n",
      "#32  ‚úÖ Agregado base: wdbc_I5_tm431_train.csv combinado con wdbc_tdataset569_tm114_test.csv\n",
      "  ‚ö™ Omitido (no es *_train.csv): wdbc_tdataset569_tm114_test.csv\n",
      "üìÇ Explorando carpeta cl√°sicos: ../datasets/datasets_aumentados/resampler_clasicos/\n",
      "#33  ‚úÖ Agregado cl√°sico: adasyn_gear_vibration_I0_sg75_train.csv combinado con gear_vibration_tdataset90_tm18_test.csv\n",
      "#34  ‚úÖ Agregado cl√°sico: adasyn_gear_vibration_I1_sg74_train.csv combinado con gear_vibration_tdataset90_tm18_test.csv\n",
      "#35  ‚úÖ Agregado cl√°sico: adasyn_gear_vibration_I3_sg74_train.csv combinado con gear_vibration_tdataset90_tm18_test.csv\n",
      "#36  ‚úÖ Agregado cl√°sico: adasyn_gear_vibration_I5_sg67_train.csv combinado con gear_vibration_tdataset90_tm18_test.csv\n",
      "#37  ‚úÖ Agregado cl√°sico: adasyn_heart_I0_sg418_train.csv combinado con heart_tdataset303_tm61_test.csv\n",
      "#38  ‚úÖ Agregado cl√°sico: adasyn_heart_I1_sg412_train.csv combinado con heart_tdataset303_tm61_test.csv\n",
      "#39  ‚úÖ Agregado cl√°sico: adasyn_heart_I3_sg400_train.csv combinado con heart_tdataset303_tm61_test.csv\n",
      "#40  ‚úÖ Agregado cl√°sico: adasyn_heart_I5_sg401_train.csv combinado con heart_tdataset303_tm61_test.csv\n",
      "#41  ‚úÖ Agregado cl√°sico: adasyn_predict_faults_I0_sg38311_train.csv combinado con predict_faults_tdataset10000_tm2000_test.csv\n",
      "#42  ‚úÖ Agregado cl√°sico: adasyn_predict_faults_I1_sg37956_train.csv combinado con predict_faults_tdataset10000_tm2000_test.csv\n",
      "#43  ‚úÖ Agregado cl√°sico: adasyn_predict_faults_I3_sg37182_train.csv combinado con predict_faults_tdataset10000_tm2000_test.csv\n",
      "#44  ‚úÖ Agregado cl√°sico: adasyn_predict_faults_I5_sg36399_train.csv combinado con predict_faults_tdataset10000_tm2000_test.csv\n",
      "#45  ‚úÖ Agregado cl√°sico: adasyn_telco_churn_I0_sg2594_train.csv combinado con telco_churn_tdataset7043_tm1409_test.csv\n",
      "#46  ‚úÖ Agregado cl√°sico: adasyn_telco_churn_I1_sg2535_train.csv combinado con telco_churn_tdataset7043_tm1409_test.csv\n",
      "#47  ‚úÖ Agregado cl√°sico: adasyn_telco_churn_I3_sg2424_train.csv combinado con telco_churn_tdataset7043_tm1409_test.csv\n",
      "#48  ‚úÖ Agregado cl√°sico: adasyn_telco_churn_I5_sg2660_train.csv combinado con telco_churn_tdataset7043_tm1409_test.csv\n",
      "#49  ‚úÖ Agregado cl√°sico: adasyn_us_crime_I0_sg1368_train.csv combinado con us_crime_tdataset1994_tm399_test.csv\n",
      "#50  ‚úÖ Agregado cl√°sico: adasyn_us_crime_I1_sg1342_train.csv combinado con us_crime_tdataset1994_tm399_test.csv\n",
      "#51  ‚úÖ Agregado cl√°sico: adasyn_us_crime_I3_sg1304_train.csv combinado con us_crime_tdataset1994_tm399_test.csv\n",
      "#52  ‚úÖ Agregado cl√°sico: adasyn_us_crime_I5_sg1266_train.csv combinado con us_crime_tdataset1994_tm399_test.csv\n",
      "#53  ‚úÖ Agregado cl√°sico: adasyn_wdbc_I0_sg121_train.csv combinado con wdbc_tdataset569_tm114_test.csv\n",
      "#54  ‚úÖ Agregado cl√°sico: adasyn_wdbc_I1_sg121_train.csv combinado con wdbc_tdataset569_tm114_test.csv\n",
      "#55  ‚úÖ Agregado cl√°sico: adasyn_wdbc_I3_sg115_train.csv combinado con wdbc_tdataset569_tm114_test.csv\n",
      "#56  ‚úÖ Agregado cl√°sico: adasyn_wdbc_I5_sg111_train.csv combinado con wdbc_tdataset569_tm114_test.csv\n",
      "#57  ‚úÖ Agregado cl√°sico: borderlinesmote_gear_vibration_I0_sg72_train.csv combinado con gear_vibration_tdataset90_tm18_test.csv\n",
      "#58  ‚úÖ Agregado cl√°sico: borderlinesmote_gear_vibration_I1_sg70_train.csv combinado con gear_vibration_tdataset90_tm18_test.csv\n",
      "#59  ‚úÖ Agregado cl√°sico: borderlinesmote_gear_vibration_I3_sg70_train.csv combinado con gear_vibration_tdataset90_tm18_test.csv\n",
      "#60  ‚úÖ Agregado cl√°sico: borderlinesmote_gear_vibration_I5_sg65_train.csv combinado con gear_vibration_tdataset90_tm18_test.csv\n",
      "#61  ‚úÖ Agregado cl√°sico: borderlinesmote_glass_I0_sg195_train.csv combinado con glass_tdataset214_tm43_test.csv\n",
      "#62  ‚úÖ Agregado cl√°sico: borderlinesmote_glass_I1_sg194_train.csv combinado con glass_tdataset214_tm43_test.csv\n",
      "#63  ‚úÖ Agregado cl√°sico: borderlinesmote_glass_I3_sg190_train.csv combinado con glass_tdataset214_tm43_test.csv\n",
      "#64  ‚úÖ Agregado cl√°sico: borderlinesmote_glass_I5_sg187_train.csv combinado con glass_tdataset214_tm43_test.csv\n",
      "#65  ‚úÖ Agregado cl√°sico: borderlinesmote_heart_I0_sg413_train.csv combinado con heart_tdataset303_tm61_test.csv\n",
      "#66  ‚úÖ Agregado cl√°sico: borderlinesmote_heart_I1_sg409_train.csv combinado con heart_tdataset303_tm61_test.csv\n",
      "#67  ‚úÖ Agregado cl√°sico: borderlinesmote_heart_I3_sg402_train.csv combinado con heart_tdataset303_tm61_test.csv\n",
      "#68  ‚úÖ Agregado cl√°sico: borderlinesmote_heart_I5_sg393_train.csv combinado con heart_tdataset303_tm61_test.csv\n",
      "#69  ‚úÖ Agregado cl√°sico: borderlinesmote_predict_faults_I0_sg38332_train.csv combinado con predict_faults_tdataset10000_tm2000_test.csv\n",
      "#70  ‚úÖ Agregado cl√°sico: borderlinesmote_predict_faults_I1_sg37947_train.csv combinado con predict_faults_tdataset10000_tm2000_test.csv\n",
      "#71  ‚úÖ Agregado cl√°sico: borderlinesmote_predict_faults_I3_sg37183_train.csv combinado con predict_faults_tdataset10000_tm2000_test.csv\n",
      "#72  ‚úÖ Agregado cl√°sico: borderlinesmote_predict_faults_I5_sg36413_train.csv combinado con predict_faults_tdataset10000_tm2000_test.csv\n",
      "#73  ‚úÖ Agregado cl√°sico: borderlinesmote_telco_churn_I0_sg2644_train.csv combinado con telco_churn_tdataset7043_tm1409_test.csv\n",
      "#74  ‚úÖ Agregado cl√°sico: borderlinesmote_telco_churn_I1_sg2617_train.csv combinado con telco_churn_tdataset7043_tm1409_test.csv\n",
      "#75  ‚úÖ Agregado cl√°sico: borderlinesmote_telco_churn_I3_sg2564_train.csv combinado con telco_churn_tdataset7043_tm1409_test.csv\n",
      "#76  ‚úÖ Agregado cl√°sico: borderlinesmote_telco_churn_I5_sg2512_train.csv combinado con telco_churn_tdataset7043_tm1409_test.csv\n",
      "#77  ‚úÖ Agregado cl√°sico: borderlinesmote_us_crime_I0_sg1355_train.csv combinado con us_crime_tdataset1994_tm399_test.csv\n",
      "#78  ‚úÖ Agregado cl√°sico: borderlinesmote_us_crime_I1_sg1342_train.csv combinado con us_crime_tdataset1994_tm399_test.csv\n",
      "#79  ‚úÖ Agregado cl√°sico: borderlinesmote_us_crime_I3_sg1314_train.csv combinado con us_crime_tdataset1994_tm399_test.csv\n",
      "#80  ‚úÖ Agregado cl√°sico: borderlinesmote_us_crime_I5_sg1287_train.csv combinado con us_crime_tdataset1994_tm399_test.csv\n",
      "#81  ‚úÖ Agregado cl√°sico: borderlinesmote_wdbc_I0_sg115_train.csv combinado con wdbc_tdataset569_tm114_test.csv\n",
      "#82  ‚úÖ Agregado cl√°sico: borderlinesmote_wdbc_I1_sg114_train.csv combinado con wdbc_tdataset569_tm114_test.csv\n",
      "#83  ‚úÖ Agregado cl√°sico: borderlinesmote_wdbc_I3_sg112_train.csv combinado con wdbc_tdataset569_tm114_test.csv\n",
      "#84  ‚úÖ Agregado cl√°sico: borderlinesmote_wdbc_I5_sg109_train.csv combinado con wdbc_tdataset569_tm114_test.csv\n",
      "#85  ‚úÖ Agregado cl√°sico: smote_gear_vibration_I0_sg72_train.csv combinado con gear_vibration_tdataset90_tm18_test.csv\n",
      "#86  ‚úÖ Agregado cl√°sico: smote_gear_vibration_I1_sg70_train.csv combinado con gear_vibration_tdataset90_tm18_test.csv\n",
      "#87  ‚úÖ Agregado cl√°sico: smote_gear_vibration_I3_sg70_train.csv combinado con gear_vibration_tdataset90_tm18_test.csv\n",
      "#88  ‚úÖ Agregado cl√°sico: smote_gear_vibration_I5_sg65_train.csv combinado con gear_vibration_tdataset90_tm18_test.csv\n",
      "#89  ‚úÖ Agregado cl√°sico: smote_glass_I0_sg195_train.csv combinado con glass_tdataset214_tm43_test.csv\n",
      "#90  ‚úÖ Agregado cl√°sico: smote_glass_I1_sg194_train.csv combinado con glass_tdataset214_tm43_test.csv\n",
      "#91  ‚úÖ Agregado cl√°sico: smote_glass_I3_sg190_train.csv combinado con glass_tdataset214_tm43_test.csv\n",
      "#92  ‚úÖ Agregado cl√°sico: smote_glass_I5_sg187_train.csv combinado con glass_tdataset214_tm43_test.csv\n",
      "#93  ‚úÖ Agregado cl√°sico: smote_heart_I0_sg413_train.csv combinado con heart_tdataset303_tm61_test.csv\n",
      "#94  ‚úÖ Agregado cl√°sico: smote_heart_I1_sg409_train.csv combinado con heart_tdataset303_tm61_test.csv\n",
      "#95  ‚úÖ Agregado cl√°sico: smote_heart_I3_sg402_train.csv combinado con heart_tdataset303_tm61_test.csv\n",
      "#96  ‚úÖ Agregado cl√°sico: smote_heart_I5_sg393_train.csv combinado con heart_tdataset303_tm61_test.csv\n",
      "#97  ‚úÖ Agregado cl√°sico: smote_predict_faults_I0_sg38332_train.csv combinado con predict_faults_tdataset10000_tm2000_test.csv\n",
      "#98  ‚úÖ Agregado cl√°sico: smote_predict_faults_I1_sg37947_train.csv combinado con predict_faults_tdataset10000_tm2000_test.csv\n",
      "#99  ‚úÖ Agregado cl√°sico: smote_predict_faults_I3_sg37183_train.csv combinado con predict_faults_tdataset10000_tm2000_test.csv\n",
      "#100  ‚úÖ Agregado cl√°sico: smote_predict_faults_I5_sg36413_train.csv combinado con predict_faults_tdataset10000_tm2000_test.csv\n",
      "#101  ‚úÖ Agregado cl√°sico: smote_telco_churn_I0_sg2644_train.csv combinado con telco_churn_tdataset7043_tm1409_test.csv\n",
      "#102  ‚úÖ Agregado cl√°sico: smote_telco_churn_I1_sg2617_train.csv combinado con telco_churn_tdataset7043_tm1409_test.csv\n",
      "#103  ‚úÖ Agregado cl√°sico: smote_telco_churn_I3_sg2564_train.csv combinado con telco_churn_tdataset7043_tm1409_test.csv\n",
      "#104  ‚úÖ Agregado cl√°sico: smote_telco_churn_I5_sg2512_train.csv combinado con telco_churn_tdataset7043_tm1409_test.csv\n",
      "#105  ‚úÖ Agregado cl√°sico: smote_us_crime_I0_sg1355_train.csv combinado con us_crime_tdataset1994_tm399_test.csv\n",
      "#106  ‚úÖ Agregado cl√°sico: smote_us_crime_I1_sg1342_train.csv combinado con us_crime_tdataset1994_tm399_test.csv\n",
      "#107  ‚úÖ Agregado cl√°sico: smote_us_crime_I3_sg1314_train.csv combinado con us_crime_tdataset1994_tm399_test.csv\n",
      "#108  ‚úÖ Agregado cl√°sico: smote_us_crime_I5_sg1287_train.csv combinado con us_crime_tdataset1994_tm399_test.csv\n",
      "#109  ‚úÖ Agregado cl√°sico: smote_wdbc_I0_sg115_train.csv combinado con wdbc_tdataset569_tm114_test.csv\n",
      "#110  ‚úÖ Agregado cl√°sico: smote_wdbc_I1_sg114_train.csv combinado con wdbc_tdataset569_tm114_test.csv\n",
      "#111  ‚úÖ Agregado cl√°sico: smote_wdbc_I3_sg112_train.csv combinado con wdbc_tdataset569_tm114_test.csv\n",
      "#112  ‚úÖ Agregado cl√°sico: smote_wdbc_I5_sg109_train.csv combinado con wdbc_tdataset569_tm114_test.csv\n",
      "üìÇ Explorando carpeta aumentados: ../datasets/datasets_aumentados/\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=ecoli, prd=35, pr=35, cp=ent, ud=080, ur=045, tipo_pureza=PE90, I=3, sv=8, sg=205\n",
      "#113  ‚úÖ Agregado pcsmote: pcs_ecoli_PRD35_PR35_CPent_UD080_PE90_UR045_I3_SV008_SG205_train.csv combinado con ecoli_tdataset336_tm68_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=ecoli, prd=35, pr=35, cp=prop, ud=080, ur=045, tipo_pureza=Upp080, I=3, sv=5, sg=122\n",
      "#114  ‚úÖ Agregado pcsmote: pcs_ecoli_PRD35_PR35_CPprop_UD080_Upp080_UR045_I3_SV005_SG122_train.csv combinado con ecoli_tdataset336_tm68_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=ecoli, prd=80, pr=40, cp=ent, ud=040, ur=045, tipo_pureza=PE40, I=0, sv=51, sg=310\n",
      "#115  ‚úÖ Agregado pcsmote: pcs_ecoli_PRD80_PR40_CPent_UD040_PE40_UR045_I0_SV051_SG310_train.csv combinado con ecoli_tdataset336_tm68_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=ecoli, prd=80, pr=40, cp=ent, ud=040, ur=045, tipo_pureza=PE40, I=1, sv=47, sg=306\n",
      "#116  ‚úÖ Agregado pcsmote: pcs_ecoli_PRD80_PR40_CPent_UD040_PE40_UR045_I1_SV047_SG306_train.csv combinado con ecoli_tdataset336_tm68_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=ecoli, prd=80, pr=40, cp=ent, ud=040, ur=045, tipo_pureza=PE90, I=0, sv=96, sg=310\n",
      "#117  ‚úÖ Agregado pcsmote: pcs_ecoli_PRD80_PR40_CPent_UD040_PE90_UR045_I0_SV096_SG310_train.csv combinado con ecoli_tdataset336_tm68_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=ecoli, prd=80, pr=40, cp=ent, ud=040, ur=045, tipo_pureza=PE90, I=5, sv=93, sg=294\n",
      "#118  ‚úÖ Agregado pcsmote: pcs_ecoli_PRD80_PR40_CPent_UD040_PE90_UR045_I5_SV093_SG294_train.csv combinado con ecoli_tdataset336_tm68_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=ecoli, prd=80, pr=40, cp=prop, ud=040, ur=045, tipo_pureza=Upp050, I=0, sv=96, sg=310\n",
      "#119  ‚úÖ Agregado pcsmote: pcs_ecoli_PRD80_PR40_CPprop_UD040_Upp050_UR045_I0_SV096_SG310_train.csv combinado con ecoli_tdataset336_tm68_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=ecoli, prd=80, pr=40, cp=prop, ud=040, ur=045, tipo_pureza=Upp050, I=1, sv=93, sg=306\n",
      "#120  ‚úÖ Agregado pcsmote: pcs_ecoli_PRD80_PR40_CPprop_UD040_Upp050_UR045_I1_SV093_SG306_train.csv combinado con ecoli_tdataset336_tm68_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=ecoli, prd=80, pr=40, cp=prop, ud=040, ur=045, tipo_pureza=Upp050, I=5, sv=93, sg=294\n",
      "#121  ‚úÖ Agregado pcsmote: pcs_ecoli_PRD80_PR40_CPprop_UD040_Upp050_UR045_I5_SV093_SG294_train.csv combinado con ecoli_tdataset336_tm68_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=ecoli, prd=95, pr=40, cp=ent, ud=030, ur=060, tipo_pureza=PE90, I=0, sv=105, sg=310\n",
      "#122  ‚úÖ Agregado pcsmote: pcs_ecoli_PRD95_PR40_CPent_UD030_PE90_UR060_I0_SV105_SG310_train.csv combinado con ecoli_tdataset336_tm68_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=ecoli, prd=95, pr=40, cp=prop, ud=030, ur=060, tipo_pureza=Upp050, I=0, sv=105, sg=310\n",
      "#123  ‚úÖ Agregado pcsmote: pcs_ecoli_PRD95_PR40_CPprop_UD030_Upp050_UR060_I0_SV105_SG310_train.csv combinado con ecoli_tdataset336_tm68_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=gear_vibration, prd=35, pr=35, cp=ent, ud=080, ur=045, tipo_pureza=PE90, I=3, sv=0, sg=0\n",
      "#124  ‚úÖ Agregado pcsmote: pcs_gear_vibration_PRD35_PR35_CPent_UD080_PE90_UR045_I3_SV000_SG000_train.csv combinado con gear_vibration_tdataset90_tm18_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=gear_vibration, prd=35, pr=35, cp=prop, ud=080, ur=045, tipo_pureza=Upp080, I=3, sv=0, sg=0\n",
      "#125  ‚úÖ Agregado pcsmote: pcs_gear_vibration_PRD35_PR35_CPprop_UD080_Upp080_UR045_I3_SV000_SG000_train.csv combinado con gear_vibration_tdataset90_tm18_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=gear_vibration, prd=80, pr=40, cp=ent, ud=040, ur=045, tipo_pureza=PE40, I=0, sv=0, sg=0\n",
      "#126  ‚úÖ Agregado pcsmote: pcs_gear_vibration_PRD80_PR40_CPent_UD040_PE40_UR045_I0_SV000_SG000_train.csv combinado con gear_vibration_tdataset90_tm18_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=gear_vibration, prd=80, pr=40, cp=ent, ud=040, ur=045, tipo_pureza=PE40, I=1, sv=0, sg=0\n",
      "#127  ‚úÖ Agregado pcsmote: pcs_gear_vibration_PRD80_PR40_CPent_UD040_PE40_UR045_I1_SV000_SG000_train.csv combinado con gear_vibration_tdataset90_tm18_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=gear_vibration, prd=80, pr=40, cp=ent, ud=040, ur=045, tipo_pureza=PE90, I=0, sv=2, sg=15\n",
      "#128  ‚úÖ Agregado pcsmote: pcs_gear_vibration_PRD80_PR40_CPent_UD040_PE90_UR045_I0_SV002_SG015_train.csv combinado con gear_vibration_tdataset90_tm18_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=gear_vibration, prd=80, pr=40, cp=ent, ud=040, ur=045, tipo_pureza=PE90, I=5, sv=4, sg=26\n",
      "#129  ‚úÖ Agregado pcsmote: pcs_gear_vibration_PRD80_PR40_CPent_UD040_PE90_UR045_I5_SV004_SG026_train.csv combinado con gear_vibration_tdataset90_tm18_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=gear_vibration, prd=80, pr=40, cp=prop, ud=040, ur=045, tipo_pureza=Upp050, I=0, sv=2, sg=15\n",
      "#130  ‚úÖ Agregado pcsmote: pcs_gear_vibration_PRD80_PR40_CPprop_UD040_Upp050_UR045_I0_SV002_SG015_train.csv combinado con gear_vibration_tdataset90_tm18_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=gear_vibration, prd=80, pr=40, cp=prop, ud=040, ur=045, tipo_pureza=Upp050, I=1, sv=3, sg=14\n",
      "#131  ‚úÖ Agregado pcsmote: pcs_gear_vibration_PRD80_PR40_CPprop_UD040_Upp050_UR045_I1_SV003_SG014_train.csv combinado con gear_vibration_tdataset90_tm18_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=gear_vibration, prd=80, pr=40, cp=prop, ud=040, ur=045, tipo_pureza=Upp050, I=5, sv=4, sg=26\n",
      "#132  ‚úÖ Agregado pcsmote: pcs_gear_vibration_PRD80_PR40_CPprop_UD040_Upp050_UR045_I5_SV004_SG026_train.csv combinado con gear_vibration_tdataset90_tm18_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=gear_vibration, prd=95, pr=40, cp=ent, ud=030, ur=060, tipo_pureza=PE90, I=0, sv=5, sg=30\n",
      "#133  ‚úÖ Agregado pcsmote: pcs_gear_vibration_PRD95_PR40_CPent_UD030_PE90_UR060_I0_SV005_SG030_train.csv combinado con gear_vibration_tdataset90_tm18_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=gear_vibration, prd=95, pr=40, cp=prop, ud=030, ur=060, tipo_pureza=Upp050, I=0, sv=5, sg=30\n",
      "#134  ‚úÖ Agregado pcsmote: pcs_gear_vibration_PRD95_PR40_CPprop_UD030_Upp050_UR060_I0_SV005_SG030_train.csv combinado con gear_vibration_tdataset90_tm18_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=glass, prd=35, pr=35, cp=ent, ud=080, ur=045, tipo_pureza=PE90, I=3, sv=11, sg=55\n",
      "#135  ‚úÖ Agregado pcsmote: pcs_glass_PRD35_PR35_CPent_UD080_PE90_UR045_I3_SV011_SG055_train.csv combinado con glass_tdataset214_tm43_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=glass, prd=35, pr=35, cp=prop, ud=080, ur=045, tipo_pureza=Upp080, I=3, sv=9, sg=5\n",
      "#136  ‚úÖ Agregado pcsmote: pcs_glass_PRD35_PR35_CPprop_UD080_Upp080_UR045_I3_SV009_SG005_train.csv combinado con glass_tdataset214_tm43_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=glass, prd=80, pr=40, cp=ent, ud=040, ur=045, tipo_pureza=PE40, I=0, sv=41, sg=94\n",
      "#137  ‚úÖ Agregado pcsmote: pcs_glass_PRD80_PR40_CPent_UD040_PE40_UR045_I0_SV041_SG094_train.csv combinado con glass_tdataset214_tm43_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=glass, prd=80, pr=40, cp=ent, ud=040, ur=045, tipo_pureza=PE40, I=1, sv=42, sg=94\n",
      "#138  ‚úÖ Agregado pcsmote: pcs_glass_PRD80_PR40_CPent_UD040_PE40_UR045_I1_SV042_SG094_train.csv combinado con glass_tdataset214_tm43_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=glass, prd=80, pr=40, cp=ent, ud=040, ur=045, tipo_pureza=PE90, I=0, sv=56, sg=94\n",
      "#139  ‚úÖ Agregado pcsmote: pcs_glass_PRD80_PR40_CPent_UD040_PE90_UR045_I0_SV056_SG094_train.csv combinado con glass_tdataset214_tm43_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=glass, prd=80, pr=40, cp=ent, ud=040, ur=045, tipo_pureza=PE90, I=5, sv=54, sg=91\n",
      "#140  ‚úÖ Agregado pcsmote: pcs_glass_PRD80_PR40_CPent_UD040_PE90_UR045_I5_SV054_SG091_train.csv combinado con glass_tdataset214_tm43_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=glass, prd=80, pr=40, cp=prop, ud=040, ur=045, tipo_pureza=Upp050, I=0, sv=57, sg=94\n",
      "#141  ‚úÖ Agregado pcsmote: pcs_glass_PRD80_PR40_CPprop_UD040_Upp050_UR045_I0_SV057_SG094_train.csv combinado con glass_tdataset214_tm43_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=glass, prd=80, pr=40, cp=prop, ud=040, ur=045, tipo_pureza=Upp050, I=1, sv=57, sg=94\n",
      "#142  ‚úÖ Agregado pcsmote: pcs_glass_PRD80_PR40_CPprop_UD040_Upp050_UR045_I1_SV057_SG094_train.csv combinado con glass_tdataset214_tm43_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=glass, prd=80, pr=40, cp=prop, ud=040, ur=045, tipo_pureza=Upp050, I=5, sv=55, sg=91\n",
      "#143  ‚úÖ Agregado pcsmote: pcs_glass_PRD80_PR40_CPprop_UD040_Upp050_UR045_I5_SV055_SG091_train.csv combinado con glass_tdataset214_tm43_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=glass, prd=95, pr=40, cp=ent, ud=030, ur=060, tipo_pureza=PE90, I=0, sv=63, sg=94\n",
      "#144  ‚úÖ Agregado pcsmote: pcs_glass_PRD95_PR40_CPent_UD030_PE90_UR060_I0_SV063_SG094_train.csv combinado con glass_tdataset214_tm43_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=glass, prd=95, pr=40, cp=prop, ud=030, ur=060, tipo_pureza=Upp050, I=0, sv=64, sg=94\n",
      "#145  ‚úÖ Agregado pcsmote: pcs_glass_PRD95_PR40_CPprop_UD030_Upp050_UR060_I0_SV064_SG094_train.csv combinado con glass_tdataset214_tm43_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=heart, prd=35, pr=35, cp=ent, ud=080, ur=045, tipo_pureza=PE90, I=3, sv=1, sg=85\n",
      "#146  ‚úÖ Agregado pcsmote: pcs_heart_PRD35_PR35_CPent_UD080_PE90_UR045_I3_SV001_SG085_train.csv combinado con heart_tdataset303_tm61_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=heart, prd=35, pr=35, cp=prop, ud=080, ur=045, tipo_pureza=Upp080, I=3, sv=0, sg=0\n",
      "#147  ‚úÖ Agregado pcsmote: pcs_heart_PRD35_PR35_CPprop_UD080_Upp080_UR045_I3_SV000_SG000_train.csv combinado con heart_tdataset303_tm61_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=heart, prd=80, pr=40, cp=ent, ud=040, ur=045, tipo_pureza=PE40, I=0, sv=1, sg=102\n",
      "#148  ‚úÖ Agregado pcsmote: pcs_heart_PRD80_PR40_CPent_UD040_PE40_UR045_I0_SV001_SG102_train.csv combinado con heart_tdataset303_tm61_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=heart, prd=80, pr=40, cp=ent, ud=040, ur=045, tipo_pureza=PE40, I=1, sv=0, sg=0\n",
      "#149  ‚úÖ Agregado pcsmote: pcs_heart_PRD80_PR40_CPent_UD040_PE40_UR045_I1_SV000_SG000_train.csv combinado con heart_tdataset303_tm61_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=heart, prd=80, pr=40, cp=ent, ud=040, ur=045, tipo_pureza=PE90, I=0, sv=5, sg=189\n",
      "#150  ‚úÖ Agregado pcsmote: pcs_heart_PRD80_PR40_CPent_UD040_PE90_UR045_I0_SV005_SG189_train.csv combinado con heart_tdataset303_tm61_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=heart, prd=80, pr=40, cp=ent, ud=040, ur=045, tipo_pureza=PE90, I=5, sv=4, sg=180\n",
      "#151  ‚úÖ Agregado pcsmote: pcs_heart_PRD80_PR40_CPent_UD040_PE90_UR045_I5_SV004_SG180_train.csv combinado con heart_tdataset303_tm61_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=heart, prd=80, pr=40, cp=prop, ud=040, ur=045, tipo_pureza=Upp050, I=0, sv=5, sg=189\n",
      "#152  ‚úÖ Agregado pcsmote: pcs_heart_PRD80_PR40_CPprop_UD040_Upp050_UR045_I0_SV005_SG189_train.csv combinado con heart_tdataset303_tm61_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=heart, prd=80, pr=40, cp=prop, ud=040, ur=045, tipo_pureza=Upp050, I=1, sv=5, sg=187\n",
      "#153  ‚úÖ Agregado pcsmote: pcs_heart_PRD80_PR40_CPprop_UD040_Upp050_UR045_I1_SV005_SG187_train.csv combinado con heart_tdataset303_tm61_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=heart, prd=80, pr=40, cp=prop, ud=040, ur=045, tipo_pureza=Upp050, I=5, sv=4, sg=180\n",
      "#154  ‚úÖ Agregado pcsmote: pcs_heart_PRD80_PR40_CPprop_UD040_Upp050_UR045_I5_SV004_SG180_train.csv combinado con heart_tdataset303_tm61_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=heart, prd=95, pr=40, cp=ent, ud=030, ur=060, tipo_pureza=PE90, I=0, sv=6, sg=189\n",
      "#155  ‚úÖ Agregado pcsmote: pcs_heart_PRD95_PR40_CPent_UD030_PE90_UR060_I0_SV006_SG189_train.csv combinado con heart_tdataset303_tm61_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=heart, prd=95, pr=40, cp=prop, ud=030, ur=060, tipo_pureza=Upp050, I=0, sv=6, sg=189\n",
      "#156  ‚úÖ Agregado pcsmote: pcs_heart_PRD95_PR40_CPprop_UD030_Upp050_UR060_I0_SV006_SG189_train.csv combinado con heart_tdataset303_tm61_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=predict_faults, prd=35, pr=35, cp=ent, ud=080, ur=045, tipo_pureza=PE90, I=3, sv=9, sg=22250\n",
      "#157  ‚úÖ Agregado pcsmote: pcs_predict_faults_PRD35_PR35_CPent_UD080_PE90_UR045_I3_SV009_SG22250_train.csv combinado con predict_faults_tdataset10000_tm2000_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=predict_faults, prd=35, pr=35, cp=prop, ud=080, ur=045, tipo_pureza=Upp080, I=3, sv=0, sg=0\n",
      "#158  ‚úÖ Agregado pcsmote: pcs_predict_faults_PRD35_PR35_CPprop_UD080_Upp080_UR045_I3_SV000_SG000_train.csv combinado con predict_faults_tdataset10000_tm2000_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=predict_faults, prd=80, pr=40, cp=ent, ud=040, ur=045, tipo_pureza=PE40, I=0, sv=28, sg=22938\n",
      "#159  ‚úÖ Agregado pcsmote: pcs_predict_faults_PRD80_PR40_CPent_UD040_PE40_UR045_I0_SV028_SG22938_train.csv combinado con predict_faults_tdataset10000_tm2000_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=predict_faults, prd=80, pr=40, cp=ent, ud=040, ur=045, tipo_pureza=PE40, I=1, sv=30, sg=22707\n",
      "#160  ‚úÖ Agregado pcsmote: pcs_predict_faults_PRD80_PR40_CPent_UD040_PE40_UR045_I1_SV030_SG22707_train.csv combinado con predict_faults_tdataset10000_tm2000_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=predict_faults, prd=80, pr=40, cp=ent, ud=040, ur=045, tipo_pureza=PE90, I=0, sv=53, sg=22938\n",
      "#161  ‚úÖ Agregado pcsmote: pcs_predict_faults_PRD80_PR40_CPent_UD040_PE90_UR045_I0_SV053_SG22938_train.csv combinado con predict_faults_tdataset10000_tm2000_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=predict_faults, prd=80, pr=40, cp=ent, ud=040, ur=045, tipo_pureza=PE90, I=5, sv=57, sg=21790\n",
      "#162  ‚úÖ Agregado pcsmote: pcs_predict_faults_PRD80_PR40_CPent_UD040_PE90_UR045_I5_SV057_SG21790_train.csv combinado con predict_faults_tdataset10000_tm2000_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=predict_faults, prd=80, pr=40, cp=prop, ud=040, ur=045, tipo_pureza=Upp050, I=0, sv=53, sg=22938\n",
      "#163  ‚úÖ Agregado pcsmote: pcs_predict_faults_PRD80_PR40_CPprop_UD040_Upp050_UR045_I0_SV053_SG22938_train.csv combinado con predict_faults_tdataset10000_tm2000_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=predict_faults, prd=80, pr=40, cp=prop, ud=040, ur=045, tipo_pureza=Upp050, I=1, sv=57, sg=22707\n",
      "#164  ‚úÖ Agregado pcsmote: pcs_predict_faults_PRD80_PR40_CPprop_UD040_Upp050_UR045_I1_SV057_SG22707_train.csv combinado con predict_faults_tdataset10000_tm2000_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=predict_faults, prd=80, pr=40, cp=prop, ud=040, ur=045, tipo_pureza=Upp050, I=5, sv=57, sg=21790\n",
      "#165  ‚úÖ Agregado pcsmote: pcs_predict_faults_PRD80_PR40_CPprop_UD040_Upp050_UR045_I5_SV057_SG21790_train.csv combinado con predict_faults_tdataset10000_tm2000_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=predict_faults, prd=95, pr=40, cp=ent, ud=030, ur=060, tipo_pureza=PE90, I=0, sv=63, sg=22938\n",
      "#166  ‚úÖ Agregado pcsmote: pcs_predict_faults_PRD95_PR40_CPent_UD030_PE90_UR060_I0_SV063_SG22938_train.csv combinado con predict_faults_tdataset10000_tm2000_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=predict_faults, prd=95, pr=40, cp=prop, ud=030, ur=060, tipo_pureza=Upp050, I=0, sv=63, sg=22938\n",
      "#167  ‚úÖ Agregado pcsmote: pcs_predict_faults_PRD95_PR40_CPprop_UD030_Upp050_UR060_I0_SV063_SG22938_train.csv combinado con predict_faults_tdataset10000_tm2000_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=telco_churn, prd=35, pr=35, cp=prop, ud=080, ur=045, tipo_pureza=Upp080, I=3, sv=94, sg=2564\n",
      "#168  ‚úÖ Agregado pcsmote: pcs_telco_churn_PRD35_PR35_CPprop_UD080_Upp080_UR045_I3_SV094_SG2564_train.csv combinado con telco_churn_tdataset7043_tm1409_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=telco_churn, prd=80, pr=40, cp=ent, ud=040, ur=045, tipo_pureza=PE40, I=0, sv=498, sg=2644\n",
      "#169  ‚úÖ Agregado pcsmote: pcs_telco_churn_PRD80_PR40_CPent_UD040_PE40_UR045_I0_SV498_SG2644_train.csv combinado con telco_churn_tdataset7043_tm1409_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=telco_churn, prd=80, pr=40, cp=ent, ud=040, ur=045, tipo_pureza=PE40, I=1, sv=498, sg=2617\n",
      "#170  ‚úÖ Agregado pcsmote: pcs_telco_churn_PRD80_PR40_CPent_UD040_PE40_UR045_I1_SV498_SG2617_train.csv combinado con telco_churn_tdataset7043_tm1409_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=telco_churn, prd=80, pr=40, cp=ent, ud=040, ur=045, tipo_pureza=PE90, I=0, sv=748, sg=2644\n",
      "#171  ‚úÖ Agregado pcsmote: pcs_telco_churn_PRD80_PR40_CPent_UD040_PE90_UR045_I0_SV748_SG2644_train.csv combinado con telco_churn_tdataset7043_tm1409_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=telco_churn, prd=80, pr=40, cp=ent, ud=040, ur=045, tipo_pureza=PE90, I=5, sv=747, sg=2512\n",
      "#172  ‚úÖ Agregado pcsmote: pcs_telco_churn_PRD80_PR40_CPent_UD040_PE90_UR045_I5_SV747_SG2512_train.csv combinado con telco_churn_tdataset7043_tm1409_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=telco_churn, prd=95, pr=40, cp=ent, ud=030, ur=060, tipo_pureza=PE90, I=0, sv=793, sg=2644\n",
      "#173  ‚úÖ Agregado pcsmote: pcs_telco_churn_PRD95_PR40_CPent_UD030_PE90_UR060_I0_SV793_SG2644_train.csv combinado con telco_churn_tdataset7043_tm1409_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=us_crime, prd=35, pr=35, cp=ent, ud=080, ur=045, tipo_pureza=PE90, I=3, sv=0, sg=0\n",
      "#174  ‚úÖ Agregado pcsmote: pcs_us_crime_PRD35_PR35_CPent_UD080_PE90_UR045_I3_SV000_SG000_train.csv combinado con us_crime_tdataset1994_tm399_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=us_crime, prd=35, pr=35, cp=prop, ud=080, ur=045, tipo_pureza=Upp080, I=3, sv=0, sg=0\n",
      "#175  ‚úÖ Agregado pcsmote: pcs_us_crime_PRD35_PR35_CPprop_UD080_Upp080_UR045_I3_SV000_SG000_train.csv combinado con us_crime_tdataset1994_tm399_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=us_crime, prd=80, pr=40, cp=ent, ud=040, ur=045, tipo_pureza=PE40, I=0, sv=0, sg=0\n",
      "#176  ‚úÖ Agregado pcsmote: pcs_us_crime_PRD80_PR40_CPent_UD040_PE40_UR045_I0_SV000_SG000_train.csv combinado con us_crime_tdataset1994_tm399_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=us_crime, prd=80, pr=40, cp=ent, ud=040, ur=045, tipo_pureza=PE40, I=1, sv=0, sg=0\n",
      "#177  ‚úÖ Agregado pcsmote: pcs_us_crime_PRD80_PR40_CPent_UD040_PE40_UR045_I1_SV000_SG000_train.csv combinado con us_crime_tdataset1994_tm399_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=us_crime, prd=80, pr=40, cp=ent, ud=040, ur=045, tipo_pureza=PE90, I=0, sv=9, sg=1355\n",
      "#178  ‚úÖ Agregado pcsmote: pcs_us_crime_PRD80_PR40_CPent_UD040_PE90_UR045_I0_SV009_SG1355_train.csv combinado con us_crime_tdataset1994_tm399_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=us_crime, prd=80, pr=40, cp=ent, ud=040, ur=045, tipo_pureza=PE90, I=5, sv=12, sg=1287\n",
      "#179  ‚úÖ Agregado pcsmote: pcs_us_crime_PRD80_PR40_CPent_UD040_PE90_UR045_I5_SV012_SG1287_train.csv combinado con us_crime_tdataset1994_tm399_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=us_crime, prd=80, pr=40, cp=prop, ud=040, ur=045, tipo_pureza=Upp050, I=0, sv=9, sg=1355\n",
      "#180  ‚úÖ Agregado pcsmote: pcs_us_crime_PRD80_PR40_CPprop_UD040_Upp050_UR045_I0_SV009_SG1355_train.csv combinado con us_crime_tdataset1994_tm399_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=us_crime, prd=80, pr=40, cp=prop, ud=040, ur=045, tipo_pureza=Upp050, I=1, sv=10, sg=1342\n",
      "#181  ‚úÖ Agregado pcsmote: pcs_us_crime_PRD80_PR40_CPprop_UD040_Upp050_UR045_I1_SV010_SG1342_train.csv combinado con us_crime_tdataset1994_tm399_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=us_crime, prd=80, pr=40, cp=prop, ud=040, ur=045, tipo_pureza=Upp050, I=5, sv=12, sg=1287\n",
      "#182  ‚úÖ Agregado pcsmote: pcs_us_crime_PRD80_PR40_CPprop_UD040_Upp050_UR045_I5_SV012_SG1287_train.csv combinado con us_crime_tdataset1994_tm399_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=us_crime, prd=95, pr=40, cp=ent, ud=030, ur=060, tipo_pureza=PE90, I=0, sv=21, sg=1355\n",
      "#183  ‚úÖ Agregado pcsmote: pcs_us_crime_PRD95_PR40_CPent_UD030_PE90_UR060_I0_SV021_SG1355_train.csv combinado con us_crime_tdataset1994_tm399_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=us_crime, prd=95, pr=40, cp=prop, ud=030, ur=060, tipo_pureza=Upp050, I=0, sv=21, sg=1355\n",
      "#184  ‚úÖ Agregado pcsmote: pcs_us_crime_PRD95_PR40_CPprop_UD030_Upp050_UR060_I0_SV021_SG1355_train.csv combinado con us_crime_tdataset1994_tm399_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=wdbc, prd=35, pr=35, cp=ent, ud=080, ur=045, tipo_pureza=PE90, I=3, sv=33, sg=112\n",
      "#185  ‚úÖ Agregado pcsmote: pcs_wdbc_PRD35_PR35_CPent_UD080_PE90_UR045_I3_SV033_SG112_train.csv combinado con wdbc_tdataset569_tm114_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=wdbc, prd=35, pr=35, cp=prop, ud=080, ur=045, tipo_pureza=Upp080, I=3, sv=28, sg=112\n",
      "#186  ‚úÖ Agregado pcsmote: pcs_wdbc_PRD35_PR35_CPprop_UD080_Upp080_UR045_I3_SV028_SG112_train.csv combinado con wdbc_tdataset569_tm114_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=wdbc, prd=80, pr=40, cp=ent, ud=040, ur=045, tipo_pureza=PE40, I=0, sv=101, sg=115\n",
      "#187  ‚úÖ Agregado pcsmote: pcs_wdbc_PRD80_PR40_CPent_UD040_PE40_UR045_I0_SV101_SG115_train.csv combinado con wdbc_tdataset569_tm114_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=wdbc, prd=80, pr=40, cp=ent, ud=040, ur=045, tipo_pureza=PE40, I=1, sv=100, sg=114\n",
      "#188  ‚úÖ Agregado pcsmote: pcs_wdbc_PRD80_PR40_CPent_UD040_PE40_UR045_I1_SV100_SG114_train.csv combinado con wdbc_tdataset569_tm114_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=wdbc, prd=80, pr=40, cp=ent, ud=040, ur=045, tipo_pureza=PE90, I=0, sv=119, sg=115\n",
      "#189  ‚úÖ Agregado pcsmote: pcs_wdbc_PRD80_PR40_CPent_UD040_PE90_UR045_I0_SV119_SG115_train.csv combinado con wdbc_tdataset569_tm114_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=wdbc, prd=80, pr=40, cp=ent, ud=040, ur=045, tipo_pureza=PE90, I=5, sv=113, sg=109\n",
      "#190  ‚úÖ Agregado pcsmote: pcs_wdbc_PRD80_PR40_CPent_UD040_PE90_UR045_I5_SV113_SG109_train.csv combinado con wdbc_tdataset569_tm114_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=wdbc, prd=80, pr=40, cp=prop, ud=040, ur=045, tipo_pureza=Upp050, I=0, sv=127, sg=115\n",
      "#191  ‚úÖ Agregado pcsmote: pcs_wdbc_PRD80_PR40_CPprop_UD040_Upp050_UR045_I0_SV127_SG115_train.csv combinado con wdbc_tdataset569_tm114_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=wdbc, prd=80, pr=40, cp=prop, ud=040, ur=045, tipo_pureza=Upp050, I=1, sv=126, sg=114\n",
      "#192  ‚úÖ Agregado pcsmote: pcs_wdbc_PRD80_PR40_CPprop_UD040_Upp050_UR045_I1_SV126_SG114_train.csv combinado con wdbc_tdataset569_tm114_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=wdbc, prd=80, pr=40, cp=prop, ud=040, ur=045, tipo_pureza=Upp050, I=5, sv=121, sg=109\n",
      "#193  ‚úÖ Agregado pcsmote: pcs_wdbc_PRD80_PR40_CPprop_UD040_Upp050_UR045_I5_SV121_SG109_train.csv combinado con wdbc_tdataset569_tm114_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=wdbc, prd=95, pr=40, cp=ent, ud=030, ur=060, tipo_pureza=PE90, I=0, sv=143, sg=115\n",
      "#194  ‚úÖ Agregado pcsmote: pcs_wdbc_PRD95_PR40_CPent_UD030_PE90_UR060_I0_SV143_SG115_train.csv combinado con wdbc_tdataset569_tm114_test.csv\n",
      "  ‚û°Ô∏è  Descifrado pcsmote: dataset=wdbc, prd=95, pr=40, cp=prop, ud=030, ur=060, tipo_pureza=Upp050, I=0, sv=152, sg=115\n",
      "#195  ‚úÖ Agregado pcsmote: pcs_wdbc_PRD95_PR40_CPprop_UD030_Upp050_UR060_I0_SV152_SG115_train.csv combinado con wdbc_tdataset569_tm114_test.csv\n",
      "üìä Total combinaciones descubiertas: 195\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# =========================\n",
    "# Estructuras de datos\n",
    "# =========================\n",
    "@dataclass\n",
    "class DatasetCombination:\n",
    "    dataset_logico: str\n",
    "    tipo_combination: str      # \"base\" | \"clasico\" | \"pcsmote\"\n",
    "    ruta_train_csv: str\n",
    "    ruta_test_csv: str\n",
    "    tecnica_aumento: str = \"base\"\n",
    "    valor_densidad: str = \"--\"\n",
    "    valor_riesgo: str = \"--\"\n",
    "\n",
    "    percentil_radio_distancia: str = \"--\",\n",
    "    percentil_riesgo: str = \"--\"\n",
    "    criterio_pureza: str = \"--\"\n",
    "    umbral_densidad: str = \"--\"\n",
    "    umbral_riesgo: str = \"--\"\n",
    "    tipo_pureza: str = \"--\"\n",
    "\n",
    "    grado_limpieza: str = \"--\"  # I0, I1, I5, etc.\n",
    "    total_muestras_train: int | None = None\n",
    "    tamanio_dataset: int | None = None  # tama√±o total del dataset (train + test)\n",
    "    sinteticos_generados: int = 0\n",
    "    semillas_validas: int = 0\n",
    "    \n",
    "    tipo_pureza: str = \"--\"           # PE.. o Ppp.. del nombre de archivo\n",
    "    nombre_configuracion: str = \"\"    # PRDxx_PRxx_CPxx_UDxxx_PE.._I.._SV.._SG..    \n",
    "\n",
    "@dataclass\n",
    "class RegistroRendimiento:\n",
    "    dataset_logico: str\n",
    "    tipo_combination: str\n",
    "    nombre_modelo_aprendizaje: str\n",
    "    tecnica_aumento: str\n",
    "    valor_densidad: str\n",
    "    valor_riesgo: str\n",
    "    criterio_pureza: str\n",
    "    grado_limpieza: str\n",
    "\n",
    "    cantidad_train: int\n",
    "    cantidad_test: int\n",
    "    cantidad_caracteristicas: int\n",
    "\n",
    "    # M√©tricas CV\n",
    "    cv_f1_macro: float\n",
    "    cv_balanced_accuracy: float\n",
    "    cv_recall_macro: float    \n",
    "\n",
    "    # M√©tricas Test\n",
    "    test_f1_macro: float\n",
    "    test_balanced_accuracy: float\n",
    "    test_recall_macro: float  \n",
    "\n",
    "    mejores_hiperparametros: str\n",
    "    tiempo_busqueda_seg: float\n",
    "\n",
    "\n",
    "\n",
    "def enumerar_combinaciones_base_y_aumentadas(\n",
    "    ruta_base,\n",
    "    ruta_clasicos,\n",
    "    ruta_aumentados,\n",
    "    verbose=True\n",
    "):\n",
    "    combinaciones = []\n",
    "    cont_combinaciones = 0\n",
    "\n",
    "    # Mapear (dataset_logico, grado_limpieza) ‚Üí tama√±o_train_base\n",
    "    tamanio_train_base_por_dataset_y_I = {}\n",
    "\n",
    "    # ==========================================================\n",
    "    # 1) BASE\n",
    "    #    train: {dataset}_I{I}_tm{n}_train.csv\n",
    "    #    test : {dataset}_tm{n}_test.csv\n",
    "    # ==========================================================\n",
    "    if verbose:\n",
    "        print(f\"üìÇ Explorando carpeta base: {ruta_base}\")\n",
    "\n",
    "    archivos_base = os.listdir(ruta_base)\n",
    "\n",
    "    for nombre in archivos_base:\n",
    "        if not nombre.endswith(\"_train.csv\"):\n",
    "            if verbose:\n",
    "                print(f\"  ‚ö™ Omitido (no es *_train.csv): {nombre}\")\n",
    "            continue\n",
    "\n",
    "        m = re.match(r\"(.+?)_I(\\d+)_tm(\\d+)_train\\.csv$\", nombre)\n",
    "        if not m:\n",
    "            if verbose:\n",
    "                print(f\"  ‚ö™ No coincide patr√≥n base con I*_tm*_train: {nombre}\")\n",
    "            continue\n",
    "\n",
    "        dataset_logico = m.group(1)\n",
    "        grado_limpieza = int(m.group(2))\n",
    "        total_muestras_train = int(m.group(3))\n",
    "\n",
    "        # Registrar tama√±o de train base para este (dataset, I)\n",
    "        clave_base = (dataset_logico, grado_limpieza)\n",
    "        tamanio_train_base_por_dataset_y_I[clave_base] = total_muestras_train\n",
    "\n",
    "        ruta_train_csv = os.path.join(ruta_base, nombre)\n",
    "\n",
    "        # Buscar test correspondiente: {dataset}_tm{n}_test.csv\n",
    "        patron_test = re.compile(rf\"^{re.escape(dataset_logico)}_tdataset(\\d+)_tm(\\d+)_test\\.csv$\")\n",
    "        nombre_test = None\n",
    "        n_test_detectado = None\n",
    "\n",
    "        for nombre_candidato in archivos_base:\n",
    "            m_test = patron_test.match(nombre_candidato)\n",
    "            if m_test:\n",
    "                # Si hubiera m√°s de uno, nos quedamos con el de mayor tm\n",
    "                n_tm = int(m_test.group(2))\n",
    "                tamanio_dataset=int(m_test.group(1))\n",
    "\n",
    "                if n_test_detectado is None or n_tm > n_test_detectado:\n",
    "                    n_test_detectado = n_tm\n",
    "                    nombre_test = nombre_candidato\n",
    "\n",
    "        if nombre_test is None:\n",
    "            if verbose:\n",
    "                print(f\"  ‚ö†Ô∏è  Falta test para dataset base '{dataset_logico}', se omite {nombre}\")\n",
    "            continue\n",
    "\n",
    "        ruta_test_csv = os.path.join(ruta_base, nombre_test)\n",
    "\n",
    "        cont_combinaciones += 1\n",
    "        print(f\"#{cont_combinaciones}  ‚úÖ Agregado base: {nombre} combinado con {nombre_test}\")\n",
    "\n",
    "        combinaciones.append(DatasetCombination(\n",
    "            dataset_logico=dataset_logico,\n",
    "            tipo_combination=\"base\",\n",
    "            ruta_train_csv=ruta_train_csv,\n",
    "            ruta_test_csv=ruta_test_csv,\n",
    "            tecnica_aumento=\"base\",\n",
    "            valor_densidad=None,\n",
    "            valor_riesgo=None,\n",
    "            criterio_pureza=None,\n",
    "            grado_limpieza=grado_limpieza,\n",
    "            total_muestras_train=total_muestras_train,\n",
    "            tamanio_dataset=tamanio_dataset\n",
    "        ))\n",
    "\n",
    "    # ==========================================================\n",
    "    # 2) CL√ÅSICOS\n",
    "    #    {tecnica}_{dataset}_I{I}_sg{sg}_train.csv\n",
    "    #    test base: {dataset}_tm{n}_test.csv (mismo criterio que base)\n",
    "    # ==========================================================\n",
    "    if verbose:\n",
    "        print(f\"üìÇ Explorando carpeta cl√°sicos: {ruta_clasicos}\")\n",
    "\n",
    "    archivos_clasicos = os.listdir(ruta_clasicos)\n",
    "\n",
    "    for nombre in archivos_clasicos:\n",
    "        if not nombre.endswith(\"_train.csv\"):\n",
    "            continue\n",
    "\n",
    "        # ejemplo: adasyn_us_crime_I1_sg120_train.csv\n",
    "        m = re.match(r\"(.+?)_(.+?)_I(\\d+)_sg(\\d+)_train\\.csv$\", nombre)\n",
    "        if not m:\n",
    "            if verbose:\n",
    "                print(f\"  ‚ö†Ô∏è  No cumple patr√≥n cl√°sicos: {nombre}\")\n",
    "            continue\n",
    "\n",
    "        tecnica = m.group(1)\n",
    "        dataset_logico = m.group(2)\n",
    "        grado_limpieza = int(m.group(3))\n",
    "        sinteticos_generados = int(m.group(4))\n",
    "\n",
    "        # Recuperar tama√±o de train base para este dataset y este I\n",
    "        clave_base = (dataset_logico, grado_limpieza)\n",
    "        total_muestras_train = tamanio_train_base_por_dataset_y_I.get(clave_base)\n",
    "\n",
    "        if total_muestras_train is None:\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"  ‚ö†Ô∏è  No se encontr√≥ tama√±o de train base para \"\n",
    "                    f\"(dataset='{dataset_logico}', I={grado_limpieza}). Se omite {nombre}\"\n",
    "                )\n",
    "            continue\n",
    "        \n",
    "        ruta_train_csv = os.path.join(ruta_clasicos, nombre)\n",
    "\n",
    "        # Buscar test correspondiente en carpeta base\n",
    "        patron_test = re.compile(rf\"^{re.escape(dataset_logico)}_tdataset(\\d+)_tm(\\d+)_test\\.csv$\")\n",
    "        nombre_test = None\n",
    "        n_test_detectado = None\n",
    "\n",
    "        for nombre_candidato in archivos_base:\n",
    "            m_test = patron_test.match(nombre_candidato)\n",
    "            if m_test:\n",
    "                n_tm = int(m_test.group(2))\n",
    "                tamanio_dataset = int(m_test.group(1))\n",
    "                if n_test_detectado is None or n_tm > n_test_detectado:\n",
    "                    n_test_detectado = n_tm\n",
    "                    nombre_test = nombre_candidato\n",
    "\n",
    "        if nombre_test is None:\n",
    "            if verbose:\n",
    "                print(f\"  ‚ö†Ô∏è  No hay test base para dataset '{dataset_logico}', se omite {nombre}\")\n",
    "            continue\n",
    "\n",
    "        ruta_test_csv = os.path.join(ruta_base, nombre_test)\n",
    "\n",
    "        cont_combinaciones += 1\n",
    "        print(f\"#{cont_combinaciones}  ‚úÖ Agregado cl√°sico: {nombre} combinado con {nombre_test}\")\n",
    "\n",
    "        combinaciones.append(DatasetCombination(\n",
    "            dataset_logico=dataset_logico,\n",
    "            tipo_combination=\"clasico\",\n",
    "            ruta_train_csv=ruta_train_csv,\n",
    "            ruta_test_csv=ruta_test_csv,\n",
    "            tecnica_aumento=tecnica.lower(),\n",
    "            valor_densidad=None,\n",
    "            valor_riesgo=None,\n",
    "            criterio_pureza=None,\n",
    "            grado_limpieza=grado_limpieza,\n",
    "            total_muestras_train=total_muestras_train,\n",
    "            sinteticos_generados=sinteticos_generados,\n",
    "            tamanio_dataset=tamanio_dataset\n",
    "        ))\n",
    "\n",
    "    # ==========================================================\n",
    "    # 3) PC-SMOTE (nuevo patr√≥n)\n",
    "    #\n",
    "    # pcs_{dataset}_PRD{prd}_PR{pr}_CP{ent|prop}_UD{ud3}_{PE..|Ppp..}_I{iso}_SG{sg}_train.csv\n",
    "    #\n",
    "    # Ej:\n",
    "    #   pcs_ecoli_PRD35_PR35_CPent_UD080_PE45_I0_SG120_train.csv\n",
    "    #   pcs_ecoli_PRD35_PR35_CPprop_UD080_Ppp041_I0_SG007_train.csv\n",
    "    #\n",
    "    # valor_densidad  ‚Üí percentil radio distancia (PRD)\n",
    "    # valor_riesgo    ‚Üí percentil riesgo (PR)\n",
    "    # criterio_pureza ‚Üí \"entropia\" / \"proporcion\"\n",
    "    # grado_limpieza  ‚Üí iso (I*)\n",
    "    # sinteticos_generados ‚Üí SG\n",
    "    # ==========================================================\n",
    "    if verbose:\n",
    "        print(f\"üìÇ Explorando carpeta aumentados: {ruta_aumentados}\")\n",
    "\n",
    "    archivos_aumentados = os.listdir(ruta_aumentados)\n",
    "\n",
    "    patron_pcsmote = re.compile(\n",
    "        r\"^pcs_(?P<dataset>.+?)_\"\n",
    "        r\"PRD(?P<prd>\\d+)_\"\n",
    "        r\"PR(?P<pr>\\d+)_\"\n",
    "        r\"CP(?P<cp>(?:ent|prop))_\"\n",
    "        r\"UD(?P<ud>\\d{3})_\"\n",
    "        r\"(?P<tipo_pureza>(?:PE\\d+|Upp\\d{3}))_\"\n",
    "        r\"UR(?P<ur>\\d{3})_\"\n",
    "        r\"I(?P<iso>\\d+)_\"\n",
    "        r\"SV(?P<sv>\\d+)_\"\n",
    "        r\"SG(?P<sg>\\d+)_train\\.csv$\"\n",
    "    )\n",
    "\n",
    "    for nombre in archivos_aumentados:\n",
    "        if not nombre.endswith(\"_train.csv\"):\n",
    "            continue\n",
    "\n",
    "        m = patron_pcsmote.match(nombre)\n",
    "        if not m:\n",
    "            if verbose:\n",
    "                print(f\"  ‚ö™ Omitido (no es pcs v√°lido): {nombre}\")\n",
    "            continue\n",
    "\n",
    "        dataset_logico = m.group(\"dataset\")\n",
    "        valor_densidad = int(m.group(\"prd\"))   # percentil radio distancia\n",
    "        valor_riesgo   = int(m.group(\"pr\"))    # percentil riesgo\n",
    "        cp_code        = m.group(\"cp\")         # \"ent\" | \"prop\"\n",
    "        ud_str       = m.group(\"ud\")        # umbral densidad en %, si despu√©s lo quer√©s usar\n",
    "        ur_str       = m.group(\"ur\")        # umbral densidad en %, si despu√©s lo quer√©s usar\n",
    "        tipo_pureza = m.group(\"tipo_pureza\")  # PE.. / Ppp.., si lo necesit√°s luego\n",
    "        grado_limpieza = int(m.group(\"iso\"))   # I*\n",
    "        semillas_validas = int(m.group(\"sv\"))\n",
    "        sinteticos_generados = int(m.group(\"sg\"))\n",
    "\n",
    "        print(f\"  ‚û°Ô∏è  Descifrado pcsmote: dataset={dataset_logico}, prd={valor_densidad}, pr={valor_riesgo}, cp={cp_code}, ud={ud_str}, ur={ur_str}, tipo_pureza={tipo_pureza}, I={grado_limpieza}, sv={semillas_validas}, sg={sinteticos_generados}\")\n",
    "\n",
    "        if cp_code == \"ent\":\n",
    "            criterio_pureza = \"entropia\"\n",
    "        else:\n",
    "            criterio_pureza = \"proporcion\"\n",
    "\n",
    "        # nombre_configuracion EXACTO seg√∫n el patr√≥n\n",
    "        nombre_configuracion = (\n",
    "            f\"PRD{valor_densidad}_\"\n",
    "            f\"PR{valor_riesgo}_\"\n",
    "            f\"CP{cp_code}_\"\n",
    "            f\"UD{ud_str}_\"\n",
    "            f\"UR{ur_str}_\"\n",
    "            f\"{tipo_pureza}_\"\n",
    "            f\"I{grado_limpieza}_\"\n",
    "            f\"SV{semillas_validas}_\"\n",
    "            f\"SG{sinteticos_generados}\"\n",
    "        )            \n",
    "\n",
    "        ruta_train_csv = os.path.join(ruta_aumentados, nombre)\n",
    "\n",
    "        # Buscar test correspondiente en carpeta base\n",
    "        patron_test = re.compile(rf\"^{re.escape(dataset_logico)}_tdataset(\\d+)_tm(\\d+)_test\\.csv$\")\n",
    "        nombre_test = None\n",
    "        n_test_detectado = None\n",
    "\n",
    "        for nombre_candidato in archivos_base:\n",
    "            m_test = patron_test.match(nombre_candidato)\n",
    "            if m_test:\n",
    "                n_tm = int(m_test.group(2))\n",
    "                tamanio_dataset = int(m_test.group(1))\n",
    "                if n_test_detectado is None or n_tm > n_test_detectado:\n",
    "                    n_test_detectado = n_tm\n",
    "                    nombre_test = nombre_candidato\n",
    "\n",
    "        if nombre_test is None:\n",
    "            if verbose:\n",
    "                print(f\"  ‚ö†Ô∏è  No hay test base para dataset '{dataset_logico}', se omite {nombre}\")\n",
    "            continue\n",
    "\n",
    "        ruta_test_csv = os.path.join(ruta_base, nombre_test)\n",
    "\n",
    "        cont_combinaciones += 1\n",
    "        print(f\"#{cont_combinaciones}  ‚úÖ Agregado pcsmote: {nombre} combinado con {nombre_test}\")\n",
    "\n",
    "        combinaciones.append(DatasetCombination(\n",
    "            dataset_logico=dataset_logico,\n",
    "            tipo_combination=\"pcsmote\",\n",
    "            ruta_train_csv=ruta_train_csv,\n",
    "            ruta_test_csv=ruta_test_csv,\n",
    "            tecnica_aumento=\"pcsmote\",\n",
    "            valor_densidad=valor_densidad,\n",
    "            valor_riesgo=valor_riesgo,\n",
    "\n",
    "            criterio_pureza=criterio_pureza,\n",
    "            percentil_radio_distancia=valor_densidad,\n",
    "            percentil_riesgo=valor_riesgo,  \n",
    "            umbral_densidad=ud_str,\n",
    "            umbral_riesgo=ur_str,\n",
    "\n",
    "            grado_limpieza=grado_limpieza,\n",
    "            sinteticos_generados=sinteticos_generados,\n",
    "            semillas_validas=semillas_validas,\n",
    "            tipo_pureza=tipo_pureza,                 \n",
    "            nombre_configuracion=nombre_configuracion,    \n",
    "            tamanio_dataset=tamanio_dataset\n",
    "    \n",
    "        ))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"üìä Total combinaciones descubiertas: {len(combinaciones)}\")\n",
    "\n",
    "    return combinaciones\n",
    "\n",
    "\n",
    "print(\"üîé Enumerando combinaciones base y aumentadas...\")\n",
    "\n",
    "combinaciones = enumerar_combinaciones_base_y_aumentadas(\n",
    "    ruta_base=RUTA_DATASETS_BASE,\n",
    "    ruta_clasicos=RUTA_DATASETS_CLASICOS,\n",
    "    ruta_aumentados=RUTA_DATASETS_AUMENTADOS,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "if not combinaciones:\n",
    "    print(\"‚ùå No se encontraron combinaciones de datasets.\")\n",
    "\n",
    "\n",
    "datasets_con_base = {c.dataset_logico for c in combinaciones if c.tipo_combination == \"base\"}\n",
    "if not datasets_con_base:\n",
    "    print(\"‚ùå No hay datasets base para comparar.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86cd9024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßÆ Tareas planificadas: 22 (excluidos por dataset: 173, por pol√≠tica SVM-Shuttle‚Üë: 0)\n",
      "üì¶ Total de tareas planificadas: 22\n"
     ]
    }
   ],
   "source": [
    "EXCLUIR_DATASETS = {\n",
    "    \"shuttle\",\n",
    "    \"iris\",\n",
    "    \"glass\",\n",
    "    \"heart\",\n",
    "    \"wdbc\",\n",
    "    \"ecoli\",\n",
    "    \"us_crime\",\n",
    "    \"predict_faults\",\n",
    "    \"gear_vibration\",\n",
    "    # \"telco_churn\",\n",
    "}  # {\"shuttle\", \"ecoli\", ...} si quisieras excluir algo\n",
    "\n",
    "def construir_lista_plana_de_tareas(model_registry,dataset_combinations, orden_modelos,\n",
    "                                    excluir_datasets=EXCLUIR_DATASETS, verbose=True):\n",
    "    \"\"\"\n",
    "    Crea una lista plana de tareas (modelo, combinaci√≥n) y aplica pol√≠ticas de exclusi√≥n.\n",
    "    - excluir_datasets: conjunto de nombres de dataset (en min√∫sculas) a excluir por completo.\n",
    "    - Mantiene la pol√≠tica existente de omitir SVM en Shuttle aumentado si est√° activa.\n",
    "    \"\"\"\n",
    "    tareas = []\n",
    "    excluidos_por_dataset = 0\n",
    "    excluidos_por_politica_svm_shuttle = 0\n",
    "\n",
    "    for nombre_modelo in orden_modelos:\n",
    "        for combo in dataset_combinations:\n",
    "            ds = combo.dataset_logico.lower()\n",
    "\n",
    "            # 1) Excluir datasets completos (p. ej., shuttle)\n",
    "            if ds in (excluir_datasets or set()):\n",
    "                excluidos_por_dataset += 1\n",
    "                continue\n",
    "\n",
    "            # 2) Pol√≠tica original: omitir SVM en Shuttle aumentado\n",
    "            if (OMITIR_SVM_EN_SHUTTLE_AUMENTADO and\n",
    "                nombre_modelo == \"SVM\" and\n",
    "                ds == \"shuttle\" and\n",
    "                combo.tipo_combination == \"aumentado\"):\n",
    "                excluidos_por_politica_svm_shuttle += 1\n",
    "                continue\n",
    "\n",
    "            tareas.append((nombre_modelo, combo))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"üßÆ Tareas planificadas: {len(tareas)} \"\n",
    "              f\"(excluidos por dataset: {excluidos_por_dataset}, \"\n",
    "              f\"por pol√≠tica SVM-Shuttle‚Üë: {excluidos_por_politica_svm_shuttle})\")\n",
    "    return tareas\n",
    "\n",
    "def construir_estimador_y_espacio_random_forest():\n",
    "    est = Pipeline([\n",
    "        ('classifier', RandomForestClassifier(\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=1,\n",
    "            bootstrap=True,\n",
    "            oob_score=False,\n",
    "            n_estimators=150,\n",
    "            max_depth=None,\n",
    "            max_features='sqrt',\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            class_weight=None,\n",
    "            criterion='gini'\n",
    "        ))\n",
    "    ])\n",
    "    # De momento no uso espacio aleatorio para RF (space vac√≠o)\n",
    "    space = {\n",
    "        \"classifier__n_estimators\": [150, 300],        # solo 2 valores\n",
    "        \"classifier__max_features\": [\"sqrt\", \"log2\"],  # regla cl√°sica vs alternativa\n",
    "    }\n",
    "    return est, space\n",
    "\n",
    "REGISTRO_MODELOS = {\n",
    "    \"RandomForest\": construir_estimador_y_espacio_random_forest,\n",
    "}\n",
    "ORDEN_MODELOS = [\"RandomForest\"]\n",
    "\n",
    "tareas = construir_lista_plana_de_tareas(\n",
    "    model_registry=REGISTRO_MODELOS,\n",
    "    dataset_combinations=combinaciones,\n",
    "    orden_modelos=ORDEN_MODELOS,\n",
    "    excluir_datasets=EXCLUIR_DATASETS,\n",
    "    verbose=True\n",
    ")\n",
    "total_tareas = len(tareas)\n",
    "print(f\"üì¶ Total de tareas planificadas: {total_tareas}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "923598eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring para RandomizedSearchCV\n",
    "SCORING_REFIT = \"f1_macro\"\n",
    "SCORING_MULTIPLE = {\n",
    "    \"f1_macro\": \"f1_macro\",\n",
    "    \"balanced_accuracy\": \"balanced_accuracy\",\n",
    "    \"recall_macro\": make_scorer(recall_score, average=\"macro\"),\n",
    "}\n",
    "\n",
    "\n",
    "def ejecutar_rs_y_comparar_cv_con_test(\n",
    "    estimator,\n",
    "    space,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    configuracion_busqueda,\n",
    "    verbose=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Ejecuta RandomizedSearchCV y devuelve:\n",
    "      - F1 macro (CV y Test)\n",
    "      - Balanced Accuracy (CV y Test)\n",
    "      - Recall macro (CV y Test)\n",
    "    \"\"\"\n",
    "    inicio = time.perf_counter()\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=estimator,\n",
    "        param_distributions=space,\n",
    "        n_iter=configuracion_busqueda[\"n_iter\"],\n",
    "        scoring=SCORING_MULTIPLE,\n",
    "        refit=SCORING_REFIT,\n",
    "        cv=configuracion_busqueda[\"cv\"],\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=configuracion_busqueda[\"n_jobs\"],\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    search.fit(X_train, y_train)\n",
    "    elapsed = time.perf_counter() - inicio\n",
    "\n",
    "    # =====================\n",
    "    # M√©tricas CV (mejor candidato)\n",
    "    # =====================\n",
    "    cv_results = search.cv_results_\n",
    "    best_idx = search.best_index_\n",
    "\n",
    "    cv_f1       = float(cv_results[\"mean_test_f1_macro\"][best_idx])\n",
    "    cv_bacc     = float(cv_results[\"mean_test_balanced_accuracy\"][best_idx])\n",
    "    cv_recall_m = float(cv_results[\"mean_test_recall_macro\"][best_idx])\n",
    "\n",
    "    # =====================\n",
    "    # M√©tricas en Test\n",
    "    # =====================\n",
    "    best_est = search.best_estimator_\n",
    "    y_pred = best_est.predict(X_test)\n",
    "\n",
    "    test_f1       = float(f1_score(y_test, y_pred, average=\"macro\"))\n",
    "    test_bacc     = float(balanced_accuracy_score(y_test, y_pred))\n",
    "    test_recall_m = float(recall_score(y_test, y_pred, average=\"macro\"))\n",
    "\n",
    "    return dict(\n",
    "        mejores_params=search.best_params_,\n",
    "        tiempo=elapsed,\n",
    "        cv=dict(\n",
    "            f1=cv_f1,\n",
    "            bacc=cv_bacc,\n",
    "            recall_macro=cv_recall_m,\n",
    "        ),\n",
    "        test=dict(\n",
    "            f1=test_f1,\n",
    "            bacc=test_bacc,\n",
    "            recall_macro=test_recall_m,\n",
    "        ),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48dd3c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üèÅ [1/22] Dataset: telco_churn | Tipo: base | Modelo: RandomForest\n",
      "üìÇ Train: telco_churn_I0_tm5634_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=4, folds=5, n_jobs=1\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "‚úÖ B√∫squeda completada en 51.74 s\n",
      "üìä F1(CV): 0.7112 | F1(Test): 0.7052\n",
      "\n",
      "================================================================================\n",
      "üèÅ [2/22] Dataset: telco_churn | Tipo: base | Modelo: RandomForest\n",
      "üìÇ Train: telco_churn_I1_tm5577_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=4, folds=5, n_jobs=1\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "‚úÖ B√∫squeda completada en 43.42 s\n",
      "üìä F1(CV): 0.7154 | F1(Test): 0.7019\n",
      "\n",
      "================================================================================\n",
      "üèÅ [3/22] Dataset: telco_churn | Tipo: base | Modelo: RandomForest\n",
      "üìÇ Train: telco_churn_I3_tm5464_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=4, folds=5, n_jobs=1\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "‚úÖ B√∫squeda completada en 43.46 s\n",
      "üìä F1(CV): 0.7241 | F1(Test): 0.7058\n",
      "\n",
      "================================================================================\n",
      "üèÅ [4/22] Dataset: telco_churn | Tipo: base | Modelo: RandomForest\n",
      "üìÇ Train: telco_churn_I5_tm5352_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=4, folds=5, n_jobs=1\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "‚úÖ B√∫squeda completada en 41.51 s\n",
      "üìä F1(CV): 0.7241 | F1(Test): 0.7006\n",
      "\n",
      "================================================================================\n",
      "üèÅ [5/22] Dataset: telco_churn | Tipo: clasico | Modelo: RandomForest\n",
      "üìÇ Train: adasyn_telco_churn_I0_sg2594_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=4, folds=5, n_jobs=1\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "‚úÖ B√∫squeda completada en 68.02 s\n",
      "üìä F1(CV): 0.8577 | F1(Test): 0.7048\n",
      "\n",
      "================================================================================\n",
      "üèÅ [6/22] Dataset: telco_churn | Tipo: clasico | Modelo: RandomForest\n",
      "üìÇ Train: adasyn_telco_churn_I1_sg2535_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=4, folds=5, n_jobs=1\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "‚úÖ B√∫squeda completada en 69.20 s\n",
      "üìä F1(CV): 0.8578 | F1(Test): 0.7102\n",
      "\n",
      "================================================================================\n",
      "üèÅ [7/22] Dataset: telco_churn | Tipo: clasico | Modelo: RandomForest\n",
      "üìÇ Train: adasyn_telco_churn_I3_sg2424_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=4, folds=5, n_jobs=1\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "‚úÖ B√∫squeda completada en 67.60 s\n",
      "üìä F1(CV): 0.8639 | F1(Test): 0.7003\n",
      "\n",
      "================================================================================\n",
      "üèÅ [8/22] Dataset: telco_churn | Tipo: clasico | Modelo: RandomForest\n",
      "üìÇ Train: adasyn_telco_churn_I5_sg2660_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=4, folds=5, n_jobs=1\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "‚úÖ B√∫squeda completada en 67.01 s\n",
      "üìä F1(CV): 0.8655 | F1(Test): 0.7129\n",
      "\n",
      "================================================================================\n",
      "üèÅ [9/22] Dataset: telco_churn | Tipo: clasico | Modelo: RandomForest\n",
      "üìÇ Train: borderlinesmote_telco_churn_I0_sg2644_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=4, folds=5, n_jobs=1\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "‚úÖ B√∫squeda completada en 70.03 s\n",
      "üìä F1(CV): 0.8661 | F1(Test): 0.7141\n",
      "\n",
      "================================================================================\n",
      "üèÅ [10/22] Dataset: telco_churn | Tipo: clasico | Modelo: RandomForest\n",
      "üìÇ Train: borderlinesmote_telco_churn_I1_sg2617_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=4, folds=5, n_jobs=1\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "‚úÖ B√∫squeda completada en 69.43 s\n",
      "üìä F1(CV): 0.8643 | F1(Test): 0.6993\n",
      "\n",
      "================================================================================\n",
      "üèÅ [11/22] Dataset: telco_churn | Tipo: clasico | Modelo: RandomForest\n",
      "üìÇ Train: borderlinesmote_telco_churn_I3_sg2564_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=4, folds=5, n_jobs=1\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "‚úÖ B√∫squeda completada en 67.91 s\n",
      "üìä F1(CV): 0.8643 | F1(Test): 0.7027\n",
      "\n",
      "================================================================================\n",
      "üèÅ [12/22] Dataset: telco_churn | Tipo: clasico | Modelo: RandomForest\n",
      "üìÇ Train: borderlinesmote_telco_churn_I5_sg2512_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=4, folds=5, n_jobs=1\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "‚úÖ B√∫squeda completada en 66.07 s\n",
      "üìä F1(CV): 0.8712 | F1(Test): 0.7154\n",
      "\n",
      "================================================================================\n",
      "üèÅ [13/22] Dataset: telco_churn | Tipo: clasico | Modelo: RandomForest\n",
      "üìÇ Train: smote_telco_churn_I0_sg2644_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=4, folds=5, n_jobs=1\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "‚úÖ B√∫squeda completada en 68.02 s\n",
      "üìä F1(CV): 0.8565 | F1(Test): 0.7171\n",
      "\n",
      "================================================================================\n",
      "üèÅ [14/22] Dataset: telco_churn | Tipo: clasico | Modelo: RandomForest\n",
      "üìÇ Train: smote_telco_churn_I1_sg2617_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=4, folds=5, n_jobs=1\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "‚úÖ B√∫squeda completada en 67.57 s\n",
      "üìä F1(CV): 0.8603 | F1(Test): 0.7128\n",
      "\n",
      "================================================================================\n",
      "üèÅ [15/22] Dataset: telco_churn | Tipo: clasico | Modelo: RandomForest\n",
      "üìÇ Train: smote_telco_churn_I3_sg2564_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=4, folds=5, n_jobs=1\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "‚úÖ B√∫squeda completada en 65.50 s\n",
      "üìä F1(CV): 0.8670 | F1(Test): 0.7137\n",
      "\n",
      "================================================================================\n",
      "üèÅ [16/22] Dataset: telco_churn | Tipo: clasico | Modelo: RandomForest\n",
      "üìÇ Train: smote_telco_churn_I5_sg2512_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=4, folds=5, n_jobs=1\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "‚úÖ B√∫squeda completada en 64.09 s\n",
      "üìä F1(CV): 0.8668 | F1(Test): 0.7136\n",
      "\n",
      "================================================================================\n",
      "üèÅ [17/22] Dataset: telco_churn | Tipo: pcsmote | Modelo: RandomForest\n",
      "üìÇ Train: pcs_telco_churn_PRD35_PR35_CPprop_UD080_Upp080_UR045_I3_SV094_SG2564_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=4, folds=5, n_jobs=1\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "‚úÖ B√∫squeda completada en 57.52 s\n",
      "üìä F1(CV): 0.8622 | F1(Test): 0.7005\n",
      "\n",
      "================================================================================\n",
      "üèÅ [18/22] Dataset: telco_churn | Tipo: pcsmote | Modelo: RandomForest\n",
      "üìÇ Train: pcs_telco_churn_PRD80_PR40_CPent_UD040_PE40_UR045_I0_SV498_SG2644_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=4, folds=5, n_jobs=1\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "‚úÖ B√∫squeda completada en 63.74 s\n",
      "üìä F1(CV): 0.8648 | F1(Test): 0.7160\n",
      "\n",
      "================================================================================\n",
      "üèÅ [19/22] Dataset: telco_churn | Tipo: pcsmote | Modelo: RandomForest\n",
      "üìÇ Train: pcs_telco_churn_PRD80_PR40_CPent_UD040_PE40_UR045_I1_SV498_SG2617_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=4, folds=5, n_jobs=1\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "‚úÖ B√∫squeda completada en 62.56 s\n",
      "üìä F1(CV): 0.8649 | F1(Test): 0.7140\n",
      "\n",
      "================================================================================\n",
      "üèÅ [20/22] Dataset: telco_churn | Tipo: pcsmote | Modelo: RandomForest\n",
      "üìÇ Train: pcs_telco_churn_PRD80_PR40_CPent_UD040_PE90_UR045_I0_SV748_SG2644_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=4, folds=5, n_jobs=1\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "‚úÖ B√∫squeda completada en 65.84 s\n",
      "üìä F1(CV): 0.8661 | F1(Test): 0.7103\n",
      "\n",
      "================================================================================\n",
      "üèÅ [21/22] Dataset: telco_churn | Tipo: pcsmote | Modelo: RandomForest\n",
      "üìÇ Train: pcs_telco_churn_PRD80_PR40_CPent_UD040_PE90_UR045_I5_SV747_SG2512_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=4, folds=5, n_jobs=1\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "‚úÖ B√∫squeda completada en 58.25 s\n",
      "üìä F1(CV): 0.8707 | F1(Test): 0.7114\n",
      "\n",
      "================================================================================\n",
      "üèÅ [22/22] Dataset: telco_churn | Tipo: pcsmote | Modelo: RandomForest\n",
      "üìÇ Train: pcs_telco_churn_PRD95_PR40_CPent_UD030_PE90_UR060_I0_SV793_SG2644_train.csv\n",
      "‚öôÔ∏è  Configuraci√≥n de b√∫squeda: n_iter=4, folds=5, n_jobs=1\n",
      "üöÄ Iniciando RandomizedSearchCV...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "‚úÖ B√∫squeda completada en 64.96 s\n",
      "üìä F1(CV): 0.8661 | F1(Test): 0.7099\n"
     ]
    }
   ],
   "source": [
    "N_ITER_BUSQUEDA_POR_DEFECTO = 4\n",
    "\n",
    "# =========================\n",
    "# Utilidades de datos\n",
    "# =========================\n",
    "def cargar_matriz_caracteristicas_y_etiquetas_desde_csv(ruta_csv):\n",
    "    \"\"\"Lee un CSV y devuelve (X, y). Usa 'target' si existe, si no la √∫ltima columna como y.\"\"\"\n",
    "    df = pd.read_csv(ruta_csv)\n",
    "    if \"target\" in df.columns:\n",
    "        X = df.drop(columns=[\"target\"]).to_numpy(dtype=np.float32, copy=False)\n",
    "        y = df[\"target\"].to_numpy()\n",
    "    else:\n",
    "        X = df.iloc[:, :-1].to_numpy(dtype=np.float32, copy=False)\n",
    "        y = df.iloc[:, -1].to_numpy()\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def definir_configuracion_busqueda_para_dataset(X_train, nombre_dataset_logico, tipo_combination):\n",
    "    \"\"\"\n",
    "    Define configuraci√≥n de b√∫squeda:\n",
    "      Shuttle aumentado -> CV=2\n",
    "      Shuttle o n>=10000 -> CV=3\n",
    "      resto -> CV=5\n",
    "    \"\"\"\n",
    "    n_muestras = X_train.shape[0]\n",
    "    es_shuttle = nombre_dataset_logico.lower() == \"shuttle\"\n",
    "\n",
    "    if es_shuttle and tipo_combination != \"base\":\n",
    "        cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=RANDOM_STATE)\n",
    "    elif es_shuttle or n_muestras >= 10000:\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "    else:\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    return {\n",
    "        \"cv\": cv,\n",
    "        \"n_iter\": N_ITER_BUSQUEDA_POR_DEFECTO,\n",
    "        \"n_jobs\": 1,\n",
    "    }\n",
    "\n",
    "registros = []\n",
    "inicio_total = time.perf_counter()\n",
    "\n",
    "for idx, (nombre_modelo, combo) in enumerate(tareas, start=1):\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üèÅ [{idx}/{total_tareas}] Dataset: {combo.dataset_logico} | \"\n",
    "          f\"Tipo: {combo.tipo_combination} | Modelo: {nombre_modelo}\")\n",
    "    print(f\"üìÇ Train: {os.path.basename(combo.ruta_train_csv)}\")\n",
    "\n",
    "    # =====================\n",
    "    # Cargar datos\n",
    "    # =====================\n",
    "    try:\n",
    "        X_train, y_train = cargar_matriz_caracteristicas_y_etiquetas_desde_csv(combo.ruta_train_csv)\n",
    "        X_test,  y_test  = cargar_matriz_caracteristicas_y_etiquetas_desde_csv(combo.ruta_test_csv)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error leyendo CSV: {e}\")\n",
    "        continue\n",
    "\n",
    "    # =====================\n",
    "    # Configuraci√≥n b√∫squeda\n",
    "    # =====================\n",
    "    configuracion_busqueda = definir_configuracion_busqueda_para_dataset(\n",
    "        X_train, combo.dataset_logico, combo.tipo_combination\n",
    "    )\n",
    "\n",
    "    print(f\"‚öôÔ∏è  Configuraci√≥n de b√∫squeda: \"\n",
    "          f\"n_iter={configuracion_busqueda['n_iter']}, \"\n",
    "          f\"folds={configuracion_busqueda['cv'].n_splits}, \"\n",
    "          f\"n_jobs={configuracion_busqueda['n_jobs']}\")\n",
    "\n",
    "    # =====================\n",
    "    # Estimador + espacio\n",
    "    # =====================\n",
    "    estimator, space = REGISTRO_MODELOS[nombre_modelo]()\n",
    "    print(\"üöÄ Iniciando RandomizedSearchCV...\")\n",
    "\n",
    "    # =====================\n",
    "    # Ejecutar b√∫squeda\n",
    "    # =====================\n",
    "    try:\n",
    "        resultados = ejecutar_rs_y_comparar_cv_con_test(\n",
    "            estimator, space, X_train, y_train, X_test, y_test,\n",
    "            configuracion_busqueda=configuracion_busqueda,\n",
    "            verbose=1\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error durante la b√∫squeda: {e}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"‚úÖ B√∫squeda completada en {resultados['tiempo']:.2f} s\")\n",
    "    print(f\"üìä F1(CV): {resultados['cv']['f1']:.4f} | \"\n",
    "          f\"F1(Test): {resultados['test']['f1']:.4f}\")\n",
    "\n",
    "    # =====================\n",
    "    # Registrar resultados\n",
    "    # =====================\n",
    "    registros.append(asdict(RegistroRendimiento(\n",
    "        dataset_logico=combo.dataset_logico,\n",
    "        tipo_combination=combo.tipo_combination,\n",
    "        nombre_modelo_aprendizaje=nombre_modelo,\n",
    "        tecnica_aumento=combo.tecnica_aumento,\n",
    "        valor_densidad=combo.valor_densidad,\n",
    "        valor_riesgo=combo.valor_riesgo,\n",
    "        criterio_pureza=combo.criterio_pureza,\n",
    "        grado_limpieza=combo.grado_limpieza,\n",
    "\n",
    "        cantidad_train=int(X_train.shape[0]),\n",
    "        cantidad_test=int(X_test.shape[0]),\n",
    "        cantidad_caracteristicas=int(X_train.shape[1]),\n",
    "\n",
    "        # M√©tricas CV (3 decimales, robusto)\n",
    "        cv_f1_macro=round(resultados[\"cv\"][\"f1\"], 3) if resultados[\"cv\"][\"f1\"] is not None else None,\n",
    "        cv_balanced_accuracy=round(resultados[\"cv\"][\"bacc\"], 3) if resultados[\"cv\"][\"bacc\"] is not None else None,\n",
    "        cv_recall_macro=round(resultados[\"cv\"][\"recall_macro\"], 3) if resultados[\"cv\"][\"recall_macro\"] is not None else None,\n",
    "\n",
    "        # M√©tricas Test (3 decimales, robusto)\n",
    "        test_f1_macro=round(resultados[\"test\"][\"f1\"], 3) if resultados[\"test\"][\"f1\"] is not None else None,\n",
    "        test_balanced_accuracy=round(resultados[\"test\"][\"bacc\"], 3) if resultados[\"test\"][\"bacc\"] is not None else None,\n",
    "        test_recall_macro=round(resultados[\"test\"][\"recall_macro\"], 3) if resultados[\"test\"][\"recall_macro\"] is not None else None,\n",
    "\n",
    "\n",
    "        mejores_hiperparametros=str(resultados[\"mejores_params\"]),\n",
    "        tiempo_busqueda_seg=float(resultados[\"tiempo\"]),\n",
    "    )))\n",
    "\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4efe3553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Compilando resultados globales...\n",
      "[DEBUG] Inicializando diccionario para dataset_base = ecoli\n",
      "[OK] BASE detectado: dataset=ecoli, grado_iso=0, train=268, ruta=../datasets/datasets_aumentados/base/ecoli_I0_tm268_train.csv\n",
      "[OK] BASE detectado: dataset=ecoli, grado_iso=1, train=262, ruta=../datasets/datasets_aumentados/base/ecoli_I1_tm262_train.csv\n",
      "[OK] BASE detectado: dataset=ecoli, grado_iso=3, train=258, ruta=../datasets/datasets_aumentados/base/ecoli_I3_tm258_train.csv\n",
      "[OK] BASE detectado: dataset=ecoli, grado_iso=5, train=254, ruta=../datasets/datasets_aumentados/base/ecoli_I5_tm254_train.csv\n",
      "[DEBUG] Inicializando diccionario para dataset_base = gear_vibration\n",
      "[OK] BASE detectado: dataset=gear_vibration, grado_iso=0, train=72, ruta=../datasets/datasets_aumentados/base/gear_vibration_I0_tm72_train.csv\n",
      "[OK] BASE detectado: dataset=gear_vibration, grado_iso=1, train=68, ruta=../datasets/datasets_aumentados/base/gear_vibration_I1_tm68_train.csv\n",
      "[OK] BASE detectado: dataset=gear_vibration, grado_iso=3, train=68, ruta=../datasets/datasets_aumentados/base/gear_vibration_I3_tm68_train.csv\n",
      "[OK] BASE detectado: dataset=gear_vibration, grado_iso=5, train=67, ruta=../datasets/datasets_aumentados/base/gear_vibration_I5_tm67_train.csv\n",
      "[DEBUG] Inicializando diccionario para dataset_base = glass\n",
      "[OK] BASE detectado: dataset=glass, grado_iso=0, train=171, ruta=../datasets/datasets_aumentados/base/glass_I0_tm171_train.csv\n",
      "[OK] BASE detectado: dataset=glass, grado_iso=1, train=166, ruta=../datasets/datasets_aumentados/base/glass_I1_tm166_train.csv\n",
      "[OK] BASE detectado: dataset=glass, grado_iso=3, train=164, ruta=../datasets/datasets_aumentados/base/glass_I3_tm164_train.csv\n",
      "[OK] BASE detectado: dataset=glass, grado_iso=5, train=161, ruta=../datasets/datasets_aumentados/base/glass_I5_tm161_train.csv\n",
      "[DEBUG] Inicializando diccionario para dataset_base = heart\n",
      "[OK] BASE detectado: dataset=heart, grado_iso=0, train=242, ruta=../datasets/datasets_aumentados/base/heart_I0_tm242_train.csv\n",
      "[OK] BASE detectado: dataset=heart, grado_iso=1, train=236, ruta=../datasets/datasets_aumentados/base/heart_I1_tm236_train.csv\n",
      "[OK] BASE detectado: dataset=heart, grado_iso=3, train=233, ruta=../datasets/datasets_aumentados/base/heart_I3_tm233_train.csv\n",
      "[OK] BASE detectado: dataset=heart, grado_iso=5, train=227, ruta=../datasets/datasets_aumentados/base/heart_I5_tm227_train.csv\n",
      "[DEBUG] Inicializando diccionario para dataset_base = predict_faults\n",
      "[OK] BASE detectado: dataset=predict_faults, grado_iso=0, train=8000, ruta=../datasets/datasets_aumentados/base/predict_faults_I0_tm8000_train.csv\n",
      "[OK] BASE detectado: dataset=predict_faults, grado_iso=1, train=7917, ruta=../datasets/datasets_aumentados/base/predict_faults_I1_tm7917_train.csv\n",
      "[OK] BASE detectado: dataset=predict_faults, grado_iso=3, train=7757, ruta=../datasets/datasets_aumentados/base/predict_faults_I3_tm7757_train.csv\n",
      "[OK] BASE detectado: dataset=predict_faults, grado_iso=5, train=7597, ruta=../datasets/datasets_aumentados/base/predict_faults_I5_tm7597_train.csv\n",
      "[DEBUG] Inicializando diccionario para dataset_base = telco_churn\n",
      "[OK] BASE detectado: dataset=telco_churn, grado_iso=0, train=5634, ruta=../datasets/datasets_aumentados/base/telco_churn_I0_tm5634_train.csv\n",
      "[OK] BASE detectado: dataset=telco_churn, grado_iso=1, train=5577, ruta=../datasets/datasets_aumentados/base/telco_churn_I1_tm5577_train.csv\n",
      "[OK] BASE detectado: dataset=telco_churn, grado_iso=3, train=5464, ruta=../datasets/datasets_aumentados/base/telco_churn_I3_tm5464_train.csv\n",
      "[OK] BASE detectado: dataset=telco_churn, grado_iso=5, train=5352, ruta=../datasets/datasets_aumentados/base/telco_churn_I5_tm5352_train.csv\n",
      "[DEBUG] Inicializando diccionario para dataset_base = us_crime\n",
      "[OK] BASE detectado: dataset=us_crime, grado_iso=0, train=1595, ruta=../datasets/datasets_aumentados/base/us_crime_I0_tm1595_train.csv\n",
      "[OK] BASE detectado: dataset=us_crime, grado_iso=1, train=1578, ruta=../datasets/datasets_aumentados/base/us_crime_I1_tm1578_train.csv\n",
      "[OK] BASE detectado: dataset=us_crime, grado_iso=3, train=1546, ruta=../datasets/datasets_aumentados/base/us_crime_I3_tm1546_train.csv\n",
      "[OK] BASE detectado: dataset=us_crime, grado_iso=5, train=1515, ruta=../datasets/datasets_aumentados/base/us_crime_I5_tm1515_train.csv\n",
      "[DEBUG] Inicializando diccionario para dataset_base = wdbc\n",
      "[OK] BASE detectado: dataset=wdbc, grado_iso=0, train=455, ruta=../datasets/datasets_aumentados/base/wdbc_I0_tm455_train.csv\n",
      "[OK] BASE detectado: dataset=wdbc, grado_iso=1, train=450, ruta=../datasets/datasets_aumentados/base/wdbc_I1_tm450_train.csv\n",
      "[OK] BASE detectado: dataset=wdbc, grado_iso=3, train=440, ruta=../datasets/datasets_aumentados/base/wdbc_I3_tm440_train.csv\n",
      "[OK] BASE detectado: dataset=wdbc, grado_iso=5, train=431, ruta=../datasets/datasets_aumentados/base/wdbc_I5_tm431_train.csv\n",
      "\n",
      "[DEBUG] train_base_por_dataset_y_iso = {'ecoli': {0: 268, 1: 262, 3: 258, 5: 254}, 'gear_vibration': {0: 72, 1: 68, 3: 68, 5: 67}, 'glass': {0: 171, 1: 166, 3: 164, 5: 161}, 'heart': {0: 242, 1: 236, 3: 233, 5: 227}, 'predict_faults': {0: 8000, 1: 7917, 3: 7757, 5: 7597}, 'telco_churn': {0: 5634, 1: 5577, 3: 5464, 5: 5352}, 'us_crime': {0: 1595, 1: 1578, 3: 1546, 5: 1515}, 'wdbc': {0: 455, 1: 450, 3: 440, 5: 431}}\n",
      "\n",
      "[DEBUG] Vista r√°pida de df_tabla_final (solo heart y glass):\n",
      "             Tecnica Database  Train\n",
      "8               base    glass    171\n",
      "9               base    glass    166\n",
      "10              base    glass    164\n",
      "11              base    glass    161\n",
      "12              base    heart    242\n",
      "13              base    heart    236\n",
      "14              base    heart    233\n",
      "15              base    heart    227\n",
      "36            adasyn    heart    242\n",
      "37            adasyn    heart    236\n",
      "38            adasyn    heart    233\n",
      "39            adasyn    heart    227\n",
      "60   borderlinesmote    glass    171\n",
      "61   borderlinesmote    glass    166\n",
      "62   borderlinesmote    glass    164\n",
      "63   borderlinesmote    glass    161\n",
      "64   borderlinesmote    heart    242\n",
      "65   borderlinesmote    heart    236\n",
      "66   borderlinesmote    heart    233\n",
      "67   borderlinesmote    heart    227\n",
      "88             smote    glass    171\n",
      "89             smote    glass    166\n",
      "90             smote    glass    164\n",
      "91             smote    glass    161\n",
      "92             smote    heart    242\n",
      "93             smote    heart    236\n",
      "94             smote    heart    233\n",
      "95             smote    heart    227\n",
      "134          pcsmote    glass    164\n",
      "135          pcsmote    glass    164\n",
      "136          pcsmote    glass    171\n",
      "137          pcsmote    glass    166\n",
      "138          pcsmote    glass    171\n",
      "139          pcsmote    glass    161\n",
      "140          pcsmote    glass    171\n",
      "141          pcsmote    glass    166\n",
      "142          pcsmote    glass    161\n",
      "143          pcsmote    glass    171\n",
      "144          pcsmote    glass    171\n",
      "145          pcsmote    heart    233\n",
      "146          pcsmote    heart    233\n",
      "147          pcsmote    heart    242\n",
      "148          pcsmote    heart    236\n",
      "149          pcsmote    heart    242\n",
      "150          pcsmote    heart    227\n",
      "151          pcsmote    heart    242\n",
      "152          pcsmote    heart    236\n",
      "153          pcsmote    heart    227\n",
      "154          pcsmote    heart    242\n",
      "155          pcsmote    heart    242\n",
      "\n",
      "üèÅ Ejecuci√≥n total completada en 1370.49 s\n",
      "üìò Archivo Excel generado: ../resultados\\resultados_RS_cv_vs_test.xlsx\n"
     ]
    }
   ],
   "source": [
    "# ----------------- DataFrame de resultados crudos -----------------\n",
    "print(\"\\nüìä Compilando resultados globales...\")\n",
    "df_resultados = pd.DataFrame(registros)\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "registros_tabla = []\n",
    "\n",
    "# ============================\n",
    "# Mapa: (dataset, grado_I) ‚Üí train_base\n",
    "# Ejemplo:  train_base_por_dataset_y_iso['ecoli'][0]  = 268   (I0)\n",
    "#           train_base_por_dataset_y_iso['ecoli'][5]  = 254   (I5)\n",
    "#           train_base_por_dataset_y_iso['ecoli'][10] = 241   (I10)\n",
    "# ============================\n",
    "train_base_por_dataset_y_iso = {}\n",
    "\n",
    "for comb in combinaciones:\n",
    "    if comb.tecnica_aumento == \"base\":\n",
    "        ruta = comb.ruta_train_csv\n",
    "        # tama√±o train desde el nombre: ..._tmXXX_train.csv\n",
    "        m_train = re.search(r\"_tm(\\d+)_train\\.csv$\", comb.ruta_train_csv)\n",
    "\n",
    "        tamanio_dataset = comb.tamanio_dataset # tama√±o total del dataset (train + test)\n",
    "\n",
    "        if not m_train:\n",
    "            print(f\"[ERROR] No se pudo extraer train_size en BASE para ruta => {ruta}\")\n",
    "            print(\"        Patr√≥n esperado: *_tmXXX_train.csv\")            \n",
    "            continue\n",
    "        train_size = int(m_train.group(1))\n",
    "\n",
    "        # grado de Isolation Forest: ..._I0_..., ..._I5_..., ..._I10_...\n",
    "        m_iso = re.search(r\"_I(\\d+)_\", comb.ruta_train_csv)\n",
    "        if not m_iso:\n",
    "            print(f\"[WARN] No se pudo extraer grado_iso en BASE para ruta => {ruta}\")\n",
    "            print(\"       Se asigna grado_iso = 0 por default\")\n",
    "            grado_iso = 0\n",
    "        else:\n",
    "            grado_iso = int(m_iso.group(1))\n",
    "\n",
    "        if comb.dataset_logico not in train_base_por_dataset_y_iso:\n",
    "            print(f\"[DEBUG] Inicializando diccionario para dataset_base = {comb.dataset_logico}\")\n",
    "            train_base_por_dataset_y_iso[comb.dataset_logico] = {}\n",
    "\n",
    "        # -------------------------\n",
    "        # Guardar resultado limpio\n",
    "        # -------------------------\n",
    "        train_base_por_dataset_y_iso[comb.dataset_logico][grado_iso] = train_size\n",
    "\n",
    "        # -------------------------\n",
    "        # DEBUG 4: confirmaci√≥n\n",
    "        # -------------------------\n",
    "        print(\n",
    "            f\"[OK] BASE detectado: dataset={comb.dataset_logico}, \"\n",
    "            f\"grado_iso={grado_iso}, train={train_size}, ruta={ruta}\"\n",
    "        )\n",
    "\n",
    "print(\"\\n[DEBUG] train_base_por_dataset_y_iso =\", train_base_por_dataset_y_iso)\n",
    "\n",
    "# ============================\n",
    "# Construir tabla resumida\n",
    "# ============================\n",
    "for comb in combinaciones:\n",
    "    tecnica = comb.tecnica_aumento           # base / smote / adasyn / borderlinesmote / pcsmote\n",
    "    dataset = comb.dataset_logico\n",
    "\n",
    "    # --- Tama√±o test (com√∫n a todas las t√©cnicas del dataset) ---\n",
    "    m_test = re.search(r\"_tm(\\d+)_test\\.csv$\", comb.ruta_test_csv)\n",
    "    test_size = int(m_test.group(1)) if m_test else None\n",
    "\n",
    "    # --- Tama√±o train (debe coincidir con el baseline del mismo grado I) ---\n",
    "    train_size = None\n",
    "\n",
    "    if tecnica == \"base\":\n",
    "        # El baseline puede tomar directamente su propio tmXXX\n",
    "        m_train = re.search(r\"_tm(\\d+)_train\\.csv$\", comb.ruta_train_csv)\n",
    "        if m_train:\n",
    "            train_size = int(m_train.group(1))\n",
    "\n",
    "    else:\n",
    "        # Para todas las t√©cnicas aumentadas (smote / adasyn / borderline / pcsmote)\n",
    "        # buscamos el grado I correspondiente y miramos el baseline.\n",
    "        #   - cl√°sicos: el grado se lee del nombre del csv de train\n",
    "        #   - pcsmote: el grado se lee del nombre de configuraci√≥n (PRD..._I0_... etc.)\n",
    "        if tecnica == \"pcsmote\":\n",
    "            fuente_iso = comb.nombre_configuracion\n",
    "        else:  # smote / adasyn / borderlinesmote\n",
    "            fuente_iso = comb.ruta_train_csv\n",
    "\n",
    "        m_iso = re.search(r\"_I(\\d+)_\", fuente_iso)\n",
    "        grado_iso = int(m_iso.group(1)) if m_iso else 0\n",
    "\n",
    "        train_size = train_base_por_dataset_y_iso.get(dataset, {}).get(grado_iso)\n",
    "\n",
    "        if train_size is None:\n",
    "            print(\n",
    "                f\"[DEBUG] Sin baseline para t√©cnica={tecnica}, \"\n",
    "                f\"dataset={dataset}, grado_I={grado_iso}. \"\n",
    "                f\"train_size quedar√° como None. ruta_train={comb.ruta_train_csv}\"\n",
    "            )\n",
    "\n",
    "    # Total = train + test (solo si ambos est√°n definidos)\n",
    "    if train_size is not None and test_size is not None:\n",
    "        total = train_size + test_size\n",
    "    else:\n",
    "        total = None\n",
    "\n",
    "    # --- Configuraci√≥n y s√≠ntesis ---\n",
    "    if tecnica == \"pcsmote\":\n",
    "        configuracion = comb.nombre_configuracion\n",
    "        semillas_candidatas = comb.semillas_validas\n",
    "        sinteticos_generados = comb.sinteticos_generados\n",
    "    elif tecnica in (\"smote\", \"adasyn\", \"borderlinesmote\"):\n",
    "        configuracion = Path(comb.ruta_train_csv).stem\n",
    "        semillas_candidatas = None\n",
    "        sinteticos_generados = comb.sinteticos_generados\n",
    "    else:  # base\n",
    "        configuracion = Path(comb.ruta_train_csv).stem\n",
    "        semillas_candidatas = \"-------\"\n",
    "        sinteticos_generados = \"-------\"\n",
    "\n",
    "    # --- M√©tricas CV/Test para ESTA combinaci√≥n concreta ---\n",
    "    if comb.tipo_combination == \"pcsmote\":\n",
    "        df_fila = df_resultados[\n",
    "            (df_resultados[\"dataset_logico\"]   == comb.dataset_logico) &\n",
    "            (df_resultados[\"tipo_combination\"] == comb.tipo_combination) &\n",
    "            (df_resultados[\"grado_limpieza\"]   == comb.grado_limpieza) &\n",
    "            (df_resultados[\"valor_densidad\"]   == comb.valor_densidad) &\n",
    "            (df_resultados[\"valor_riesgo\"]     == comb.valor_riesgo) &\n",
    "            (df_resultados[\"criterio_pureza\"]  == comb.criterio_pureza)\n",
    "        ]\n",
    "    else:\n",
    "        # baseline / smote / adasyn / borderline\n",
    "        df_fila = df_resultados[\n",
    "            (df_resultados[\"dataset_logico\"]   == comb.dataset_logico) &\n",
    "            (df_resultados[\"tipo_combination\"] == comb.tipo_combination) &\n",
    "            (df_resultados[\"grado_limpieza\"]   == comb.grado_limpieza)\n",
    "        ]\n",
    "\n",
    "    # --- Extraer m√©tricas ---\n",
    "    if df_fila.empty:\n",
    "        f1_cv   = None\n",
    "        ba_cv   = None\n",
    "        f1_test = None\n",
    "        ba_test = None\n",
    "    else:\n",
    "        fila = df_fila.iloc[0]\n",
    "        f1_cv   = fila[\"cv_f1_macro\"]\n",
    "        ba_cv   = fila[\"cv_balanced_accuracy\"]\n",
    "        f1_test = fila[\"test_f1_macro\"]\n",
    "        ba_test = fila[\"test_balanced_accuracy\"]\n",
    "\n",
    "\n",
    "    registros_tabla.append({\n",
    "        \"Tecnica\": tecnica,\n",
    "        \"Database\": dataset,\n",
    "        # \"Configuracion\": configuracion,\n",
    "\n",
    "        \"percentil radio distancia\": comb.percentil_radio_distancia if tecnica == \"pcsmote\" else \"--\",\n",
    "        \"percentil riesgo\": comb.percentil_riesgo if tecnica == \"pcsmote\" else \"--\",\n",
    "        \"Criterio pureza\": comb.criterio_pureza if tecnica == \"pcsmote\" else \"--\",\n",
    "        \"umbral densidad\": comb.umbral_densidad if tecnica == \"pcsmote\" else \"--\",\n",
    "        \"umbral riesgo\": comb.umbral_riesgo if tecnica == \"pcsmote\" else \"--\",\n",
    "        \"Tipo pureza\": comb.tipo_pureza if tecnica == \"pcsmote\" else \"--\",\n",
    "\n",
    "        \"Total N\": comb.tamanio_dataset,\n",
    "        \"Grado Isolation Forest\": comb.grado_limpieza,\n",
    "        \"Total Isolation\": total,\n",
    "        \"Test\": test_size,\n",
    "        \"Train\": train_size,\n",
    "        \"Semillas candidatas train\": semillas_candidatas,\n",
    "        \"Sinteticas generadas\": sinteticos_generados,\n",
    "        \"F1 macro CV\": f1_cv,\n",
    "        \"Balanced Accuracy CV\": ba_cv,\n",
    "        \"F1 macro Test\": f1_test,\n",
    "        \"Balanced Accuracy Test\": ba_test,\n",
    "    })\n",
    "\n",
    "df_tabla_final = pd.DataFrame(registros_tabla)\n",
    "\n",
    "print(\"\\n[DEBUG] Vista r√°pida de df_tabla_final (solo heart y glass):\")\n",
    "print(\n",
    "    df_tabla_final[\n",
    "        df_tabla_final[\"Database\"].isin([\"glass\", \"heart\"])\n",
    "    ][[\"Tecnica\", \"Database\", \"Train\"]]\n",
    ")\n",
    "\n",
    "df_tabla_final.to_excel(NOMBRE_ARCHIVO_EXCEL, index=False)\n",
    "\n",
    "fin_total = time.perf_counter()\n",
    "duracion = round(fin_total - inicio_total, 2)\n",
    "print(f\"\\nüèÅ Ejecuci√≥n total completada en {duracion} s\")\n",
    "print(f\"üìò Archivo Excel generado: {NOMBRE_ARCHIVO_EXCEL}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
