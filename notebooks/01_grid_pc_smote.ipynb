{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "482b3476",
   "metadata": {},
   "source": [
    "# Evaluaci√≥n de PC-SMOTE con Grid Search en el dataset Shuttle (Generaci√≥n de caso base y datasets aumentados)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27267283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lo que hace es modificar la lista de rutas de b√∫squeda de m√≥dulos de Python (sys.path) para incluir las carpetas ../scripts y ../datasets como ubicaciones adicionales donde Python puede buscar m√≥dulos o paquetes cuando hac√©s un import.\n",
    "import sys\n",
    "sys.path.append(\"../scripts\")\n",
    "sys.path.append(\"../datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9810c62f",
   "metadata": {},
   "source": [
    "## Importaci√≥n de m√≥dulos y librer√≠as necesarias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ee7abae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- M√≥dulos propios del proyecto ---\n",
    "from cargar_dataset import cargar_dataset, obtener_metadata_dataset # Funci√≥n para cargar datasets seg√∫n configuraci√≥n\n",
    "from config_datasets import config_datasets                         # Diccionario de configuraci√≥n de datasets\n",
    "from evaluacion import evaluar_sampler_holdout                      # Evaluaci√≥n de sobremuestreo con partici√≥n hold-out\n",
    "from pc_smote import PCSMOTE                                        # Implementaci√≥n principal de PCSMOTE\n",
    "from graficador2d import Graficador2D                               # Clase para graficar resultados en 2D\n",
    "from isolation_cleaner import IsolationCleaner                      # Clase para limpieza de outliers con Isolation Forest\n",
    "from Utils import Utils                                             # Clase utilitaria con funciones auxiliares\n",
    "from limpiador import LimpiadorOutliers                             # Clase para limpieza de datos\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN\n",
    "\n",
    "# --- Librer√≠as est√°ndar de Python ---\n",
    "from datetime import datetime, timedelta                       # Manejo de fechas y tiempos\n",
    "from itertools import product                                  # Generaci√≥n de combinaciones de par√°metros\n",
    "import os                                                      # Operaciones con el sistema de archivos\n",
    "from pathlib import Path\n",
    "\n",
    "import traceback\n",
    "# --- Librer√≠as cient√≠ficas ---\n",
    "import numpy as np                                              # Operaciones num√©ricas y algebra lineal\n",
    "import pandas as pd                                             # Manipulaci√≥n y an√°lisis de datos tabulares\n",
    "from scipy.stats import uniform                                 # Distribuciones para b√∫squeda de hiperpar√°metros\n",
    "\n",
    "# --- Scikit-learn: preprocesamiento ---\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler # Codificaci√≥n de etiquetas y escalado de datos\n",
    "from sklearn.pipeline import make_pipeline, Pipeline            # Creaci√≥n de pipelines de procesamiento y modelado\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# --- Scikit-learn: divisi√≥n y validaci√≥n ---\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,                                           # Divisi√≥n de datos en train/test\n",
    "    StratifiedKFold,                                            # Validaci√≥n cruzada estratificada\n",
    "    RandomizedSearchCV                                          # B√∫squeda aleatoria de hiperpar√°metros\n",
    ")\n",
    "\n",
    "# --- Scikit-learn: reducci√≥n de dimensionalidad ---\n",
    "from sklearn.decomposition import PCA                           # An√°lisis de Componentes Principales\n",
    "\n",
    "# --- Scikit-learn: m√©tricas ---\n",
    "from sklearn.metrics import (\n",
    "    f1_score,                                                    # M√©trica F1-Score\n",
    "    balanced_accuracy_score,                                     # Precisi√≥n balanceada\n",
    "    matthews_corrcoef,                                           # Coeficiente MCC\n",
    "    cohen_kappa_score,                                           # Kappa de Cohen\n",
    "    make_scorer                                            \n",
    ")\n",
    "\n",
    "# --- Scikit-learn: clasificadores ---\n",
    "from sklearn.ensemble import RandomForestClassifier             # Clasificador Random Forest\n",
    "from sklearn.linear_model import LogisticRegression             # Regresi√≥n log√≠stica\n",
    "from sklearn.svm import SVC                                      # M√°quinas de Vectores de Soporte (SVM)\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "RUTA_CLASICOS = \"../datasets/datasets_aumentados/resampler_clasicos/\"\n",
    "RUTA_CLASICOS = Path(RUTA_CLASICOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab2e1ca",
   "metadata": {},
   "source": [
    "## Generaci√≥n del caso base\n",
    "\n",
    "Este c√≥digo realiza dos tareas principales para cada dataset configurado en `config_datasets`:\n",
    "\n",
    "1. **Generar el caso base** (subcarpeta `datasets_aumentados/base/`):\n",
    "   - Se crea un directorio espec√≠fico para almacenar la versi√≥n original del dataset sin ning√∫n tipo de sobremuestreo.\n",
    "   - El dataset se carga utilizando la misma funci√≥n `cargar_dataset` empleada en el pipeline principal.\n",
    "   - Si las etiquetas (`y`) est√°n en formato de texto u objeto, se convierten a valores num√©ricos con `LabelEncoder`.\n",
    "   - Se realiza una divisi√≥n estratificada en conjuntos de entrenamiento y prueba (`train/test`) utilizando `train_test_split` con una proporci√≥n 70/30 y una semilla fija para asegurar reproducibilidad.\n",
    "   - Se guardan dos archivos CSV: `<nombre_dataset>_train.csv` y `<nombre_dataset>_test.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca24bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "def generar_caso_base(\n",
    "    nombre_dataset: str,\n",
    "    config: dict,\n",
    "    ruta_base: str = \"../datasets/datasets_aumentados/base/\",\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    overwrite: bool = False,\n",
    "    porcentaje_limpieza: float = 0.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Genera el caso base (sin sobremuestreo) aplicando:\n",
    "      1) Carga del dataset seg√∫n config_datasets y cargar_dataset()\n",
    "      2) Split train/test\n",
    "      3) RobustScaler (fit SOLO en X_train, transform en X_train/X_test)\n",
    "      4) (Opcional) Limpieza de outliers con IsolationCleaner (IsolationForest por percentil)\n",
    "         SOLO sobre X_train_scaled.\n",
    "\n",
    "    IMPORTANTE:\n",
    "    - `porcentaje_limpieza` se usa para LIMPIAR realmente el conjunto\n",
    "      de entrenamiento con IsolationCleaner (si > 0). Internamente se pasa como\n",
    "      `percentil_umbral`.\n",
    "    - El valor de `porcentaje_limpieza` se refleja en el nombre del archivo de TRAIN\n",
    "      como sufijo `_I{porcentaje_limpieza}_tm{n_train}` para dejar traza.\n",
    "    - El TEST nunca se limpia con IsolationForest, por eso su nombre NO lleva sufijo `I*`,\n",
    "      solo `_tm{n_test}`.\n",
    "\n",
    "    El resultado son dos CSV:\n",
    "      - {ruta_base}/{nombre_dataset}_I{porcentaje_limpieza}_tm{n_train}_train.csv\n",
    "        (train escalado y, si corresponde, limpiado con IF por percentil)\n",
    "      - {ruta_base}/{nombre_dataset}_tm{n_test}_test.csv\n",
    "        (test escalado, sin limpieza IF)\n",
    "\n",
    "    Adem√°s, devuelve tambi√©n:\n",
    "      - X_test_base, y_test_base (versiones numpy ya escaladas)\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(ruta_base, exist_ok=True)\n",
    "\n",
    "    col_target = config[\"col_target\"]\n",
    "    col_features = config.get(\"col_features\", None)\n",
    "\n",
    "    # Normalizamos el valor de I para el nombre (entero)\n",
    "    if porcentaje_limpieza is None:\n",
    "        valor_I = 0\n",
    "    else:\n",
    "        valor_I = int(porcentaje_limpieza)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # CASO 1: intentar reutilizar archivos existentes (patr√≥n con tm)\n",
    "    # ------------------------------------------------------------------\n",
    "    path_train_existente = None\n",
    "    path_test_existente = None\n",
    "\n",
    "    prefijo_train = f\"{nombre_dataset}_I{valor_I}_tm\"\n",
    "    prefijo_test = f\"{nombre_dataset}_tm\"\n",
    "\n",
    "    for fname in os.listdir(ruta_base):\n",
    "        # buscamos algo como: {nombre_dataset}_I{I}_tm{N}_train.csv\n",
    "        if fname.startswith(prefijo_train) and fname.endswith(\"_train.csv\"):\n",
    "            path_train_existente = os.path.join(ruta_base, fname)\n",
    "        # y algo como: {nombre_dataset}_tm{M}_test.csv\n",
    "        if fname.startswith(prefijo_test) and fname.endswith(\"_test.csv\"):\n",
    "            path_test_existente = os.path.join(ruta_base, fname)\n",
    "\n",
    "    if (\n",
    "        not overwrite\n",
    "        and path_train_existente is not None\n",
    "        and path_test_existente is not None\n",
    "    ):\n",
    "        print(f\"‚ö†Ô∏è Caso base ya existe para {nombre_dataset}. Usando archivos existentes.\")\n",
    "        print(f\"   Train existente: {path_train_existente}\")\n",
    "        print(f\"   Test existente : {path_test_existente}\")\n",
    "\n",
    "        df_train_existente = pd.read_csv(path_train_existente)\n",
    "        df_test_existente = pd.read_csv(path_test_existente)\n",
    "\n",
    "        if col_target not in df_train_existente.columns:\n",
    "            raise ValueError(\n",
    "                f\"La columna target '{col_target}' no est√° en el train existente {path_train_existente}\"\n",
    "            )\n",
    "        if col_target not in df_test_existente.columns:\n",
    "            raise ValueError(\n",
    "                f\"La columna target '{col_target}' no est√° en el test existente {path_test_existente}\"\n",
    "            )\n",
    "\n",
    "        if col_features is None:\n",
    "            col_features = [c for c in df_train_existente.columns if c != col_target]\n",
    "\n",
    "        X_test_base = df_test_existente[col_features].values\n",
    "        y_test_base = df_test_existente[col_target].values\n",
    "\n",
    "        return path_train_existente, path_test_existente, X_test_base, y_test_base\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # CASO 2: generar caso base desde cero\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    # 1) Cargar dataset crudo seg√∫n config_datasets usando cargar_dataset()\n",
    "    df_features, y, clases = cargar_dataset(\n",
    "        path=config[\"path\"],\n",
    "        clase_minoria=config.get(\"clase_minoria\"),\n",
    "        col_features=config.get(\"col_features\"),\n",
    "        col_target=config.get(\"col_target\"),\n",
    "        sep=config.get(\"sep\"),\n",
    "        header=config.get(\"header\"),\n",
    "        binarizar=config.get(\"binarizar\", False),\n",
    "        tipo=config.get(\"tipo\", \"tabular\"),\n",
    "        impute=config.get(\"impute\", \"median\"),\n",
    "        na_values=config.get(\"na_values\", (\"?\", \"NA\", \"None\")),\n",
    "        dataset_name=config.get(\"dataset_name\", nombre_dataset),\n",
    "        names=config.get(\"esquema\"),\n",
    "    )\n",
    "\n",
    "    # X son directamente las features que devuelve cargar_dataset\n",
    "    X = df_features.values  # ya tiene col_features como columnas\n",
    "\n",
    "    idx_original = np.arange(len(X), dtype=int)\n",
    "\n",
    "    tamanio_tn_X = X.shape[0]\n",
    "\n",
    "    # 2) Train / test split (estratificado)\n",
    "    X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        idx_original,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y,\n",
    "    )\n",
    "\n",
    "\n",
    "    print(f\"[{nombre_dataset}] Split: X_train={X_train.shape}, X_test={X_test.shape}\")\n",
    "\n",
    "    # 3) Escalado robusto (fit en train, transform en train y test)\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # 4) (Opcional) Limpieza de outliers con IsolationCleaner SOLO sobre TRAIN\n",
    "    if porcentaje_limpieza is not None and porcentaje_limpieza > 0:\n",
    "        print(\n",
    "            f\"[{nombre_dataset}] Aplicando IsolationCleaner \"\n",
    "            f\"(percentil={porcentaje_limpieza}%) sobre TRAIN\"\n",
    "        )\n",
    "\n",
    "        X_train_scaled, y_train, idx_train, info_limpieza = IsolationCleaner.limpiarOutliers(\n",
    "            X=X_train_scaled,\n",
    "            y=y_train,\n",
    "            idx_original=idx_train,\n",
    "            percentil_umbral=float(porcentaje_limpieza),\n",
    "            contamination=\"auto\",\n",
    "            n_estimators=200,\n",
    "            max_samples=\"auto\",\n",
    "            random_state=random_state,\n",
    "            bootstrap=False,\n",
    "            normalizar_scores=False,\n",
    "            devolver_info=True,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        removed_total = info_limpieza.get(\"removed_total\", 0)\n",
    "        total_final = len(y_train)\n",
    "        total_inicial = removed_total + total_final\n",
    "        print(\n",
    "            f\"[{nombre_dataset}] Limpieza IF (percentil): \"\n",
    "            f\"removidos={removed_total} / total_inicial‚âà{total_inicial} \"\n",
    "            f\"(train_final={total_final})\"\n",
    "        )\n",
    "\n",
    "    # Si col_features es None ac√°, las sacamos de df_features:\n",
    "    if col_features is None:\n",
    "        col_features = list(df_features.columns)\n",
    "\n",
    "    # 5) Reconstruir DataFrames con columnas\n",
    "    df_train = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(X_train_scaled, columns=col_features),\n",
    "            pd.Series(y_train, name=col_target),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    df_test = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(X_test_scaled, columns=col_features),\n",
    "            pd.Series(y_test, name=col_target),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Cantidades finales (ya con posible limpieza IF en train)\n",
    "    n_train_final = df_train.shape[0]\n",
    "    n_test_final = df_test.shape[0]\n",
    "\n",
    "    # 6) Construir nombres de archivo con patr√≥n solicitado\n",
    "    nombre_train = f\"{nombre_dataset}_I{valor_I}_tm{n_train_final}_train.csv\"\n",
    "    nombre_test = f\"{nombre_dataset}_tdataset{tamanio_tn_X}_tm{n_test_final}_test.csv\"\n",
    "\n",
    "    path_train = os.path.join(ruta_base, nombre_train)\n",
    "    path_test = os.path.join(ruta_base, nombre_test)\n",
    "\n",
    "    # 7) Guardar CSV base\n",
    "    df_train.to_csv(path_train, index=False)\n",
    "    df_test.to_csv(path_test, index=False)\n",
    "\n",
    "    print(f\"‚úÖ Caso base generado para {nombre_dataset}\")\n",
    "    print(f\"   Train: {path_train}  (rows={n_train_final})\")\n",
    "    print(f\"   Test : {path_test}   (rows={n_test_final})\")\n",
    "\n",
    "    # 8) Devolver tambi√©n X_test_base / y_test_base para metadata / m√©tricas\n",
    "    X_test_base = X_test_scaled\n",
    "    y_test_base = y_test\n",
    "\n",
    "    return path_train, path_test, X_test_base, y_test_base, idx_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d665899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aumentar_dataset_pcsmote_y_guardar( \n",
    "    nombre_dataset,\n",
    "    X_train_base,\n",
    "    y_train_base,\n",
    "    percentil_radio_densidad,\n",
    "    umbral_densidad,\n",
    "    umbral_pureza_proporcion,\n",
    "    umbral_riesgo,\n",
    "    percentil_riesgo,\n",
    "    criterio_pureza,\n",
    "    percentil_entropia,\n",
    "    col_target=\"target\",\n",
    "    percentil_isolation_etiqueta: float = 0.0,\n",
    "    X_test_base=None,\n",
    "    y_test_base=None,\n",
    "    ruta_pcsmote=\"../datasets/datasets_aumentados/\",\n",
    "    overwrite=False,\n",
    "    idx_train=None\n",
    "):\n",
    "    try:\n",
    "        # ================================\n",
    "        # 1) Crear sampler y resamplear\n",
    "        # ================================\n",
    "        sampler = PCSMOTE(\n",
    "            random_state=42,\n",
    "            criterio_pureza=criterio_pureza,\n",
    "            percentil_dist_densidad=percentil_radio_densidad,\n",
    "            percentil_dist_riesgo=percentil_riesgo,\n",
    "            percentil_entropia=percentil_entropia,\n",
    "            umbral_densidad=umbral_densidad,\n",
    "            umbral_pureza=umbral_pureza_proporcion,\n",
    "            umbral_riesgo=umbral_riesgo,\n",
    "            grado_iso=percentil_isolation_etiqueta\n",
    "        )\n",
    "\n",
    "        X_res, y_res = sampler.fit_resample(X_train_base, y_train_base, idx_original=idx_train)\n",
    "\n",
    "        # cantidad de sint√©ticos generados\n",
    "        n_original = len(X_train_base)\n",
    "        n_res = len(X_res)\n",
    "        cant_sinteticos_generados = n_res - n_original\n",
    "\n",
    "        # ================================\n",
    "        # 2) Guardar dataset aumentado\n",
    "        # ================================\n",
    "        os.makedirs(ruta_pcsmote, exist_ok=True)\n",
    "\n",
    "        # ----------------------------\n",
    "        # Tags para el nombre del file\n",
    "        # ----------------------------\n",
    "        # PRD: percentil radio distancia\n",
    "        tag_prd = f\"PRD{int(percentil_radio_densidad)}\"\n",
    "\n",
    "        # PR: percentil riesgo\n",
    "        tag_pr = f\"PR{int(percentil_riesgo)}\"\n",
    "\n",
    "        # CP: criterio pureza (entrop√≠a | proporci√≥n)\n",
    "        criterio_lower = str(criterio_pureza).lower()\n",
    "        if criterio_lower == \"entropia\":\n",
    "            tag_cp = \"CPent\"\n",
    "        elif criterio_lower == \"proporcion\":\n",
    "            tag_cp = \"CPprop\"\n",
    "        else:\n",
    "            raise ValueError(f\"Criterio de pureza desconocido: {criterio_pureza}\")\n",
    "\n",
    "        # UD: umbral densidad en %, padded a 3 d√≠gitos ‚Üí UD080\n",
    "        valor_ud = int(round(umbral_densidad * 100))\n",
    "        tag_ud = f\"UD{valor_ud:03d}\"\n",
    "\n",
    "        # Pureza extra:\n",
    "        # - entrop√≠a: PE{percentil_entropia} ‚Üí PE45\n",
    "        # - proporci√≥n: Ppp{upp% en 3 d√≠gitos} ‚Üí Ppp041\n",
    "        if criterio_lower == \"entropia\":\n",
    "            if percentil_entropia is None:\n",
    "                raise ValueError(\"percentil_entropia no puede ser None cuando criterio_pureza='entropia'\")\n",
    "            tag_pureza = f\"PE{int(percentil_entropia)}\"\n",
    "        else:  # \"proporcion\"\n",
    "            if umbral_pureza_proporcion is None:\n",
    "                raise ValueError(\"umbral_pureza_proporcion no puede ser None cuando criterio_pureza='proporcion'\")\n",
    "            valor_upp = int(round(umbral_pureza_proporcion * 100))\n",
    "            tag_pureza = f\"Upp{valor_upp:03d}\"\n",
    "\n",
    "        # UR: umbral riesgo\n",
    "        tag_ur = f\"UR{int(round(umbral_riesgo*100)):03d}\"\n",
    "\n",
    "        # I: percentil de isolation\n",
    "        tag_iso = f\"I{int(percentil_isolation_etiqueta)}\"\n",
    "\n",
    "        # ‚úÖ SV: cantidad de semillas v√°lidas (candidatas)\n",
    "        semillas_validas = getattr(sampler, \"cantidad_semillas_candidatas\", 0)\n",
    "        tag_sv = f\"SV{int(semillas_validas):03d}\"\n",
    "\n",
    "        # SG: cantidad de sint√©ticos generados, padded a 3 d√≠gitos\n",
    "        tag_sg = f\"SG{cant_sinteticos_generados:03d}\"\n",
    "\n",
    "        # Nombre final del archivo\n",
    "        # Ej entrop√≠a:\n",
    "        #   pcs_ecoli_PRD35_PR35_CPent_UD080_PE45_I0_SG120.csv\n",
    "        # Ej proporci√≥n:\n",
    "        #   pcs_ecoli_PRD35_PR35_CPprop_UD080_Ppp041_I0_SG007.csv\n",
    "        nombre_archivo = (\n",
    "            f\"pcs_{nombre_dataset}_\"\n",
    "            f\"{tag_prd}_\"\n",
    "            f\"{tag_pr}_\"\n",
    "            f\"{tag_cp}_\"\n",
    "            f\"{tag_ud}_\"\n",
    "            f\"{tag_pureza}_\"\n",
    "            f\"{tag_ur}_\"\n",
    "            f\"{tag_iso}_\"\n",
    "            f\"{tag_sv}_\"\n",
    "            f\"{tag_sg}_train.csv\"\n",
    "        )\n",
    "\n",
    "        nombre_archivo = Utils.safe_token(nombre_archivo)\n",
    "        path_salida = os.path.join(ruta_pcsmote, nombre_archivo)\n",
    "\n",
    "        if overwrite or not os.path.exists(path_salida):\n",
    "            # Reconstruir columnas correctamente\n",
    "            df_res = pd.DataFrame(X_res)\n",
    "            df_res[col_target] = y_res\n",
    "            df_res.to_csv(path_salida, index=False)\n",
    "            print(\n",
    "                f\"   üü¢ PCSMOTE guardado: {path_salida} \"\n",
    "                f\"(sg={cant_sinteticos_generados})\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"   ‚ö™ Omitido (ya existe): {path_salida}\")\n",
    "\n",
    "        # ================================\n",
    "        # 3) LOG POR MUESTRA\n",
    "        # ================================\n",
    "        df_log = pd.DataFrame(sampler.logs_por_muestra)\n",
    "\n",
    "        # ================================\n",
    "        # 4) METADATA DEL DATASET\n",
    "        # ================================\n",
    "        metadata = obtener_metadata_dataset(\n",
    "            nombre_dataset,\n",
    "            X_train_base,\n",
    "            y_train_base,\n",
    "            X_test=X_test_base,\n",
    "            y_test=y_test_base\n",
    "        )\n",
    "\n",
    "        df_header = (\n",
    "            pd.DataFrame(metadata, index=[0])\n",
    "            .T.rename(columns={0: \"valor\"})\n",
    "        )\n",
    "        df_header.reset_index(inplace=True)\n",
    "        df_header.columns = [\"campo\", \"valor\"]\n",
    "\n",
    "        return df_header, df_log, sampler\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        print(f\"‚ùå Error al aumentar dataset {nombre_dataset}: {e}\")\n",
    "        return None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0d5fb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_aumentaciones_clasicas_y_guardar(\n",
    "    nombre_dataset: str,\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    col_target: str,\n",
    "    ruta_clasicos: str = \"../datasets/datasets_aumentados/resampler_clasicos/\",\n",
    "    overwrite: bool = False,\n",
    "    percentil_isolation_etiqueta: float = 0.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Genera datasets aumentados con t√©cnicas cl√°sicas a partir de X_train, y_train:\n",
    "      - SMOTE\n",
    "      - BorderlineSMOTE\n",
    "      - ADASYN\n",
    "\n",
    "    Guarda los resultados con patr√≥n:\n",
    "      {resampler}_{dataset}_I{percentil}_sg{cant_sinteticos}_train.csv\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(ruta_clasicos, exist_ok=True)\n",
    "\n",
    "    columnas = list(X_train.columns)\n",
    "    n_original = len(X_train)\n",
    "\n",
    "    print(f\"üîß Aumentaci√≥n cl√°sica (en memoria) para: {nombre_dataset}\")\n",
    "    print(f\"   X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "\n",
    "    resamplers = [\n",
    "        (\"smote\", SMOTE(random_state=RANDOM_STATE)),\n",
    "        (\"borderlinesmote\", BorderlineSMOTE(random_state=RANDOM_STATE, kind=\"borderline-1\")),\n",
    "        (\"adasyn\", ADASYN(random_state=RANDOM_STATE)),\n",
    "    ]\n",
    "\n",
    "    for nombre_resampler, resampler in resamplers:\n",
    "\n",
    "        print(f\"   üîÅ Aplicando {nombre_resampler} ...\")\n",
    "\n",
    "        try:\n",
    "            X_res, y_res = resampler.fit_resample(X_train.values, y_train.values)\n",
    "        except ValueError as e:\n",
    "            print(f\"   ‚ö†Ô∏è {nombre_resampler} no gener√≥ muestras sint√©ticas: {e}\")\n",
    "            print(f\"      ‚Üí Se omite guardar {nombre_resampler}_{nombre_dataset}_*.csv\")\n",
    "            continue\n",
    "\n",
    "        # ----------------------------\n",
    "        # Cantidad de sint√©ticos\n",
    "        # ----------------------------\n",
    "        n_res = len(X_res)\n",
    "        cant_sinteticos = n_res - n_original\n",
    "\n",
    "        # Archivo con patr√≥n nuevo\n",
    "        nombre_archivo = (\n",
    "            f\"{nombre_resampler}_{nombre_dataset}\"\n",
    "            f\"_I{percentil_isolation_etiqueta}\"\n",
    "            f\"_sg{cant_sinteticos}_train.csv\"\n",
    "        )\n",
    "        path_salida = os.path.join(ruta_clasicos, nombre_archivo)\n",
    "\n",
    "        # Evitar sobrescritura si no corresponde\n",
    "        if not overwrite and os.path.exists(path_salida):\n",
    "            print(f\"   ‚ö™ Omitido ({nombre_resampler}), ya existe: {nombre_archivo}\")\n",
    "            continue\n",
    "\n",
    "        # ----------------------------\n",
    "        # Guardar CSV\n",
    "        # ----------------------------\n",
    "        df_res = pd.DataFrame(X_res, columns=columnas)\n",
    "        df_res[col_target] = y_res\n",
    "\n",
    "        df_res.to_csv(path_salida, index=False)\n",
    "        print(f\"   ‚úÖ Guardado: {path_salida} (sg={cant_sinteticos})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adbfd800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "def dibujarDistribuciones4_por_clase(\n",
    "    listas_entropias,\n",
    "    listas_mascara_entropia_baja,\n",
    "    listas_mascara_vecino_minoritario,\n",
    "    listas_mascara_pureza,\n",
    "    listas_densidades,\n",
    "    listas_riesgos,\n",
    "    etiquetas_clases,\n",
    "    nombre_dataset,\n",
    "    etiqueta_configuracion,\n",
    "    percentil_entropia=None,\n",
    "    percentil_densidad=None,\n",
    "    percentil_riesgo=None,\n",
    "    carpeta_salida=\"../datasets/datasets_aumentados/logs/pcsmote/distribuciones\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Figura por dataset:\n",
    "      filas   = n¬∫ de clases OVA\n",
    "      columnas= [m√°scaras binarias, hist entrop√≠a, densidad, riesgo]\n",
    "    \"\"\"\n",
    "\n",
    "    numero_clases = len(etiquetas_clases)\n",
    "\n",
    "    # ========================\n",
    "    # DEBUG INICIAL\n",
    "    # ========================\n",
    "    print(\"\\n[DEBUG GRAFICADOR PCSMOTE POR CLASE]\")\n",
    "    print(f\"  Dataset: {nombre_dataset}\")\n",
    "    print(f\"  Config : {etiqueta_configuracion}\")\n",
    "    print(f\"  N¬∫ clases: {numero_clases}\")\n",
    "\n",
    "    if not (\n",
    "        len(listas_entropias) == len(listas_mascara_entropia_baja)\n",
    "        == len(listas_mascara_vecino_minoritario)\n",
    "        == len(listas_mascara_pureza)\n",
    "        == len(listas_densidades)\n",
    "        == len(listas_riesgos)\n",
    "        == numero_clases\n",
    "    ):\n",
    "        raise ValueError(\"Todas las listas deben tener la misma cantidad de clases.\")\n",
    "\n",
    "    for i, etiqueta in enumerate(etiquetas_clases):\n",
    "        print(\n",
    "            f\"  clase {etiqueta}: \"\n",
    "            f\"len(ent)={len(listas_entropias[i])}, \"\n",
    "            f\"len(mask_ent)={len(listas_mascara_entropia_baja[i])}, \"\n",
    "            f\"len(mask_vec)={len(listas_mascara_vecino_minoritario[i])}, \"\n",
    "            f\"len(mask_pur)={len(listas_mascara_pureza[i])}, \"\n",
    "            f\"len(dens)={len(listas_densidades[i])}, \"\n",
    "            f\"len(riesgo)={len(listas_riesgos[i])}\"\n",
    "        )\n",
    "\n",
    "    # ========================\n",
    "    # SETUP FIGURA\n",
    "    # ========================\n",
    "    carpeta = Path(carpeta_salida)\n",
    "    carpeta.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        numero_clases,\n",
    "        4,\n",
    "        figsize=(22, 3.8 * numero_clases),\n",
    "        squeeze=False,\n",
    "    )\n",
    "\n",
    "    # Dejamos buen margen izquierdo y derecho global,\n",
    "    # y algo de espacio vertical/horizontal entre subplots\n",
    "    fig.subplots_adjust(left=0.12, right=0.13, wspace=0.5, hspace=0.12)\n",
    "\n",
    "    # ========================\n",
    "    # LOOP POR CLASE\n",
    "    # ========================\n",
    "    for i in range(numero_clases):\n",
    "        etiqueta_clase = etiquetas_clases[i]\n",
    "\n",
    "        # --- vectores num√©ricos ---\n",
    "        ent = np.asarray(listas_entropias[i], dtype=float)\n",
    "        ent = ent[~np.isnan(ent)]\n",
    "\n",
    "        dens = np.asarray(listas_densidades[i], dtype=float)\n",
    "        dens = dens[~np.isnan(dens)]\n",
    "\n",
    "        rie = np.asarray(listas_riesgos[i], dtype=float)\n",
    "        rie = rie[~np.isnan(rie)]\n",
    "\n",
    "        # --- m√°scaras binarias ---\n",
    "        mask_ent = np.asarray(listas_mascara_entropia_baja[i], dtype=float)\n",
    "        mask_vec = np.asarray(listas_mascara_vecino_minoritario[i], dtype=float)\n",
    "        mask_pur = np.asarray(listas_mascara_pureza[i], dtype=float)\n",
    "\n",
    "        c_ent = int(np.sum(mask_ent == 1))\n",
    "        c_vec = int(np.sum(mask_vec == 1))\n",
    "        c_pur = int(np.sum(mask_pur == 1))\n",
    "\n",
    "        # =====================================================\n",
    "        # Columna 0: M√ÅSCARAS BINARIAS (barras 0/1 con f=...)\n",
    "        # =====================================================\n",
    "        ax0 = axes[i, 0]\n",
    "\n",
    "        x = np.array([0, 1, 2])\n",
    "        cantidades = [c_ent, c_vec, c_pur]\n",
    "        etiquetas_barras = [\"Entrop√≠a baja\", \"Vecino min.\", \"Pureza\"]\n",
    "        colores_barras = [\"C0\", \"green\", \"orange\"]\n",
    "\n",
    "        max_f = max(cantidades) if cantidades else 0\n",
    "        offset = max_f * 0.06 if max_f > 0 else 0.5\n",
    "\n",
    "        for j in range(3):\n",
    "            ax0.bar(\n",
    "                x[j],\n",
    "                cantidades[j],\n",
    "                width=0.6,\n",
    "                alpha=0.8,\n",
    "                edgecolor=\"black\",\n",
    "                color=colores_barras[j],\n",
    "            )\n",
    "            ax0.text(\n",
    "                x[j],\n",
    "                cantidades[j] + offset,\n",
    "                f\"f={cantidades[j]}\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                fontsize=9,\n",
    "            )\n",
    "\n",
    "        ax0.set_xticks(x)\n",
    "        ax0.set_xticklabels(etiquetas_barras, rotation=0)\n",
    "        ax0.set_ylabel(\"Frecuencia\")\n",
    "        ax0.set_title(\"m√°scaras (binario)\", loc=\"center\", y=1.08)\n",
    "\n",
    "        # Leyenda a la derecha, sin pisar el siguiente subplot\n",
    "        box0 = ax0.get_position()\n",
    "        ax0.set_position([box0.x0, box0.y0, box0.width * 0.8, box0.height])\n",
    "        ax0.legend(\n",
    "            [\"Entrop√≠a baja\", \"Vecino minoritario\", \"Pureza (AND)\"],\n",
    "            loc=\"center left\",\n",
    "            bbox_to_anchor=(1.02, 0.5),\n",
    "            borderaxespad=0.0,\n",
    "            fontsize=8,\n",
    "        )\n",
    "\n",
    "        # =====================================================\n",
    "        # Columna 1: ENTROP√çA (histograma + percentil)\n",
    "        # =====================================================\n",
    "        ax1 = axes[i, 1]\n",
    "\n",
    "        if ent.size > 0:\n",
    "            counts_e, bins_e, _ = ax1.hist(\n",
    "                ent,\n",
    "                bins=30,\n",
    "                density=False,\n",
    "                alpha=0.5,\n",
    "                edgecolor=\"black\",\n",
    "                label=\"Entrop√≠a\",\n",
    "            )\n",
    "\n",
    "            if percentil_entropia is not None:\n",
    "                umbral_e = float(np.percentile(ent, percentil_entropia))\n",
    "                ax1.axvline(\n",
    "                    umbral_e,\n",
    "                    color=\"red\",\n",
    "                    linewidth=2,\n",
    "                    label=f\"P{percentil_entropia:.1f} = {umbral_e:.4f}\",\n",
    "                )\n",
    "\n",
    "                # f del bin donde cae el percentil\n",
    "                idx_bin = np.digitize([umbral_e], bins_e) - 1\n",
    "                idx_bin = int(np.clip(idx_bin[0], 0, len(counts_e) - 1))\n",
    "                f_bin = int(counts_e[idx_bin])\n",
    "\n",
    "                ax1.text(\n",
    "                    umbral_e,\n",
    "                    f_bin + max(counts_e) * 0.06,\n",
    "                    f\"f={f_bin}\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"bottom\",\n",
    "                    fontsize=9,\n",
    "                )\n",
    "        else:\n",
    "            ax1.text(0.5, 0.5, \"Sin datos\", ha=\"center\", va=\"center\")\n",
    "\n",
    "        ax1.set_title(\"entrop√≠a\", loc=\"center\", y=1.08)\n",
    "        ax1.set_xlabel(\"entrop√≠a\")\n",
    "        ax1.set_ylabel(\"Frecuencia\")\n",
    "\n",
    "        box1 = ax1.get_position()\n",
    "        ax1.set_position([box1.x0, box1.y0, box1.width * 0.8, box1.height])\n",
    "        ax1.legend(\n",
    "            loc=\"center left\",\n",
    "            bbox_to_anchor=(1.02, 0.5),\n",
    "            borderaxespad=0.0,\n",
    "            fontsize=8,\n",
    "        )\n",
    "\n",
    "        # =====================================================\n",
    "        # Columna 2: DENSIDAD\n",
    "        # =====================================================\n",
    "        ax2 = axes[i, 2]\n",
    "\n",
    "        if dens.size > 0:\n",
    "            counts_d, _, _ = ax2.hist(\n",
    "                dens,\n",
    "                bins=30,\n",
    "                density=False,\n",
    "                alpha=0.5,\n",
    "                edgecolor=\"black\",\n",
    "                label=\"Densidad\",\n",
    "            )\n",
    "\n",
    "            if percentil_densidad is not None:\n",
    "                umbral_d = float(np.percentile(dens, percentil_densidad))\n",
    "                ax2.axvline(\n",
    "                    umbral_d,\n",
    "                    color=\"red\",\n",
    "                    linewidth=2,\n",
    "                    label=f\"P{percentil_densidad:.1f} = {umbral_d:.4f}\",\n",
    "                )\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, \"Sin datos\", ha=\"center\", va=\"center\")\n",
    "\n",
    "        ax2.set_title(\"densidad\", loc=\"center\", y=1.08)\n",
    "        ax2.set_xlabel(\"densidad\")\n",
    "        ax2.set_ylabel(\"Frecuencia\")\n",
    "\n",
    "        box2 = ax2.get_position()\n",
    "        ax2.set_position([box2.x0, box2.y0, box2.width * 0.8, box2.height])\n",
    "        ax2.legend(\n",
    "            loc=\"center left\",\n",
    "            bbox_to_anchor=(1.02, 0.5),\n",
    "            borderaxespad=0.0,\n",
    "            fontsize=8,\n",
    "        )\n",
    "\n",
    "        # =====================================================\n",
    "        # Columna 3: RIESGO\n",
    "        # =====================================================\n",
    "        ax3 = axes[i, 3]\n",
    "\n",
    "        if rie.size > 0:\n",
    "            counts_r, _, _ = ax3.hist(\n",
    "                rie,\n",
    "                bins=30,\n",
    "                density=False,\n",
    "                alpha=0.5,\n",
    "                edgecolor=\"black\",\n",
    "                label=\"Riesgo\",\n",
    "            )\n",
    "\n",
    "            if percentil_riesgo is not None:\n",
    "                umbral_r = float(np.percentile(rie, percentil_riesgo))\n",
    "                ax3.axvline(\n",
    "                    umbral_r,\n",
    "                    color=\"red\",\n",
    "                    linewidth=2,\n",
    "                    label=f\"P{percentil_riesgo:.1f} = {umbral_r:.4f}\",\n",
    "                )\n",
    "        else:\n",
    "            ax3.text(0.5, 0.5, \"Sin datos\", ha=\"center\", va=\"center\")\n",
    "\n",
    "        ax3.set_title(\"riesgo\", loc=\"center\", y=1.08)\n",
    "        ax3.set_xlabel(\"riesgo\")\n",
    "        ax3.set_ylabel(\"Frecuencia\")\n",
    "\n",
    "        box3 = ax3.get_position()\n",
    "        ax3.set_position([box3.x0, box3.y0, box3.width * 0.8, box3.height])\n",
    "        ax3.legend(\n",
    "            loc=\"center left\",\n",
    "            bbox_to_anchor=(1.02, 0.5),\n",
    "            borderaxespad=0.0,\n",
    "            fontsize=8,\n",
    "        )\n",
    "\n",
    "        # ===== t√≠tulo de fila: clase =====\n",
    "        axes[i, 0].text(\n",
    "            -0.22,\n",
    "            0.5,\n",
    "            f\"clase {etiqueta_clase}\",\n",
    "            transform=axes[i, 0].transAxes,\n",
    "            rotation=90,\n",
    "            va=\"center\",\n",
    "            ha=\"right\",\n",
    "            fontsize=11,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    # ========================\n",
    "    # T√çTULO Y GUARDADO\n",
    "    # ========================\n",
    "    fig.suptitle(f\"{nombre_dataset} | {etiqueta_configuracion}\", fontsize=14)\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.94])\n",
    "\n",
    "    fname = f\"distrib_4x_clases_{nombre_dataset}_{etiqueta_configuracion}.png\"\n",
    "    ruta_salida = carpeta / fname\n",
    "    fig.savefig(ruta_salida, dpi=150)\n",
    "    plt.close(fig)\n",
    " \n",
    "\n",
    "    print(f\"üñº  Distribuciones por CLASE guardadas en: {ruta_salida}\")\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98760b9d",
   "metadata": {},
   "source": [
    "### üß¨ Aumento de Datasets mediante T√©cnicas de Sobremuestreo\n",
    "\n",
    "En esta etapa se genera una versi√≥n balanceada de cada dataset original mediante la aplicaci√≥n de t√©cnicas de sobremuestreo, con el objetivo de mitigar el desbalance de clases antes del entrenamiento de los modelos.\n",
    "\n",
    "Actualmente, se emplea la t√©cnica:\n",
    "\n",
    "- `PCSMOTE` (Percentile-Controlled SMOTE), que permite controlar la generaci√≥n de muestras sint√©ticas en funci√≥n de percentiles de densidad, riesgo y pureza.\n",
    "\n",
    "Para cada dataset, se exploran combinaciones espec√≠ficas de par√°metros seg√∫n la t√©cnica utilizada. Los datasets resultantes se almacenan en el directorio `datasets/datasets_aumentados/`, utilizando nombres de archivo que reflejan la configuraci√≥n empleada (por ejemplo: `pcsmote_nombre_D25_R50_Pentropia_train.csv`).\n",
    "\n",
    "> ‚ö†Ô∏è Esta fase no incluye entrenamiento ni validaci√≥n de modelos. Su √∫nico prop√≥sito es generar conjuntos de datos aumentados a partir del conjunto de entrenamiento. La partici√≥n `train/test` se realiza previamente, y **solo la parte de entrenamiento es sometida a sobremuestreo**. El conjunto de prueba permanece sin modificar para garantizar una evaluaci√≥n imparcial posterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da2fb544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# DENSIDAD (radio por percentil) y umbral de densidad\n",
    "# -------------------------\n",
    "PERCENTILES_RADIO_DENSIDAD = [85]          # PRD\n",
    "UMBRALES_DENSIDAD = [0.50]         # UD\n",
    "\n",
    "# -------------------------\n",
    "# RIESGO (radio por percentil) y umbral de riesgo\n",
    "# -------------------------\n",
    "PERCENTILES_RADIO_RIESGO = [50]        # PR\n",
    "UMBRALES_RIESGO = [0.55]                 # UR\n",
    "\n",
    "# -------------------------\n",
    "# PUREZA\n",
    "# -------------------------\n",
    "# proporci√≥n (secundaria / apoyo)\n",
    "# Bajamos el piso para no matar semillas en alta dimensi√≥n\n",
    "UMBRALES_PROPORCION = [0.45]             # Upp\n",
    "\n",
    "# entrop√≠a (principal)\n",
    "PERCENTILES_ENTROPIA = [60]        # PE\n",
    "\n",
    "# -------------------------\n",
    "# Isolation\n",
    "# -------------------------\n",
    "# Mantener bajo para no destruir vecindarios √∫tiles\n",
    "PERCENTILES_ISOLATION = [0, 1]\n",
    "        # I\n",
    "\n",
    "percentiles_radio_densidad = []\n",
    "percentiles_riesgo = []\n",
    "criterios_pureza = []\n",
    "umbrales_densidad = []\n",
    "percentiles_entropia = []\n",
    "umbrales_pureza_proporcion = []\n",
    "umbral_riesgo = []\n",
    "percentiles_isolation = []\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# FAMILIA 1: PUREZA = PROPORCION (PRINCIPAL)\n",
    "# -------------------------\n",
    "for prd in PERCENTILES_RADIO_DENSIDAD:\n",
    "    for ud in UMBRALES_DENSIDAD:\n",
    "        for pr in PERCENTILES_RADIO_RIESGO:\n",
    "            for ur in UMBRALES_RIESGO:\n",
    "                for upp in UMBRALES_PROPORCION:\n",
    "                    for iso in PERCENTILES_ISOLATION:\n",
    "\n",
    "                        percentiles_radio_densidad.append(prd)\n",
    "                        percentiles_riesgo.append(pr)\n",
    "                        criterios_pureza.append(\"proporcion\")\n",
    "                        umbrales_densidad.append(ud)\n",
    "                        percentiles_entropia.append(None)\n",
    "                        umbrales_pureza_proporcion.append(upp)\n",
    "                        umbral_riesgo.append(ur)\n",
    "                        percentiles_isolation.append(iso)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# FAMILIA 2: PUREZA = ENTROPIA (SECUNDARIA)\n",
    "# (solo contraste, muy acotada)\n",
    "# -------------------------\n",
    "for prd in PERCENTILES_RADIO_DENSIDAD:\n",
    "    for ud in UMBRALES_DENSIDAD:\n",
    "        for pr in [40]:               # fijo para no inflar el espacio\n",
    "            for ur in [0.45]:         # fijo (punto medio)\n",
    "                for pe in PERCENTILES_ENTROPIA:\n",
    "                    for iso in [0, 1]:  # acotado\n",
    "\n",
    "                        percentiles_radio_densidad.append(prd)\n",
    "                        percentiles_riesgo.append(pr)\n",
    "                        criterios_pureza.append(\"entropia\")\n",
    "                        umbrales_densidad.append(ud)\n",
    "                        percentiles_entropia.append(pe)\n",
    "                        umbrales_pureza_proporcion.append(None)\n",
    "                        umbral_riesgo.append(ur)\n",
    "                        percentiles_isolation.append(iso)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15d7ac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def actualizar_eta(\n",
    "    idx_actual,\n",
    "    total,\n",
    "    duracion_iteracion,\n",
    "    estado_eta\n",
    "):\n",
    "    \"\"\"\n",
    "    Actualiza un promedio m√≥vil de duraci√≥n por iteraci√≥n y estima ETA.\n",
    "\n",
    "    Par√°metros\n",
    "    ----------\n",
    "    idx_actual : int\n",
    "        Iteraci√≥n actual (1-based).\n",
    "    total : int\n",
    "        Total de iteraciones.\n",
    "    duracion_iteracion : float\n",
    "        Tiempo (en segundos) que tard√≥ la iteraci√≥n actual.\n",
    "    estado_eta : dict\n",
    "        Estado mutable con:\n",
    "            - 'promedio_movil'\n",
    "            - 'alpha'\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    dict con:\n",
    "        - promedio_movil\n",
    "        - eta_segundos\n",
    "        - restante\n",
    "    \"\"\"\n",
    "\n",
    "    alpha = estado_eta[\"alpha\"]\n",
    "\n",
    "    if estado_eta[\"promedio_movil\"] is None:\n",
    "        promedio_movil = duracion_iteracion\n",
    "    else:\n",
    "        promedio_movil = (\n",
    "            alpha * duracion_iteracion\n",
    "            + (1.0 - alpha) * estado_eta[\"promedio_movil\"]\n",
    "        )\n",
    "\n",
    "    estado_eta[\"promedio_movil\"] = promedio_movil\n",
    "\n",
    "    restantes = total - idx_actual\n",
    "    eta_segundos = restantes * promedio_movil\n",
    "\n",
    "    return {\n",
    "        \"promedio_movil\": promedio_movil,\n",
    "        \"eta_segundos\": eta_segundos,\n",
    "        \"restantes\": restantes,\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9f74ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ Dataset: predict_faults\n",
      "\n",
      "   ‚ñ∂ Ejecuci√≥n (1/4) [ 25.00%] | D=85 | R=50 | P=proporcion | udensidad=0.5 | pentropia=None | uprop=0.45 | I=0\n",
      "[predict_faults] Split: X_train=(8000, 5), X_test=(2000, 5)\n",
      "‚úÖ Caso base generado para predict_faults\n",
      "   Train: ../datasets/datasets_aumentados/base/predict_faults_I0_tm8000_train.csv  (rows=8000)\n",
      "   Test : ../datasets/datasets_aumentados/base/predict_faults_tdataset10000_tm2000_test.csv   (rows=2000)\n",
      "üü¶ Caso base generado (I0):\n",
      " - Train: ../datasets/datasets_aumentados/base/predict_faults_I0_tm8000_train.csv\n",
      " - Test: ../datasets/datasets_aumentados/base/predict_faults_tdataset10000_tm2000_test.csv\n",
      "üîß Aumentaci√≥n cl√°sica (en memoria) para: predict_faults\n",
      "   X_train shape: (8000, 5), y_train shape: (8000,)\n",
      "   üîÅ Aplicando smote ...\n",
      "   ‚ö™ Omitido (smote), ya existe: smote_predict_faults_I0_sg38332_train.csv\n",
      "   üîÅ Aplicando borderlinesmote ...\n",
      "   ‚ö™ Omitido (borderlinesmote), ya existe: borderlinesmote_predict_faults_I0_sg38332_train.csv\n",
      "   üîÅ Aplicando adasyn ...\n",
      "   ‚ö™ Omitido (adasyn), ya existe: adasyn_predict_faults_I0_sg38311_train.csv\n",
      "   üü¢ PCSMOTE guardado: ../datasets/datasets_aumentados/pcs_predict_faults_PRD85_PR50_CPprop_UD050_Upp045_UR055_I0_SV051_SG22938_train.csv (sg=22938)\n",
      "\n",
      "   ‚ñ∂ Ejecuci√≥n (2/4) [ 50.00%] | D=85 | R=50 | P=proporcion | udensidad=0.5 | pentropia=None | uprop=0.45 | I=1\n",
      "[predict_faults] Split: X_train=(8000, 5), X_test=(2000, 5)\n",
      "[predict_faults] Aplicando IsolationCleaner (percentil=1%) sobre TRAIN\n",
      "üßπ IF (por_clase, percentil=1.0%): removidos 83; quedan 7917 de 8000.\n",
      "[predict_faults] Limpieza IF (percentil): removidos=83 / total_inicial‚âà8000 (train_final=7917)\n",
      "‚úÖ Caso base generado para predict_faults\n",
      "   Train: ../datasets/datasets_aumentados/base/predict_faults_I1_tm7917_train.csv  (rows=7917)\n",
      "   Test : ../datasets/datasets_aumentados/base/predict_faults_tdataset10000_tm2000_test.csv   (rows=2000)\n",
      "üü¶ Caso base generado (I1):\n",
      " - Train: ../datasets/datasets_aumentados/base/predict_faults_I1_tm7917_train.csv\n",
      " - Test: ../datasets/datasets_aumentados/base/predict_faults_tdataset10000_tm2000_test.csv\n",
      "üîß Aumentaci√≥n cl√°sica (en memoria) para: predict_faults\n",
      "   X_train shape: (7917, 5), y_train shape: (7917,)\n",
      "   üîÅ Aplicando smote ...\n",
      "   ‚ö™ Omitido (smote), ya existe: smote_predict_faults_I1_sg37947_train.csv\n",
      "   üîÅ Aplicando borderlinesmote ...\n",
      "   ‚ö™ Omitido (borderlinesmote), ya existe: borderlinesmote_predict_faults_I1_sg37947_train.csv\n",
      "   üîÅ Aplicando adasyn ...\n",
      "   ‚ö™ Omitido (adasyn), ya existe: adasyn_predict_faults_I1_sg37956_train.csv\n",
      "   üü¢ PCSMOTE guardado: ../datasets/datasets_aumentados/pcs_predict_faults_PRD85_PR50_CPprop_UD050_Upp045_UR055_I1_SV054_SG22707_train.csv (sg=22707)\n",
      "\n",
      "   ‚ñ∂ Ejecuci√≥n (3/4) [ 75.00%] | D=85 | R=40 | P=entropia | udensidad=0.5 | pentropia=60 | uprop=None | I=0\n",
      "[predict_faults] Split: X_train=(8000, 5), X_test=(2000, 5)\n",
      "‚úÖ Caso base generado para predict_faults\n",
      "   Train: ../datasets/datasets_aumentados/base/predict_faults_I0_tm8000_train.csv  (rows=8000)\n",
      "   Test : ../datasets/datasets_aumentados/base/predict_faults_tdataset10000_tm2000_test.csv   (rows=2000)\n",
      "üü¶ Caso base generado (I0):\n",
      " - Train: ../datasets/datasets_aumentados/base/predict_faults_I0_tm8000_train.csv\n",
      " - Test: ../datasets/datasets_aumentados/base/predict_faults_tdataset10000_tm2000_test.csv\n",
      "üîß Aumentaci√≥n cl√°sica (en memoria) para: predict_faults\n",
      "   X_train shape: (8000, 5), y_train shape: (8000,)\n",
      "   üîÅ Aplicando smote ...\n",
      "   ‚ö™ Omitido (smote), ya existe: smote_predict_faults_I0_sg38332_train.csv\n",
      "   üîÅ Aplicando borderlinesmote ...\n",
      "   ‚ö™ Omitido (borderlinesmote), ya existe: borderlinesmote_predict_faults_I0_sg38332_train.csv\n",
      "   üîÅ Aplicando adasyn ...\n",
      "   ‚ö™ Omitido (adasyn), ya existe: adasyn_predict_faults_I0_sg38311_train.csv\n",
      "   üü¢ PCSMOTE guardado: ../datasets/datasets_aumentados/pcs_predict_faults_PRD85_PR40_CPent_UD050_PE60_UR045_I0_SV030_SG22938_train.csv (sg=22938)\n",
      "\n",
      "   ‚ñ∂ Ejecuci√≥n (4/4) [100.00%] | D=85 | R=40 | P=entropia | udensidad=0.5 | pentropia=60 | uprop=None | I=1\n",
      "[predict_faults] Split: X_train=(8000, 5), X_test=(2000, 5)\n",
      "[predict_faults] Aplicando IsolationCleaner (percentil=1%) sobre TRAIN\n",
      "üßπ IF (por_clase, percentil=1.0%): removidos 83; quedan 7917 de 8000.\n",
      "[predict_faults] Limpieza IF (percentil): removidos=83 / total_inicial‚âà8000 (train_final=7917)\n",
      "‚úÖ Caso base generado para predict_faults\n",
      "   Train: ../datasets/datasets_aumentados/base/predict_faults_I1_tm7917_train.csv  (rows=7917)\n",
      "   Test : ../datasets/datasets_aumentados/base/predict_faults_tdataset10000_tm2000_test.csv   (rows=2000)\n",
      "üü¶ Caso base generado (I1):\n",
      " - Train: ../datasets/datasets_aumentados/base/predict_faults_I1_tm7917_train.csv\n",
      " - Test: ../datasets/datasets_aumentados/base/predict_faults_tdataset10000_tm2000_test.csv\n",
      "üîß Aumentaci√≥n cl√°sica (en memoria) para: predict_faults\n",
      "   X_train shape: (7917, 5), y_train shape: (7917,)\n",
      "   üîÅ Aplicando smote ...\n",
      "   ‚ö™ Omitido (smote), ya existe: smote_predict_faults_I1_sg37947_train.csv\n",
      "   üîÅ Aplicando borderlinesmote ...\n",
      "   ‚ö™ Omitido (borderlinesmote), ya existe: borderlinesmote_predict_faults_I1_sg37947_train.csv\n",
      "   üîÅ Aplicando adasyn ...\n",
      "   ‚ö™ Omitido (adasyn), ya existe: adasyn_predict_faults_I1_sg37956_train.csv\n",
      "   üü¢ PCSMOTE guardado: ../datasets/datasets_aumentados/pcs_predict_faults_PRD85_PR40_CPent_UD050_PE60_UR045_I1_SV031_SG22707_train.csv (sg=22707)\n",
      "üìÑ Log por MUESTRA volcado a CSV (final):\n",
      "    ..\\datasets\\datasets_aumentados\\logs\\pcsmote\\por_muestras\\log_pcsmote_x_muestra_predict_faults.csv\n",
      "\n",
      "‚úÖ XLSX final generado:\n",
      "    ..\\datasets\\datasets_aumentados\\logs\\pcsmote\\por_muestras\\log_pcsmote_x_muestra_predict_faults.xlsx\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "combinaciones = list(zip(\n",
    "    percentiles_radio_densidad,\n",
    "    percentiles_riesgo,\n",
    "    criterios_pureza,\n",
    "    umbrales_densidad,\n",
    "    percentiles_entropia,\n",
    "    umbrales_pureza_proporcion,\n",
    "    umbral_riesgo,\n",
    "    percentiles_isolation,\n",
    "))\n",
    "\n",
    "datasets_a_ignorar = {\n",
    "    \"shuttle\",\n",
    "    \"iris\",\n",
    "    \"glass\",\n",
    "    \"heart\",\n",
    "    \"wdbc\",\n",
    "    \"ecoli\",\n",
    "    \"us_crime\",\n",
    "    # \"predict_faults\",\n",
    "    \"gear_vibration\",\n",
    "    \"telco_churn\"\n",
    "}\n",
    "\n",
    "# Asumimos que solo us√°s [0, 1, 3]\n",
    "ISOS_VALIDOS = {0, 1, 3}\n",
    "\n",
    "def _hms_desde_segundos(segundos):\n",
    "    segundos = int(segundos)\n",
    "    h = segundos // 3600\n",
    "    m = (segundos % 3600) // 60\n",
    "    s = segundos % 60\n",
    "    return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
    "\n",
    "# ‚úÖ instancia de Utils (si ya ten√©s una global, us√° esa)\n",
    "utils = Utils()\n",
    "\n",
    "for nombre_dataset, config in config_datasets.items():\n",
    "    if nombre_dataset in datasets_a_ignorar:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nüìÅ Dataset: {nombre_dataset}\")\n",
    "\n",
    "    total_configs = len(combinaciones)\n",
    "\n",
    "    # ‚úÖ ETA separada por isolation (I=0,1,3)\n",
    "    estado_eta_por_iso = {\n",
    "        0: {\"promedio_movil\": None, \"alpha\": 0.20},\n",
    "        1: {\"promedio_movil\": None, \"alpha\": 0.20},\n",
    "        3: {\"promedio_movil\": None, \"alpha\": 0.20},\n",
    "    }\n",
    "\n",
    "    # ‚úÖ acumuladores por iso: promedios por bloque de 10 PARA CADA ISO\n",
    "    acumulado_segundos_por_iso = {0: 0.0, 1: 0.0, 3: 0.0}\n",
    "    acumulado_iters_por_iso = {0: 0, 1: 0, 3: 0}\n",
    "\n",
    "    # ‚úÖ contadores de cu√°ntas configs totales hay por iso (para ponderar ETA)\n",
    "    total_por_iso = {0: 0, 1: 0, 3: 0}\n",
    "    for (_, _, _, _, _, _, _, iso) in combinaciones:\n",
    "        iso_int = int(iso)\n",
    "        if iso_int in total_por_iso:\n",
    "            total_por_iso[iso_int] += 1\n",
    "\n",
    "    # ‚úÖ contadores de progreso por iso (cu√°ntas ya se hicieron por iso)\n",
    "    hechas_por_iso = {0: 0, 1: 0, 3: 0}\n",
    "\n",
    "    # =======================================================\n",
    "    # ‚úÖ LOG POR MUESTRA: buffer externo -> CSV -> XLSX al final\n",
    "    # =======================================================\n",
    "    base_logs = Path(\"../datasets/datasets_aumentados/logs/pcsmote/por_muestras\")\n",
    "    base_logs.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    log_path_csv  = base_logs / Utils.safe_token(f\"log_pcsmote_x_muestra_{nombre_dataset}.csv\")\n",
    "    log_path_xlsx = base_logs / Utils.safe_token(f\"log_pcsmote_x_muestra_{nombre_dataset}.xlsx\")\n",
    "\n",
    "    # ‚úÖ corrida limpia de logs\n",
    "    utils.borrar_archivo_log(log_path_csv)\n",
    "    utils.borrar_archivo_log(log_path_xlsx)\n",
    "\n",
    "    # ‚úÖ buffer externo (cero p√©rdida). Guardamos dicts de filas.\n",
    "    buffer_logs_por_muestra = []\n",
    "    TAM_BLOQUE_LOG = 10  # cada 10 configuraciones volcamos a CSV\n",
    "\n",
    "    # (opcional) contador de configuraciones con sampler v√°lido (para flush por bloque)\n",
    "    configs_desde_ultimo_flush = 0\n",
    "\n",
    "    for idx, (prdens, priesgo, criterio, udens, pentropia, upproporcion, uriesgo, p_isol) in enumerate(combinaciones, start=1):\n",
    "\n",
    "        iso_int = int(p_isol)\n",
    "        if iso_int not in ISOS_VALIDOS:\n",
    "            raise ValueError(f\"Se esperaba percentil_isolation en {sorted(ISOS_VALIDOS)}, pero lleg√≥: {p_isol}\")\n",
    "\n",
    "        porcentaje = (idx / total_configs) * 100.0\n",
    "        print(\n",
    "            f\"\\n   ‚ñ∂ Ejecuci√≥n ({idx}/{total_configs}) \"\n",
    "            f\"[{porcentaje:6.2f}%] | \"\n",
    "            f\"D={prdens} | R={priesgo} | P={criterio} | \"\n",
    "            f\"udensidad={udens} | pentropia={pentropia} | \"\n",
    "            f\"uprop={upproporcion} | I={iso_int}\"\n",
    "        )\n",
    "\n",
    "        t_ini = time.perf_counter()\n",
    "\n",
    "        try:\n",
    "            # Caso base para este percentil de IsolationForest\n",
    "            base_train, base_test, X_test_base, y_test_base, idx_train = generar_caso_base(\n",
    "                nombre_dataset,\n",
    "                config,\n",
    "                porcentaje_limpieza=iso_int,\n",
    "            )\n",
    "            print(f\"üü¶ Caso base generado (I{iso_int}):\\n - Train: {base_train}\\n - Test: {base_test}\")\n",
    "\n",
    "            col_target = config.get(\"col_target\", \"target\")\n",
    "            df_base_train = pd.read_csv(base_train)\n",
    "\n",
    "            if col_target not in df_base_train.columns:\n",
    "                raise ValueError(f\"La columna target '{col_target}' no est√° en {base_train}\")\n",
    "\n",
    "            X_train_df = df_base_train.drop(columns=[col_target])\n",
    "            y_train_series = df_base_train[col_target]\n",
    "\n",
    "            X_train_base = X_train_df.values\n",
    "            y_train_base = y_train_series.values\n",
    "\n",
    "            # Cl√°sicos\n",
    "            generar_aumentaciones_clasicas_y_guardar(\n",
    "                nombre_dataset=nombre_dataset,\n",
    "                X_train=X_train_df,\n",
    "                y_train=y_train_series,\n",
    "                col_target=col_target,\n",
    "                ruta_clasicos=RUTA_CLASICOS,\n",
    "                overwrite=False,\n",
    "                percentil_isolation_etiqueta=iso_int,\n",
    "            )\n",
    "\n",
    "            # PCSMOTE\n",
    "            df_header, df_log, sampler = aumentar_dataset_pcsmote_y_guardar(\n",
    "                nombre_dataset               = nombre_dataset,\n",
    "                X_train_base                 = X_train_base,\n",
    "                y_train_base                 = y_train_base,\n",
    "\n",
    "                percentil_radio_densidad     = prdens,\n",
    "                percentil_entropia           = pentropia,\n",
    "                percentil_riesgo             = priesgo,\n",
    "                criterio_pureza              = criterio,\n",
    "                umbral_densidad              = udens,\n",
    "                umbral_pureza_proporcion     = upproporcion,\n",
    "                umbral_riesgo                = uriesgo,\n",
    "                percentil_isolation_etiqueta = iso_int,\n",
    "\n",
    "                col_target                   = col_target,\n",
    "                X_test_base                  = X_test_base,\n",
    "                y_test_base                  = y_test_base,\n",
    "                idx_train                    = idx_train,\n",
    "            )\n",
    "\n",
    "            if sampler is None:\n",
    "                print(\"‚ùå Fall√≥ la generaci√≥n con PCSMOTE.\")\n",
    "                continue\n",
    "\n",
    "            # =======================================================\n",
    "            # ‚úÖ BUFFER EXTERNO: recolectar filas desde sampler.logs_por_muestra\n",
    "            # =======================================================\n",
    "            logs_iteracion = getattr(sampler, \"logs_por_muestra\", None)\n",
    "            if logs_iteracion:\n",
    "                # logs_por_muestra es lista de dicts (seg√∫n tu clase)\n",
    "                for fila in logs_iteracion:\n",
    "                    buffer_logs_por_muestra.append(fila)\n",
    "\n",
    "            configs_desde_ultimo_flush += 1\n",
    "\n",
    "            # ‚úÖ FLUSH cada 10 configs (append a CSV, r√°pido)\n",
    "            if configs_desde_ultimo_flush >= TAM_BLOQUE_LOG:\n",
    "                if buffer_logs_por_muestra:\n",
    "                    # volcamos buffer a CSV usando el m√©todo del sampler (reutiliza tu estilo)\n",
    "                    sampler.logs_por_muestra = buffer_logs_por_muestra\n",
    "                    sampler.exportar_log_muestras_csv(\n",
    "                        ruta_csv=log_path_csv,\n",
    "                        append=True,\n",
    "                    )\n",
    "\n",
    "                    # limpiar buffer externo + limpiar logs internos por prolijidad\n",
    "                    buffer_logs_por_muestra = []\n",
    "                    sampler.limpiar_logs_por_muestra()\n",
    "\n",
    "                    print(f\"üìÑ Log por MUESTRA volcado a CSV (bloque):\\n    {log_path_csv}\\n\")\n",
    "\n",
    "                configs_desde_ultimo_flush = 0\n",
    "\n",
    "        finally:\n",
    "            t_fin = time.perf_counter()\n",
    "            duracion = t_fin - t_ini\n",
    "\n",
    "            # ‚úÖ progreso por iso\n",
    "            hechas_por_iso[iso_int] += 1\n",
    "\n",
    "            # ‚úÖ acumular bloque por iso\n",
    "            acumulado_segundos_por_iso[iso_int] += duracion\n",
    "            acumulado_iters_por_iso[iso_int] += 1\n",
    "\n",
    "            # ‚úÖ cada 10 ITERACIONES DE ESE ISO: actualizar su ETA\n",
    "            if acumulado_iters_por_iso[iso_int] >= 10:\n",
    "                prom_bloque_iso = acumulado_segundos_por_iso[iso_int] / acumulado_iters_por_iso[iso_int]\n",
    "\n",
    "                actualizar_eta(\n",
    "                    idx_actual=hechas_por_iso[iso_int],\n",
    "                    total=total_por_iso[iso_int],\n",
    "                    duracion_iteracion=prom_bloque_iso,\n",
    "                    estado_eta=estado_eta_por_iso[iso_int],\n",
    "                )\n",
    "\n",
    "                # reset bloque por iso\n",
    "                acumulado_segundos_por_iso[iso_int] = 0.0\n",
    "                acumulado_iters_por_iso[iso_int] = 0\n",
    "\n",
    "            # ‚úÖ imprimir ETA global ponderada cada 10 configs globales\n",
    "            if idx % 10 == 0:\n",
    "                eta_total = 0.0\n",
    "                detalle = []\n",
    "\n",
    "                for iso_k in [0, 1, 3]:\n",
    "                    promedio_movil = estado_eta_por_iso[iso_k][\"promedio_movil\"]\n",
    "\n",
    "                    restantes_iso = total_por_iso[iso_k] - hechas_por_iso[iso_k]\n",
    "                    if restantes_iso < 0:\n",
    "                        restantes_iso = 0\n",
    "\n",
    "                    if promedio_movil is None:\n",
    "                        detalle.append(f\"I{iso_k}: sin base | rest={restantes_iso}\")\n",
    "                        continue\n",
    "\n",
    "                    eta_iso = restantes_iso * promedio_movil\n",
    "                    eta_total += eta_iso\n",
    "                    detalle.append(f\"I{iso_k}: {promedio_movil:.2f}s | rest={restantes_iso}\")\n",
    "\n",
    "                print(\n",
    "                    \"   ‚è±Ô∏è ETA ponderada por I | \"\n",
    "                    f\"ETA aprox: {_hms_desde_segundos(eta_total)} | \"\n",
    "                    f\"{idx}/{total_configs} | \"\n",
    "                    + \" | \".join(detalle)\n",
    "                )\n",
    "\n",
    "    # =======================================================\n",
    "    # ‚úÖ FLUSH FINAL (cero p√©rdida): volcar lo que quede en buffer\n",
    "    # =======================================================\n",
    "    if buffer_logs_por_muestra:\n",
    "        # creamos un objeto \"contenedor\" para reutilizar el m√©todo de export\n",
    "        # Si tu exportar_log_muestras_csv est√° en sampler y no en utils, necesitamos un sampler.\n",
    "        # Como esta rama es final, si el √∫ltimo sampler no existi√≥, usamos un UtilsLogDummy m√≠nimo.\n",
    "\n",
    "        try:\n",
    "            # Si existe alg√∫n sampler en scope y tiene el m√©todo, lo usamos\n",
    "            if \"sampler\" in locals() and sampler is not None and hasattr(sampler, \"exportar_log_muestras_csv\"):\n",
    "                sampler.logs_por_muestra = buffer_logs_por_muestra\n",
    "                sampler.exportar_log_muestras_csv(ruta_csv=log_path_csv, append=True)\n",
    "                sampler.limpiar_logs_por_muestra()\n",
    "            else:\n",
    "                # fallback: usar Utils para exportar desde un DataFrame (sin perder datos)\n",
    "                df_flush = pd.DataFrame(buffer_logs_por_muestra)\n",
    "                existe = log_path_csv.exists()\n",
    "                df_flush.to_csv(\n",
    "                    log_path_csv,\n",
    "                    mode=\"a\",\n",
    "                    index=False,\n",
    "                    header=(not existe),\n",
    "                    encoding=\"utf-8\",\n",
    "                )\n",
    "\n",
    "            print(f\"üìÑ Log por MUESTRA volcado a CSV (final):\\n    {log_path_csv}\\n\")\n",
    "\n",
    "        finally:\n",
    "            buffer_logs_por_muestra = []\n",
    "\n",
    "    # =======================================================\n",
    "    # ‚úÖ CONVERSI√ìN FINAL: CSV -> XLSX (una vez)\n",
    "    # =======================================================\n",
    "    utils.convertir_csv_a_excel(\n",
    "        ruta_csv=log_path_csv,\n",
    "        ruta_excel=log_path_xlsx,\n",
    "    )\n",
    "    print(f\"‚úÖ XLSX final generado:\\n    {log_path_xlsx}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
