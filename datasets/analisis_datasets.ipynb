{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7389a834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Analizando dataset: US_CRIME\n",
      "üéØ Valores √∫nicos del target: [-1, 1]\n",
      "üìä Distribuci√≥n de clases:\n",
      "   - -1: 1844 (92.48%)\n",
      "   - 1: 150 (7.52%)\n",
      "‚úÖ Clase minoritaria real: 1\n",
      "‚ö†Ô∏è Clase configurada como minoritaria: 1\n",
      "\n",
      "üîç Analizando dataset: SHUTTLE\n",
      "üéØ Valores √∫nicos del target: [1, 4, 5, 3, 2, 7, 6]\n",
      "üìä Distribuci√≥n de clases:\n",
      "   - 1: 45586 (78.6%)\n",
      "   - 4: 8903 (15.35%)\n",
      "   - 5: 3267 (5.63%)\n",
      "   - 3: 171 (0.29%)\n",
      "   - 2: 50 (0.09%)\n",
      "   - 7: 13 (0.02%)\n",
      "   - 6: 10 (0.02%)\n",
      "‚úÖ Clase minoritaria real: 6\n",
      "‚ö†Ô∏è Clase configurada como minoritaria: 6\n",
      "\n",
      "üîç Analizando dataset: WDBC\n",
      "üéØ Valores √∫nicos del target: ['B', 'M']\n",
      "üìä Distribuci√≥n de clases:\n",
      "   - B: 357 (62.74%)\n",
      "   - M: 212 (37.26%)\n",
      "‚úÖ Clase minoritaria real: M\n",
      "‚ö†Ô∏è Clase configurada como minoritaria: M\n",
      "\n",
      "üîç Analizando dataset: GLASS\n",
      "üéØ Valores √∫nicos del target: [2, 1, 7, 3, 5, 6]\n",
      "üìä Distribuci√≥n de clases:\n",
      "   - 2: 76 (35.51%)\n",
      "   - 1: 70 (32.71%)\n",
      "   - 7: 29 (13.55%)\n",
      "   - 3: 17 (7.94%)\n",
      "   - 5: 13 (6.07%)\n",
      "   - 6: 9 (4.21%)\n",
      "‚úÖ Clase minoritaria real: 6\n",
      "‚ö†Ô∏è Clase configurada como minoritaria: 6\n",
      "\n",
      "üîç Analizando dataset: HEART\n",
      "üéØ Valores √∫nicos del target: [0, 1, 2, 3, 4]\n",
      "üìä Distribuci√≥n de clases:\n",
      "   - 0: 164 (54.13%)\n",
      "   - 1: 55 (18.15%)\n",
      "   - 2: 36 (11.88%)\n",
      "   - 3: 35 (11.55%)\n",
      "   - 4: 13 (4.29%)\n",
      "‚úÖ Clase minoritaria real: 4\n",
      "‚ö†Ô∏è Clase configurada como minoritaria: 4\n",
      "\n",
      "üîç Analizando dataset: IRIS\n",
      "üéØ Valores √∫nicos del target: ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
      "üìä Distribuci√≥n de clases:\n",
      "   - Iris-setosa: 50 (33.33%)\n",
      "   - Iris-versicolor: 50 (33.33%)\n",
      "   - Iris-virginica: 50 (33.33%)\n",
      "‚úÖ Clase minoritaria real: Iris-setosa\n",
      "‚ö†Ô∏è Clase configurada como minoritaria: None\n",
      "‚ÑπÔ∏è No se defini√≥ clase minoritaria (modo multiclase o imagen).\n",
      "\n",
      "üîç Analizando dataset: ECOLI\n",
      "üéØ Valores √∫nicos del target: ['cp', 'im', 'pp', 'imU', 'om', 'omL', 'imS', 'imL']\n",
      "üìä Distribuci√≥n de clases:\n",
      "   - cp: 143 (42.56%)\n",
      "   - im: 77 (22.92%)\n",
      "   - pp: 52 (15.48%)\n",
      "   - imU: 35 (10.42%)\n",
      "   - om: 20 (5.95%)\n",
      "   - omL: 5 (1.49%)\n",
      "   - imS: 2 (0.6%)\n",
      "   - imL: 2 (0.6%)\n",
      "‚úÖ Clase minoritaria real: imS\n",
      "‚ö†Ô∏è Clase configurada como minoritaria: imL\n",
      "üö® POSIBLE ERROR DE CONFIGURACI√ìN ‚ùó\n",
      "\n",
      "üîç Analizando dataset: PREDICT_FAULTS\n",
      "üéØ Valores √∫nicos del target: ['No Failure', 'Heat Dissipation Failure', 'Power Failure', 'Overstrain Failure', 'Tool Wear Failure', 'Random Failures']\n",
      "üìä Distribuci√≥n de clases:\n",
      "   - No Failure: 9652 (96.52%)\n",
      "   - Heat Dissipation Failure: 112 (1.12%)\n",
      "   - Power Failure: 95 (0.95%)\n",
      "   - Overstrain Failure: 78 (0.78%)\n",
      "   - Tool Wear Failure: 45 (0.45%)\n",
      "   - Random Failures: 18 (0.18%)\n",
      "‚úÖ Clase minoritaria real: Random Failures\n",
      "‚ö†Ô∏è Clase configurada como minoritaria: Random Failures\n",
      "\n",
      "üîç Analizando dataset: GEAR_VIBRATION\n",
      "‚ùå Error al analizar gear_vibration: ‚ùå X contiene NaN o infinitos luego del preprocesamiento.\n",
      "\n",
      "üìÅ An√°lisis guardado en: resultados/reporte_distribucion_2025-12-11_0357.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from config_datasets import config_datasets\n",
    "from cargar_dataset import cargar_dataset, graficar_distribucion_clases\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "# Crear carpeta de resultados si no existe\n",
    "Path(\"resultados\").mkdir(exist_ok=True)\n",
    "Path(\"figuras\").mkdir(exist_ok=True)\n",
    "\n",
    "# Nombre de archivo con fecha y hora\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H%M\")\n",
    "nombre_archivo = f\"resultados/reporte_distribucion_{timestamp}.txt\"\n",
    "\n",
    "# Lista de l√≠neas para guardar en el archivo\n",
    "lineas_resultado = []\n",
    "\n",
    "for nombre, cfg in config_datasets.items():\n",
    "    lineas_resultado.append(f\"\\nüîç Analizando dataset: {nombre.upper()}\")\n",
    "    print(f\"\\nüîç Analizando dataset: {nombre.upper()}\")\n",
    "    try:\n",
    "        # Cargar el dataset (multiclase)\n",
    "        names = cfg.get(\"esquema\") if cfg.get(\"header\", None) is None else None\n",
    "\n",
    "        X, y, _ = cargar_dataset(\n",
    "            path=cfg.get(\"path\"),\n",
    "            clase_minoria=cfg.get(\"clase_minoria\"),\n",
    "            col_features=cfg.get(\"col_features\"),\n",
    "            col_target=cfg.get(\"col_target\"),\n",
    "            sep=cfg.get(\"sep\", \",\"),\n",
    "            header=cfg.get(\"header\", None),\n",
    "            binarizar=False,\n",
    "            tipo=cfg.get(\"tipo\", \"tabular\"),\n",
    "            impute=\"median\",\n",
    "            names=names\n",
    "        )\n",
    "\n",
    "\n",
    "        # Contar clases originales\n",
    "        conteo = pd.Series(y).value_counts()\n",
    "        clase_min_real = conteo.idxmin()\n",
    "        total = conteo.sum()\n",
    "        proporcion = (conteo / total * 100).round(2)\n",
    "\n",
    "        # Mostrar\n",
    "        print(\"üéØ Valores √∫nicos del target:\", list(conteo.index))\n",
    "        print(\"üìä Distribuci√≥n de clases:\")\n",
    "        # Guardar resultados\n",
    "        lineas_resultado.append(f\"üéØ Valores √∫nicos del target: {list(conteo.index)}\")\n",
    "        lineas_resultado.append(\"üìä Distribuci√≥n de clases:\")\n",
    "\n",
    "        for clase, count in conteo.items():\n",
    "            print(f\"   - {clase}: {count} ({proporcion[clase]}%)\")\n",
    "            lineas_resultado.append(f\"   - {clase}: {count} ({proporcion[clase]}%)\")\n",
    "\n",
    "        print(f\"‚úÖ Clase minoritaria real: {clase_min_real}\")\n",
    "        print(f\"‚ö†Ô∏è Clase configurada como minoritaria: {cfg['clase_minoria']}\")\n",
    "        lineas_resultado.append(f\"‚úÖ Clase minoritaria real: {clase_min_real}\")\n",
    "        lineas_resultado.append(f\"‚ö†Ô∏è Clase configurada como minoritaria: {cfg.get('clase_minoria')}\")\n",
    "\n",
    "        if \"clase_minoria\" in cfg and cfg[\"clase_minoria\"] is not None:\n",
    "            if clase_min_real != cfg[\"clase_minoria\"]:\n",
    "                print(\"üö® POSIBLE ERROR DE CONFIGURACI√ìN ‚ùó\")\n",
    "                lineas_resultado.append(\"üö® POSIBLE ERROR DE CONFIGURACI√ìN ‚ùó\")\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è No se defini√≥ clase minoritaria (modo multiclase o imagen).\")\n",
    "            lineas_resultado.append(\"‚ÑπÔ∏è No se defini√≥ clase minoritaria (modo multiclase o imagen).\")\n",
    "\n",
    "        # üîΩ Agregar gr√°fico descriptivo por dataset\n",
    "        nombre_figura = f\"figuras/{nombre.lower()}_distribucion_{timestamp}.png\"\n",
    "        graficar_distribucion_clases(y, nombre_dataset=nombre, guardar_en=nombre_figura)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al analizar {nombre}: {e}\")\n",
    "        lineas_resultado.append(f\"‚ùå Error al analizar {nombre}: {e}\")\n",
    "\n",
    "# Guardar a archivo\n",
    "with open(nombre_archivo, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lineas_resultado))\n",
    "\n",
    "print(f\"\\nüìÅ An√°lisis guardado en: {nombre_archivo}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9bcce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import shapiro, skew, kurtosis\n",
    "\n",
    "# --- Utilidades num√©ricas seguras ---\n",
    "def es_numerica(serie):\n",
    "    return np.issubdtype(serie.dtype, np.number)\n",
    "\n",
    "def obtener_matriz_correlacion_segura(df_numerico):\n",
    "    # Evita NaNs en correlaci√≥n\n",
    "    df_sin_nan = df_numerico.fillna(df_numerico.median(numeric_only=True))\n",
    "    return df_sin_nan.corr().values, list(df_numerico.columns)\n",
    "\n",
    "# --- M√©tricas de calidad del dataset ---\n",
    "def calcular_metricas_basicas_dataframe(X_df, y_series):\n",
    "    metricas = {}\n",
    "\n",
    "    # Filtrado num√©rico\n",
    "    columnas_numericas = [c for c in X_df.columns if es_numerica(X_df[c])]\n",
    "    X_num = X_df[columnas_numericas].copy()\n",
    "\n",
    "    # Tama√±os b√°sicos\n",
    "    metricas[\"cantidad_muestras\"] = int(X_df.shape[0])\n",
    "    metricas[\"cantidad_atributos\"] = int(X_df.shape[1])\n",
    "\n",
    "    # Faltantes y duplicados\n",
    "    total_celdas = int(X_df.shape[0] * X_df.shape[1])\n",
    "    cantidad_faltantes = int(X_df.isnull().sum().sum())\n",
    "    metricas[\"porcentaje_faltantes\"] = float((cantidad_faltantes / total_celdas) * 100.0)\n",
    "\n",
    "    cantidad_duplicados = int(X_df.duplicated().sum())\n",
    "    metricas[\"porcentaje_duplicados\"] = float((cantidad_duplicados / X_df.shape[0]) * 100.0)\n",
    "\n",
    "    # Distribuci√≥n de clases\n",
    "    valores_unicos, conteos = np.unique(y_series, return_counts=True)\n",
    "    cantidad_clases = int(len(valores_unicos))\n",
    "    indice_min = int(np.argmin(conteos))\n",
    "    indice_max = int(np.argmax(conteos))\n",
    "    clase_min = valores_unicos[indice_min]\n",
    "    clase_max = valores_unicos[indice_max]\n",
    "    n_min = int(conteos[indice_min])\n",
    "    n_max = int(conteos[indice_max])\n",
    "\n",
    "    metricas[\"cantidad_clases\"] = cantidad_clases\n",
    "    metricas[\"clase_minima_real\"] = str(clase_min)\n",
    "    metricas[\"clase_mayoritaria_real\"] = str(clase_max)\n",
    "    metricas[\"n_min\"] = n_min\n",
    "    metricas[\"n_max\"] = n_max\n",
    "    metricas[\"ratio_desequilibrio_max\"] = float(n_max / max(1, n_min))\n",
    "\n",
    "    # Entrop√≠a de clases y tama√±o efectivo\n",
    "    proporciones = conteos / conteos.sum()\n",
    "    entropia = float(-(proporciones * np.log(proporciones + 1e-12)).sum())\n",
    "    metricas[\"entropia_clases\"] = entropia\n",
    "    metricas[\"tamano_efectivo_clases\"] = float(np.exp(entropia))  # Effective Number of Classes\n",
    "\n",
    "    # Rango de escalas por columna (para sugerir scaler)\n",
    "    rangos = []\n",
    "    for nombre_col in columnas_numericas:\n",
    "        col = X_num[nombre_col].dropna()\n",
    "        if col.shape[0] > 0:\n",
    "            valor_min = float(np.min(col))\n",
    "            valor_max = float(np.max(col))\n",
    "            rango = float(valor_max - valor_min)\n",
    "            rangos.append(rango)\n",
    "    if len(rangos) > 0:\n",
    "        metricas[\"rango_mediano_variables\"] = float(np.median(rangos))\n",
    "        metricas[\"rango_maximo_variables\"] = float(np.max(rangos))\n",
    "    else:\n",
    "        metricas[\"rango_mediano_variables\"] = 0.0\n",
    "        metricas[\"rango_maximo_variables\"] = 0.0\n",
    "\n",
    "    # Asimetr√≠a y curtosis agregadas\n",
    "    skew_acumulado = []\n",
    "    kurt_acumulado = []\n",
    "    for nombre_col in columnas_numericas:\n",
    "        col = X_num[nombre_col].dropna().astype(float)\n",
    "        if col.shape[0] > 3:\n",
    "            skew_acumulado.append(float(skew(col)))\n",
    "            kurt_acumulado.append(float(kurtosis(col, fisher=True)))\n",
    "    if len(skew_acumulado) > 0:\n",
    "        metricas[\"asimetria_mediana\"] = float(np.median(skew_acumulado))\n",
    "        metricas[\"curtosis_mediana\"] = float(np.median(kurt_acumulado))\n",
    "    else:\n",
    "        metricas[\"asimetria_mediana\"] = 0.0\n",
    "        metricas[\"curtosis_mediana\"] = 0.0\n",
    "\n",
    "    # Normalidad (Shapiro-Wilk sobre una muestra si N>5000)\n",
    "    # Devuelve porcentaje de variables con p>0.05 (no se rechaza normalidad)\n",
    "    porcentaje_normalidad = 0.0\n",
    "    variables_evaluadas = 0\n",
    "    for nombre_col in columnas_numericas:\n",
    "        col = X_num[nombre_col].dropna().astype(float)\n",
    "        tam_col = col.shape[0]\n",
    "        if tam_col > 3:\n",
    "            if tam_col > 5000:\n",
    "                # muestreo simple para evitar l√≠mites de Shapiro\n",
    "                col = col.sample(5000, random_state=123).astype(float)\n",
    "            try:\n",
    "                estadistico, pvalor = shapiro(col.values)\n",
    "                variables_evaluadas += 1\n",
    "                if pvalor > 0.05:\n",
    "                    porcentaje_normalidad += 1.0\n",
    "            except Exception:\n",
    "                # si Shapiro falla en alguna variable, la omitimos\n",
    "                pass\n",
    "    if variables_evaluadas > 0:\n",
    "        metricas[\"porcentaje_variables_con_normalidad_no_rechazada\"] = float((porcentaje_normalidad / variables_evaluadas) * 100.0)\n",
    "    else:\n",
    "        metricas[\"porcentaje_variables_con_normalidad_no_rechazada\"] = 0.0\n",
    "\n",
    "    # Correlaci√≥n: m√°ximo |r| y % de pares con |r| > 0.9\n",
    "    if X_num.shape[1] >= 2:\n",
    "        matriz_corr, nombres = obtener_matriz_correlacion_segura(X_num)\n",
    "        valores_superior = []\n",
    "        cantidad_altamente_correl = 0\n",
    "        total_pares = 0\n",
    "\n",
    "        for i in range(len(nombres)):\n",
    "            for j in range(i + 1, len(nombres)):\n",
    "                r = float(matriz_corr[i, j])\n",
    "                valores_superior.append(abs(r))\n",
    "                total_pares += 1\n",
    "                if abs(r) > 0.9:\n",
    "                    cantidad_altamente_correl += 1\n",
    "\n",
    "        if len(valores_superior) > 0:\n",
    "            metricas[\"correlacion_absoluta_maxima\"] = float(np.max(valores_superior))\n",
    "            metricas[\"porcentaje_pares_correlacion_mayor_0_9\"] = float((cantidad_altamente_correl / total_pares) * 100.0)\n",
    "        else:\n",
    "            metricas[\"correlacion_absoluta_maxima\"] = 0.0\n",
    "            metricas[\"porcentaje_pares_correlacion_mayor_0_9\"] = 0.0\n",
    "    else:\n",
    "        metricas[\"correlacion_absoluta_maxima\"] = 0.0\n",
    "        metricas[\"porcentaje_pares_correlacion_mayor_0_9\"] = 0.0\n",
    "\n",
    "    # Sugerencia de escalado (reglas simples y expl√≠citas)\n",
    "    sugerencia_escalado = \"MinMaxScaler\"\n",
    "    if metricas[\"porcentaje_variables_con_normalidad_no_rechazada\"] > 60.0 and metricas[\"correlacion_absoluta_maxima\"] < 0.95:\n",
    "        sugerencia_escalado = \"StandardScaler\"\n",
    "    if metricas[\"asimetria_mediana\"] > 1.0 or metricas[\"curtosis_mediana\"] > 1.0:\n",
    "        sugerencia_escalado = \"RobustScaler\"\n",
    "    metricas[\"sugerencia_escalado\"] = sugerencia_escalado\n",
    "\n",
    "    return metricas\n",
    "\n",
    "# --- Detecci√≥n de outliers por IQR ---\n",
    "def calcular_porcentaje_outliers_por_variable(X_df):\n",
    "    resultado = {}\n",
    "    for nombre_columna in X_df.columns:\n",
    "        if es_numerica(X_df[nombre_columna]):\n",
    "            serie = X_df[nombre_columna].dropna().astype(float)\n",
    "            if serie.shape[0] > 0:\n",
    "                q1 = float(np.percentile(serie, 25))\n",
    "                q3 = float(np.percentile(serie, 75))\n",
    "                iqr = float(q3 - q1)\n",
    "                limite_inferior = q1 - 1.5 * iqr\n",
    "                limite_superior = q3 + 1.5 * iqr\n",
    "\n",
    "                cantidad = int(serie.shape[0])\n",
    "                cantidad_out = 0\n",
    "                indice = 0\n",
    "                valores = serie.values\n",
    "                while indice < cantidad:\n",
    "                    valor_actual = float(valores[indice])\n",
    "                    if valor_actual < limite_inferior or valor_actual > limite_superior:\n",
    "                        cantidad_out += 1\n",
    "                    indice += 1\n",
    "                porcentaje = float((cantidad_out / cantidad) * 100.0)\n",
    "                resultado[nombre_columna] = porcentaje\n",
    "    return resultado\n",
    "\n",
    "# --- Gr√°ficos simples y guardado ---\n",
    "def graficar_histograma_variables(X_df, nombre_dataset, ruta_base, max_columnas=12):\n",
    "    # Toma las primeras N num√©ricas para no explotar la figura\n",
    "    columnas = [c for c in X_df.columns if es_numerica(X_df[c])]\n",
    "    columnas = columnas[:max_columnas]\n",
    "    for nombre_col in columnas:\n",
    "        plt.figure()\n",
    "        valores = X_df[nombre_col].dropna().values.astype(float)\n",
    "        plt.hist(valores, bins=30)\n",
    "        plt.title(f\"{nombre_dataset} ¬∑ Histograma: {nombre_col}\")\n",
    "        plt.xlabel(nombre_col)\n",
    "        plt.ylabel(\"Frecuencia\")\n",
    "        ruta_salida = f\"{ruta_base}/{nombre_dataset}_hist_{nombre_col}.png\"\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(ruta_salida, dpi=140)\n",
    "        plt.close()\n",
    "\n",
    "def graficar_boxplot_variables(X_df, nombre_dataset, ruta_base, max_columnas=12):\n",
    "    columnas = []\n",
    "    for c in X_df.columns:\n",
    "        if es_numerica(X_df[c]):\n",
    "            columnas.append(c)\n",
    "    # Limitar cantidad para no generar demasiadas figuras\n",
    "    if len(columnas) > max_columnas:\n",
    "        columnas = columnas[:max_columnas]\n",
    "\n",
    "    for nombre_col in columnas:\n",
    "        # Preparar datos\n",
    "        serie = X_df[nombre_col].dropna()\n",
    "        valores = serie.values.astype(float)\n",
    "\n",
    "        # Crear figura\n",
    "        plt.figure()\n",
    "        # 1) NO pasamos labels/tick_labels (evita deprecations/errores entre versiones)\n",
    "        try:\n",
    "            plt.boxplot(valores, vert=True, showfliers=True)\n",
    "        except Exception as e:\n",
    "            # Fallback: dibujar un punto para no romper el loop\n",
    "            plt.plot([1], [valores[0] if valores.size>0 else 0], marker=\"o\")\n",
    "\n",
    "        # 2) Setear el tick X manualmente (compatible 100%)\n",
    "        plt.xticks([1], [nombre_col])\n",
    "\n",
    "        # 3) T√≠tulo y guardado\n",
    "        plt.title(f\"{nombre_dataset} ¬∑ Boxplot: {nombre_col}\")\n",
    "        ruta_salida = f\"{ruta_base}/{nombre_dataset}_box_{nombre_col}.png\"\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(ruta_salida, dpi=140)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def graficar_mapa_correlaciones(X_df, nombre_dataset, ruta_base):\n",
    "    columnas = [c for c in X_df.columns if es_numerica(X_df[c])]\n",
    "    if len(columnas) < 2:\n",
    "        return\n",
    "    X_num = X_df[columnas].fillna(X_df[columnas].median(numeric_only=True))\n",
    "    matriz_corr = X_num.corr().values\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(matriz_corr, aspect='auto', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(np.arange(len(columnas)), columnas, rotation=90)\n",
    "    plt.yticks(np.arange(len(columnas)), columnas)\n",
    "    plt.title(f\"{nombre_dataset} ¬∑ Matriz de correlaciones (Pearson)\")\n",
    "    plt.tight_layout()\n",
    "    ruta_salida = f\"{ruta_base}/{nombre_dataset}_correlaciones.png\"\n",
    "    plt.savefig(ruta_salida, dpi=140)\n",
    "    plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1268d60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé An√°lisis extendido: US_CRIME\n",
      "\n",
      "üîé An√°lisis extendido: SHUTTLE\n",
      "\n",
      "üîé An√°lisis extendido: WDBC\n",
      "\n",
      "üîé An√°lisis extendido: GLASS\n",
      "\n",
      "üîé An√°lisis extendido: HEART\n",
      "\n",
      "üîé An√°lisis extendido: IRIS\n",
      "‚ÑπÔ∏è No se defini√≥ clase minoritaria (modo multiclase o imagen).\n",
      "\n",
      "üîé An√°lisis extendido: ECOLI\n",
      "üö® POSIBLE ERROR DE CONFIGURACI√ìN ‚ùó\n",
      "\n",
      "üîé An√°lisis extendido: PREDICT_FAULTS\n",
      "\n",
      "üîé An√°lisis extendido: GEAR_VIBRATION\n",
      "‚ùå Error al analizar gear_vibration: ‚ùå X contiene NaN o infinitos luego del preprocesamiento.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "ruta_metricas_csv = f\"resultados/metricas_eda_{timestamp}.csv\"\n",
    "encabezados_csv = [\n",
    "    \"dataset\",\"cantidad_muestras\",\"cantidad_atributos\",\"cantidad_clases\",\n",
    "    \"clase_minima_real\",\"clase_mayoritaria_real\",\"n_min\",\"n_max\",\"ratio_desequilibrio_max\",\n",
    "    \"entropia_clases\",\"tamano_efectivo_clases\",\"porcentaje_faltantes\",\"porcentaje_duplicados\",\n",
    "    \"asimetria_mediana\",\"curtosis_mediana\",\"porcentaje_variables_con_normalidad_no_rechazada\",\n",
    "    \"correlacion_absoluta_maxima\",\"porcentaje_pares_correlacion_mayor_0_9\",\n",
    "    \"rango_mediano_variables\",\"rango_maximo_variables\",\"sugerencia_escalado\"\n",
    "]\n",
    "with open(ruta_metricas_csv, \"w\", newline=\"\", encoding=\"utf-8\") as fcsv:\n",
    "    escritor = csv.writer(fcsv)\n",
    "    escritor.writerow(encabezados_csv)\n",
    "\n",
    "for nombre, cfg in config_datasets.items():\n",
    "    lineas_resultado.append(f\"\\nüîé An√°lisis extendido: {nombre.upper()}\")\n",
    "    print(f\"\\nüîé An√°lisis extendido: {nombre.upper()}\")\n",
    "    try:\n",
    "        # === Cargar dataset (tu misma funci√≥n) ===\n",
    "        names = cfg.get(\"esquema\") if cfg.get(\"header\", None) is None else None\n",
    "\n",
    "        X, y, _ = cargar_dataset(\n",
    "            path=cfg.get(\"path\"),\n",
    "            clase_minoria=cfg.get(\"clase_minoria\"),\n",
    "            col_features=cfg.get(\"col_features\"),\n",
    "            col_target=cfg.get(\"col_target\"),\n",
    "            sep=cfg.get(\"sep\", \",\"),\n",
    "            header=cfg.get(\"header\", None),\n",
    "            binarizar=False,\n",
    "            tipo=cfg.get(\"tipo\", \"tabular\"),\n",
    "            impute=\"median\",\n",
    "            names=names\n",
    "        )\n",
    "        # === Reporte de clases (ya lo haces) ===\n",
    "        conteo = pd.Series(y).value_counts()\n",
    "        clase_min_real = conteo.idxmin()\n",
    "        total = int(conteo.sum())\n",
    "        proporcion = (conteo / total * 100).round(2)\n",
    "\n",
    "        lineas_resultado.append(f\"üéØ Valores √∫nicos del target: {list(conteo.index)}\")\n",
    "        lineas_resultado.append(\"üìä Distribuci√≥n de clases:\")\n",
    "        for clase, count in conteo.items():\n",
    "            lineas_resultado.append(f\"   - {clase}: {int(count)} ({float(proporcion[clase])}%)\")\n",
    "\n",
    "        lineas_resultado.append(f\"‚úÖ Clase minoritaria real: {clase_min_real}\")\n",
    "        lineas_resultado.append(f\"‚ö†Ô∏è Clase configurada como minoritaria: {cfg.get('clase_minoria')}\")\n",
    "\n",
    "        if \"clase_minoria\" in cfg and cfg[\"clase_minoria\"] is not None:\n",
    "            if clase_min_real != cfg[\"clase_minoria\"]:\n",
    "                aviso = \"üö® POSIBLE ERROR DE CONFIGURACI√ìN ‚ùó\"\n",
    "                print(aviso)\n",
    "                lineas_resultado.append(aviso)\n",
    "        else:\n",
    "            info = \"‚ÑπÔ∏è No se defini√≥ clase minoritaria (modo multiclase o imagen).\"\n",
    "            print(info)\n",
    "            lineas_resultado.append(info)\n",
    "\n",
    "        # === M√©tricas de calidad globales ===\n",
    "        X_df = pd.DataFrame(X, columns=[f\"col_{i}\" for i in range(X.shape[1])]) if not isinstance(X, pd.DataFrame) else X.copy()\n",
    "        y_series = pd.Series(y)\n",
    "\n",
    "        metricas = calcular_metricas_basicas_dataframe(X_df, y_series)\n",
    "        lineas_resultado.append(f\"üß™ M√©tricas clave: {metricas}\")\n",
    "\n",
    "        # Guardar m√©tricas al CSV\n",
    "        with open(ruta_metricas_csv, \"a\", newline=\"\", encoding=\"utf-8\") as fcsv:\n",
    "            escritor = csv.writer(fcsv)\n",
    "            fila = [\n",
    "                nombre,\n",
    "                metricas[\"cantidad_muestras\"],\n",
    "                metricas[\"cantidad_atributos\"],\n",
    "                metricas[\"cantidad_clases\"],\n",
    "                metricas[\"clase_minima_real\"],\n",
    "                metricas[\"clase_mayoritaria_real\"],\n",
    "                metricas[\"n_min\"],\n",
    "                metricas[\"n_max\"],\n",
    "                round(metricas[\"ratio_desequilibrio_max\"], 4),\n",
    "                round(metricas[\"entropia_clases\"], 4),\n",
    "                round(metricas[\"tamano_efectivo_clases\"], 4),\n",
    "                round(metricas[\"porcentaje_faltantes\"], 2),\n",
    "                round(metricas[\"porcentaje_duplicados\"], 2),\n",
    "                round(metricas[\"asimetria_mediana\"], 3),\n",
    "                round(metricas[\"curtosis_mediana\"], 3),\n",
    "                round(metricas[\"porcentaje_variables_con_normalidad_no_rechazada\"], 2),\n",
    "                round(metricas[\"correlacion_absoluta_maxima\"], 3),\n",
    "                round(metricas[\"porcentaje_pares_correlacion_mayor_0_9\"], 2),\n",
    "                round(metricas[\"rango_mediano_variables\"], 3),\n",
    "                round(metricas[\"rango_maximo_variables\"], 3),\n",
    "                metricas[\"sugerencia_escalado\"]\n",
    "            ]\n",
    "            escritor.writerow(fila)\n",
    "\n",
    "        # === Outliers por IQR (resumen) ===\n",
    "        porcentajes_outliers = calcular_porcentaje_outliers_por_variable(X_df)\n",
    "        # Reporta top 8 variables con mayor % de outliers\n",
    "        pares_ordenados = sorted(porcentajes_outliers.items(), key=lambda p: p[1], reverse=True)\n",
    "        top_out = pares_ordenados[:8]\n",
    "        lineas_resultado.append(\"üìå Top variables con mayor porcentaje de outliers (IQR):\")\n",
    "        for nombre_col, pct in top_out:\n",
    "            lineas_resultado.append(f\"   - {nombre_col}: {round(pct,2)}%\")\n",
    "\n",
    "        # === Figuras: hist, box, correlaci√≥n ===\n",
    "        carpeta_figuras_dataset = f\"figuras/{nombre.lower()}_{timestamp}\"\n",
    "        Path(carpeta_figuras_dataset).mkdir(exist_ok=True)\n",
    "\n",
    "        graficar_distribucion_clases(y, nombre_dataset=nombre, guardar_en=f\"{carpeta_figuras_dataset}/{nombre.lower()}_clases.png\")\n",
    "        graficar_histograma_variables(X_df, nombre, carpeta_figuras_dataset, max_columnas=12)\n",
    "        graficar_boxplot_variables(X_df, nombre, carpeta_figuras_dataset, max_columnas=12)\n",
    "        graficar_mapa_correlaciones(X_df, nombre, carpeta_figuras_dataset)\n",
    "\n",
    "    except Exception as e:\n",
    "        mensaje_error = f\"‚ùå Error al analizar {nombre}: {e}\"\n",
    "        print(mensaje_error)\n",
    "        lineas_resultado.append(mensaje_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31f10dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando (PRE): us_crime\n",
      "  Guardado PRE: ks_pre_us_crime.csv\n",
      "Procesando (PRE): shuttle\n",
      "  Guardado PRE: ks_pre_shuttle.csv\n",
      "Procesando (PRE): wdbc\n",
      "  Guardado PRE: ks_pre_wdbc.csv\n",
      "Procesando (PRE): glass\n",
      "  Guardado PRE: ks_pre_glass.csv\n",
      "Procesando (PRE): heart\n",
      "  Guardado PRE: ks_pre_heart.csv\n",
      "Procesando (PRE): iris\n",
      "  Guardado PRE: ks_pre_iris.csv\n",
      "Procesando (PRE): ecoli\n",
      "  Guardado PRE: ks_pre_ecoli.csv\n",
      "Procesando (PRE): predict_faults\n",
      "  Guardado PRE: ks_pre_predict_faults.csv\n",
      "Procesando (PRE): gear_vibration\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.83 MiB for an array with shape (120000,) and data type complex128",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# 1) Cargar dataset seg√∫n tu convenci√≥n\u001b[39;00m\n\u001b[32m     49\u001b[39m names = cfg.get(\u001b[33m\"\u001b[39m\u001b[33mesquema\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m cfg.get(\u001b[33m\"\u001b[39m\u001b[33mheader\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m X_np, y_np, _ = \u001b[43mcargar_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpath\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclase_minoria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mclase_minoria\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcol_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcol_features\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcol_target\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcol_target\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msep\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m,\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mheader\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbinarizar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtipo\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtipo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtabular\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimpute\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmedian\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# 2) Asegurar DataFrame con nombres de columnas\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X_np, pd.DataFrame):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Documentos_D\\TESIS\\armado tesina\\codigo\\datasets\\cargar_dataset.py:111\u001b[39m, in \u001b[36mcargar_dataset\u001b[39m\u001b[34m(path, clase_minoria, col_features, col_target, sep, header, binarizar, tipo, impute, na_values, dataset_name, names)\u001b[39m\n\u001b[32m    101\u001b[39m     df = pd.read_csv(\n\u001b[32m    102\u001b[39m         path,\n\u001b[32m    103\u001b[39m         header=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    108\u001b[39m         skipinitialspace=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    109\u001b[39m     )\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m        \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpython\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskipinitialspace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# --- Anti-encabezado duplicado (si se pas√≥ names y header=None) ---\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m usar_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m header \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m    935\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m    936\u001b[39m     dialect,\n\u001b[32m    937\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    944\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m    945\u001b[39m )\n\u001b[32m    946\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m--> \u001b[39m\u001b[32m948\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:617\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    614\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1748\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1741\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1743\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1744\u001b[39m     (\n\u001b[32m   1745\u001b[39m         index,\n\u001b[32m   1746\u001b[39m         columns,\n\u001b[32m   1747\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1748\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1749\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1750\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1751\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1752\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\python_parser.py:289\u001b[39m, in \u001b[36mPythonParser.read\u001b[39m\u001b[34m(self, rows)\u001b[39m\n\u001b[32m    286\u001b[39m alldata = \u001b[38;5;28mself\u001b[39m._rows_to_cols(content)\n\u001b[32m    287\u001b[39m data, columns = \u001b[38;5;28mself\u001b[39m._exclude_implicit_index(alldata)\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m conv_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    290\u001b[39m columns, conv_data = \u001b[38;5;28mself\u001b[39m._do_date_conversions(columns, conv_data)\n\u001b[32m    292\u001b[39m index, result_columns = \u001b[38;5;28mself\u001b[39m._make_index(\n\u001b[32m    293\u001b[39m     conv_data, alldata, columns, indexnamerow\n\u001b[32m    294\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\python_parser.py:360\u001b[39m, in \u001b[36mPythonParser._convert_data\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    357\u001b[39m     clean_na_values = \u001b[38;5;28mself\u001b[39m.na_values\n\u001b[32m    358\u001b[39m     clean_na_fvalues = \u001b[38;5;28mself\u001b[39m.na_fvalues\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_to_ndarrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclean_na_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclean_na_fvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclean_conv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclean_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\base_parser.py:579\u001b[39m, in \u001b[36mParserBase._convert_to_ndarrays\u001b[39m\u001b[34m(self, dct, na_values, na_fvalues, verbose, converters, dtypes)\u001b[39m\n\u001b[32m    576\u001b[39m try_num_bool = \u001b[38;5;129;01mnot\u001b[39;00m (cast_type \u001b[38;5;129;01mand\u001b[39;00m is_str_or_ea_dtype)\n\u001b[32m    578\u001b[39m \u001b[38;5;66;03m# general type inference and conversion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m cvals, na_count = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_infer_types\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcol_na_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m|\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol_na_fvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtry_num_bool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    584\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[38;5;66;03m# type specified in dtype param or cast_type is an EA\u001b[39;00m\n\u001b[32m    587\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cast_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\base_parser.py:710\u001b[39m, in \u001b[36mParserBase._infer_types\u001b[39m\u001b[34m(self, values, na_values, no_dtype_specified, try_num_bool)\u001b[39m\n\u001b[32m    707\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m try_num_bool \u001b[38;5;129;01mand\u001b[39;00m is_object_dtype(values.dtype):\n\u001b[32m    708\u001b[39m     \u001b[38;5;66;03m# exclude e.g DatetimeIndex here\u001b[39;00m\n\u001b[32m    709\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m710\u001b[39m         result, result_mask = \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaybe_convert_numeric\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m            \u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconvert_to_masked_nullable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_default_dtype_backend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]  # noqa: E501\u001b[39;49;00m\n\u001b[32m    715\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    716\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m    717\u001b[39m         \u001b[38;5;66;03m# e.g. encountering datetime string gets ValueError\u001b[39;00m\n\u001b[32m    718\u001b[39m         \u001b[38;5;66;03m#  TypeError can be raised in floatify\u001b[39;00m\n\u001b[32m    719\u001b[39m         na_count = parsers.sanitize_objects(values, na_values)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2283\u001b[39m, in \u001b[36mpandas._libs.lib.maybe_convert_numeric\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 1.83 MiB for an array with shape (120000,) and data type complex128"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import kstest\n",
    "\n",
    "# --- Funci√≥n expl√≠cita K-S por feature ---\n",
    "def kolmogorov_por_feature(X_df):\n",
    "    resultados = []\n",
    "    columnas = list(X_df.columns)\n",
    "    indice = 0\n",
    "    while indice < len(columnas):\n",
    "        nombre_columna = columnas[indice]\n",
    "        serie = X_df[nombre_columna].dropna()\n",
    "\n",
    "        media = float(serie.mean())\n",
    "        desvio = float(serie.std(ddof=1))\n",
    "\n",
    "        if desvio == 0.0 or np.isnan(desvio):\n",
    "            resultados.append({\n",
    "                \"feature\": nombre_columna,\n",
    "                \"D\": np.nan,\n",
    "                \"p_value\": np.nan,\n",
    "                \"normal_h0_p_ge_0_05\": False,\n",
    "                \"nota\": \"std=0 o NaN\"\n",
    "            })\n",
    "            indice += 1\n",
    "            continue\n",
    "\n",
    "        serie_std = (serie - media) / desvio\n",
    "        D, p = kstest(serie_std, \"norm\")\n",
    "        resultados.append({\n",
    "            \"feature\": nombre_columna,\n",
    "            \"D\": float(D),\n",
    "            \"p_value\": float(p),\n",
    "            \"normal_h0_p_ge_0_05\": bool(p >= 0.05),\n",
    "            \"nota\": \"\"\n",
    "        })\n",
    "        indice += 1\n",
    "\n",
    "    df = pd.DataFrame(resultados)\n",
    "    return df\n",
    "\n",
    "# --- Iteraci√≥n sobre todos los datasets definidos en config_datasets ---\n",
    "resumen_pre = []\n",
    "\n",
    "for nombre_dataset, cfg in config_datasets.items():\n",
    "    print(f\"Procesando (PRE): {nombre_dataset}\")\n",
    "\n",
    "    # 1) Cargar dataset seg√∫n tu convenci√≥n\n",
    "    names = cfg.get(\"esquema\") if cfg.get(\"header\", None) is None else None\n",
    "    X_np, y_np, _ = cargar_dataset(\n",
    "        path=cfg.get(\"path\"),\n",
    "        clase_minoria=cfg.get(\"clase_minoria\"),\n",
    "        col_features=cfg.get(\"col_features\"),\n",
    "        col_target=cfg.get(\"col_target\"),\n",
    "        sep=cfg.get(\"sep\", \",\"),\n",
    "        header=cfg.get(\"header\", None),\n",
    "        binarizar=False,\n",
    "        tipo=cfg.get(\"tipo\", \"tabular\"),\n",
    "        impute=\"median\",\n",
    "        names=names\n",
    "    )\n",
    "\n",
    "    # 2) Asegurar DataFrame con nombres de columnas\n",
    "    if isinstance(X_np, pd.DataFrame):\n",
    "        X_df = X_np.copy()\n",
    "    else:\n",
    "        nombres_columnas = cfg.get(\"esquema\") or [f\"feat_{i}\" for i in range(X_np.shape[1])]\n",
    "        X_df = pd.DataFrame(X_np, columns=nombres_columnas)\n",
    "\n",
    "    # 3) K-S PRE por feature\n",
    "    df_ks_pre = kolmogorov_por_feature(X_df)\n",
    "\n",
    "    # 4) Resumen por dataset\n",
    "    total_feats = int(len(df_ks_pre))\n",
    "    normales = int(df_ks_pre[\"normal_h0_p_ge_0_05\"].sum())\n",
    "    pct_normales = float(100.0 * normales / max(1, total_feats))\n",
    "    D_medio = float(df_ks_pre[\"D\"].dropna().mean()) if df_ks_pre[\"D\"].notna().any() else np.nan\n",
    "\n",
    "    resumen_pre.append({\n",
    "        \"dataset\": nombre_dataset,\n",
    "        \"features_evaluadas\": total_feats,\n",
    "        \"%_features_normales_pre\": pct_normales,\n",
    "        \"D_medio_pre\": D_medio\n",
    "    })\n",
    "\n",
    "    # 5) Guardado por dataset (opcional)\n",
    "    ruta_csv = f\"ks_pre_{nombre_dataset}.csv\"\n",
    "    df_ks_pre.to_csv(ruta_csv, index=False)\n",
    "    print(f\"  Guardado PRE: {ruta_csv}\")\n",
    "\n",
    "df_resumen_pre = pd.DataFrame(resumen_pre)\n",
    "df_resumen_pre.to_csv(\"ks_resumen_pre.csv\", index=False)\n",
    "df_resumen_pre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187fdcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === REPORTE VISUAL UNIFICADO (HTML) ===\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import shapiro, skew, kurtosis\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Configuraci√≥n general de salida\n",
    "# -------------------------------------------------------------------\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H%M\")\n",
    "carpeta_resultados = Path(f\"resultados_reporte_unificado_{timestamp}\")\n",
    "carpeta_figuras = carpeta_resultados / \"figuras\"\n",
    "carpeta_resultados.mkdir(exist_ok=True, parents=True)\n",
    "carpeta_figuras.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Utilidades base\n",
    "# -------------------------------------------------------------------\n",
    "def es_numerica(serie):\n",
    "    return np.issubdtype(serie.dtype, np.number)\n",
    "\n",
    "def obtener_matriz_correlacion_segura(df_numerico):\n",
    "    df_sin_nan = df_numerico.fillna(df_numerico.median(numeric_only=True))\n",
    "    return df_sin_nan.corr().values, list(df_numerico.columns)\n",
    "\n",
    "def calcular_metricas_basicas_dataframe(X_df, y_series):\n",
    "    metricas = {}\n",
    "    columnas_numericas = []\n",
    "    for nombre_col in X_df.columns:\n",
    "        if es_numerica(X_df[nombre_col]):\n",
    "            columnas_numericas.append(nombre_col)\n",
    "    X_num = X_df[columnas_numericas].copy()\n",
    "\n",
    "    metricas[\"cantidad_muestras\"] = int(X_df.shape[0])\n",
    "    metricas[\"cantidad_atributos\"] = int(X_df.shape[1])\n",
    "\n",
    "    total_celdas = int(X_df.shape[0] * X_df.shape[1])\n",
    "    cantidad_faltantes = int(X_df.isnull().sum().sum())\n",
    "    metricas[\"porcentaje_faltantes\"] = float((cantidad_faltantes / total_celdas) * 100.0) if total_celdas > 0 else 0.0\n",
    "\n",
    "    cantidad_duplicados = int(X_df.duplicated().sum())\n",
    "    metricas[\"porcentaje_duplicados\"] = float((cantidad_duplicados / X_df.shape[0]) * 100.0) if X_df.shape[0] > 0 else 0.0\n",
    "\n",
    "    valores_unicos, conteos = np.unique(y_series, return_counts=True)\n",
    "    cantidad_clases = int(len(valores_unicos))\n",
    "    indice_min = int(np.argmin(conteos))\n",
    "    indice_max = int(np.argmax(conteos))\n",
    "    clase_min = valores_unicos[indice_min] if cantidad_clases > 0 else \"\"\n",
    "    clase_max = valores_unicos[indice_max] if cantidad_clases > 0 else \"\"\n",
    "    n_min = int(conteos[indice_min]) if cantidad_clases > 0 else 0\n",
    "    n_max = int(conteos[indice_max]) if cantidad_clases > 0 else 0\n",
    "\n",
    "    metricas[\"cantidad_clases\"] = cantidad_clases\n",
    "    metricas[\"clase_minima_real\"] = str(clase_min)\n",
    "    metricas[\"clase_mayoritaria_real\"] = str(clase_max)\n",
    "    metricas[\"n_min\"] = n_min\n",
    "    metricas[\"n_max\"] = n_max\n",
    "    metricas[\"ratio_desequilibrio_max\"] = float(n_max / max(1, n_min)) if n_min > 0 else float(\"inf\")\n",
    "\n",
    "    proporciones = conteos / conteos.sum() if conteos.sum() > 0 else np.array([1.0])\n",
    "    entropia = float(-(proporciones * np.log(proporciones + 1e-12)).sum())\n",
    "    metricas[\"entropia_clases\"] = entropia\n",
    "    metricas[\"tamano_efectivo_clases\"] = float(np.exp(entropia))\n",
    "\n",
    "    rangos = []\n",
    "    for nombre_col in columnas_numericas:\n",
    "        col = X_num[nombre_col].dropna()\n",
    "        if col.shape[0] > 0:\n",
    "            valor_min = float(np.min(col))\n",
    "            valor_max = float(np.max(col))\n",
    "            rango = float(valor_max - valor_min)\n",
    "            rangos.append(rango)\n",
    "    if len(rangos) > 0:\n",
    "        metricas[\"rango_mediano_variables\"] = float(np.median(rangos))\n",
    "        metricas[\"rango_maximo_variables\"] = float(np.max(rangos))\n",
    "    else:\n",
    "        metricas[\"rango_mediano_variables\"] = 0.0\n",
    "        metricas[\"rango_maximo_variables\"] = 0.0\n",
    "\n",
    "    skew_acumulado = []\n",
    "    kurt_acumulado = []\n",
    "    for nombre_col in columnas_numericas:\n",
    "        col = X_num[nombre_col].dropna().astype(float)\n",
    "        if col.shape[0] > 3:\n",
    "            skew_acumulado.append(float(skew(col)))\n",
    "            kurt_acumulado.append(float(kurtosis(col, fisher=True)))\n",
    "    metricas[\"asimetria_mediana\"] = float(np.median(skew_acumulado)) if len(skew_acumulado) > 0 else 0.0\n",
    "    metricas[\"curtosis_mediana\"] = float(np.median(kurt_acumulado)) if len(kurt_acumulado) > 0 else 0.0\n",
    "\n",
    "    porcentaje_normalidad = 0.0\n",
    "    variables_evaluadas = 0\n",
    "    for nombre_col in columnas_numericas:\n",
    "        col = X_num[nombre_col].dropna().astype(float)\n",
    "        tam_col = col.shape[0]\n",
    "        if tam_col > 3:\n",
    "            if tam_col > 5000:\n",
    "                col = col.sample(5000, random_state=123).astype(float)\n",
    "            try:\n",
    "                estadistico, pvalor = shapiro(col.values)\n",
    "                variables_evaluadas += 1\n",
    "                if pvalor > 0.05:\n",
    "                    porcentaje_normalidad += 1.0\n",
    "            except Exception:\n",
    "                pass\n",
    "    if variables_evaluadas > 0:\n",
    "        metricas[\"porcentaje_variables_con_normalidad_no_rechazada\"] = float((porcentaje_normalidad / variables_evaluadas) * 100.0)\n",
    "    else:\n",
    "        metricas[\"porcentaje_variables_con_normalidad_no_rechazada\"] = 0.0\n",
    "\n",
    "    if X_num.shape[1] >= 2:\n",
    "        matriz_corr, nombres = obtener_matriz_correlacion_segura(X_num)\n",
    "        valores_superior = []\n",
    "        cantidad_altamente_correl = 0\n",
    "        total_pares = 0\n",
    "        for i in range(len(nombres)):\n",
    "            for j in range(i + 1, len(nombres)):\n",
    "                r = float(matriz_corr[i, j])\n",
    "                valores_superior.append(abs(r))\n",
    "                total_pares += 1\n",
    "                if abs(r) > 0.9:\n",
    "                    cantidad_altamente_correl += 1\n",
    "        if len(valores_superior) > 0:\n",
    "            metricas[\"correlacion_absoluta_maxima\"] = float(np.max(valores_superior))\n",
    "            metricas[\"porcentaje_pares_correlacion_mayor_0_9\"] = float((cantidad_altamente_correl / total_pares) * 100.0)\n",
    "        else:\n",
    "            metricas[\"correlacion_absoluta_maxima\"] = 0.0\n",
    "            metricas[\"porcentaje_pares_correlacion_mayor_0_9\"] = 0.0\n",
    "    else:\n",
    "        metricas[\"correlacion_absoluta_maxima\"] = 0.0\n",
    "        metricas[\"porcentaje_pares_correlacion_mayor_0_9\"] = 0.0\n",
    "\n",
    "    sugerencia_escalado = \"MinMaxScaler\"\n",
    "    if metricas[\"porcentaje_variables_con_normalidad_no_rechazada\"] > 60.0 and metricas[\"correlacion_absoluta_maxima\"] < 0.95:\n",
    "        sugerencia_escalado = \"StandardScaler\"\n",
    "    if metricas[\"asimetria_mediana\"] > 1.0 or metricas[\"curtosis_mediana\"] > 1.0:\n",
    "        sugerencia_escalado = \"RobustScaler\"\n",
    "    metricas[\"sugerencia_escalado\"] = sugerencia_escalado\n",
    "\n",
    "    return metricas\n",
    "\n",
    "def calcular_porcentaje_outliers_por_variable(X_df):\n",
    "    resultado = {}\n",
    "    for nombre_columna in X_df.columns:\n",
    "        if es_numerica(X_df[nombre_columna]):\n",
    "            serie = X_df[nombre_columna].dropna().astype(float)\n",
    "            if serie.shape[0] > 0:\n",
    "                q1 = float(np.percentile(serie, 25))\n",
    "                q3 = float(np.percentile(serie, 75))\n",
    "                iqr = float(q3 - q1)\n",
    "                limite_inferior = q1 - 1.5 * iqr\n",
    "                limite_superior = q3 + 1.5 * iqr\n",
    "                cantidad = int(serie.shape[0])\n",
    "                cantidad_out = 0\n",
    "                indice = 0\n",
    "                valores = serie.values\n",
    "                while indice < cantidad:\n",
    "                    valor_actual = float(valores[indice])\n",
    "                    if valor_actual < limite_inferior or valor_actual > limite_superior:\n",
    "                        cantidad_out += 1\n",
    "                    indice += 1\n",
    "                porcentaje = float((cantidad_out / cantidad) * 100.0)\n",
    "                resultado[nombre_columna] = porcentaje\n",
    "    return resultado\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Gr√°ficos (solo Matplotlib) ‚Äî compatibles con cualquier versi√≥n\n",
    "# -------------------------------------------------------------------\n",
    "def graficar_histograma_variables(X_df, nombre_dataset, ruta_base, max_columnas=12):\n",
    "    columnas_numericas = []\n",
    "    for c in X_df.columns:\n",
    "        if es_numerica(X_df[c]):\n",
    "            columnas_numericas.append(c)\n",
    "    if len(columnas_numericas) > max_columnas:\n",
    "        columnas_numericas = columnas_numericas[:max_columnas]\n",
    "    for nombre_col in columnas_numericas:\n",
    "        plt.figure()\n",
    "        valores = X_df[nombre_col].dropna().values.astype(float)\n",
    "        plt.hist(valores, bins=30)\n",
    "        plt.title(f\"{nombre_dataset} ¬∑ Histograma: {nombre_col}\")\n",
    "        plt.xlabel(nombre_col)\n",
    "        plt.ylabel(\"Frecuencia\")\n",
    "        ruta_salida = os.path.join(ruta_base, f\"{nombre_dataset}_hist_{nombre_col}.png\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(ruta_salida, dpi=140)\n",
    "        plt.close()\n",
    "\n",
    "def graficar_boxplot_variables(X_df, nombre_dataset, ruta_base, max_columnas=12):\n",
    "    columnas = []\n",
    "    for c in X_df.columns:\n",
    "        if es_numerica(X_df[c]):\n",
    "            columnas.append(c)\n",
    "    if len(columnas) > max_columnas:\n",
    "        columnas = columnas[:max_columnas]\n",
    "    for nombre_col in columnas:\n",
    "        plt.figure()\n",
    "        valores = X_df[nombre_col].dropna().values.astype(float)\n",
    "        try:\n",
    "            plt.boxplot(valores, vert=True, showfliers=True)  # sin labels/tick_labels\n",
    "        except Exception:\n",
    "            if valores.size > 0:\n",
    "                plt.plot([1], [valores[0]], marker=\"o\")\n",
    "        plt.xticks([1], [nombre_col])  # seteamos el tick manualmente\n",
    "        plt.title(f\"{nombre_dataset} ¬∑ Boxplot: {nombre_col}\")\n",
    "        ruta_salida = os.path.join(ruta_base, f\"{nombre_dataset}_box_{nombre_col}.png\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(ruta_salida, dpi=140)\n",
    "        plt.close()\n",
    "\n",
    "def graficar_mapa_correlaciones(X_df, nombre_dataset, ruta_base):\n",
    "    columnas = [c for c in X_df.columns if es_numerica(X_df[c])]\n",
    "    if len(columnas) < 2:\n",
    "        return\n",
    "    X_num = X_df[columnas].fillna(X_df[columnas].median(numeric_only=True))\n",
    "    matriz_corr = X_num.corr().values\n",
    "    plt.figure()\n",
    "    plt.imshow(matriz_corr, aspect='auto', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    # ticks compatibles con cualquier versi√≥n\n",
    "    posiciones = np.arange(len(columnas))\n",
    "    plt.xticks(posiciones, columnas, rotation=90)\n",
    "    plt.yticks(posiciones, columnas)\n",
    "    plt.title(f\"{nombre_dataset} ¬∑ Matriz de correlaciones (Pearson)\")\n",
    "    plt.tight_layout()\n",
    "    ruta_salida = os.path.join(ruta_base, f\"{nombre_dataset}_correlaciones.png\")\n",
    "    plt.savefig(ruta_salida, dpi=140)\n",
    "    plt.close()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# PCA (varianza explicada)\n",
    "# -------------------------------------------------------------------\n",
    "def calcular_varianza_explicada_pca(X_df, k=5):\n",
    "    columnas_num = []\n",
    "    for c in X_df.columns:\n",
    "        if es_numerica(X_df[c]):\n",
    "            columnas_num.append(c)\n",
    "    if len(columnas_num) < 2:\n",
    "        return []\n",
    "    X_num = X_df[columnas_num].fillna(X_df[columnas_num].median(numeric_only=True)).astype(float)\n",
    "    componentes = int(min(k, X_num.shape[1]))\n",
    "    pca = PCA(n_components=componentes, random_state=123)\n",
    "    pca.fit(X_num)\n",
    "    lista_var = []\n",
    "    for v in pca.explained_variance_ratio_:\n",
    "        lista_var.append(float(v))\n",
    "    return lista_var\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Generaci√≥n del reporte HTML por dataset\n",
    "# -------------------------------------------------------------------\n",
    "def generar_html_dataset(nombre_dataset, resumen_clases, metricas, varianza_pca, rutas_figuras, ruta_html_salida):\n",
    "    # Construcci√≥n manual del HTML (sin librer√≠as externas)\n",
    "    html = []\n",
    "    html.append(\"<html><head><meta charset='utf-8'><title>Reporte \" + str(nombre_dataset) + \"</title>\")\n",
    "    html.append(\"<style>body{font-family:Arial,Helvetica,sans-serif;max-width:1100px;margin:24px auto;padding:0 12px;}h1{margin-bottom:6px;}h2{margin-top:28px;}table{border-collapse:collapse;width:100%;}table,th,td{border:1px solid #ddd;}th,td{padding:8px;text-align:left;}code{background:#f5f5f5;padding:2px 4px;border-radius:4px;}</style>\")\n",
    "    html.append(\"</head><body>\")\n",
    "    html.append(\"<h1>Reporte de an√°lisis ‚Äî \" + str(nombre_dataset) + \"</h1>\")\n",
    "\n",
    "    # Resumen de clases\n",
    "    html.append(\"<h2>Distribuci√≥n de clases</h2>\")\n",
    "    html.append(\"<table><thead><tr><th>Clase</th><th>Conteo</th><th>Proporci√≥n (%)</th></tr></thead><tbody>\")\n",
    "    for fila in resumen_clases:  # lista de tuplas (clase, n, pct)\n",
    "        html.append(\"<tr><td>\" + str(fila[0]) + \"</td><td>\" + str(fila[1]) + \"</td><td>\" + str(round(fila[2],2)) + \"</td></tr>\")\n",
    "    html.append(\"</tbody></table>\")\n",
    "\n",
    "    # M√©tricas clave\n",
    "    html.append(\"<h2>M√©tricas clave del dataset</h2>\")\n",
    "    html.append(\"<table><tbody>\")\n",
    "    for k in [\n",
    "        \"cantidad_muestras\",\"cantidad_atributos\",\"cantidad_clases\",\n",
    "        \"clase_minima_real\",\"clase_mayoritaria_real\",\"n_min\",\"n_max\",\"ratio_desequilibrio_max\",\n",
    "        \"entropia_clases\",\"tamano_efectivo_clases\",\"porcentaje_faltantes\",\"porcentaje_duplicados\",\n",
    "        \"asimetria_mediana\",\"curtosis_mediana\",\"porcentaje_variables_con_normalidad_no_rechazada\",\n",
    "        \"correlacion_absoluta_maxima\",\"porcentaje_pares_correlacion_mayor_0_9\",\n",
    "        \"rango_mediano_variables\",\"rango_maximo_variables\",\"sugerencia_escalado\"\n",
    "    ]:\n",
    "        html.append(\"<tr><th>\"+str(k)+\"</th><td>\"+str(metricas.get(k,\"\"))+\"</td></tr>\")\n",
    "    html.append(\"</tbody></table>\")\n",
    "\n",
    "    # PCA\n",
    "    html.append(\"<h2>PCA ‚Äî Varianza explicada</h2>\")\n",
    "    if len(varianza_pca) > 0:\n",
    "        suma_2 = sum(varianza_pca[:2])\n",
    "        suma_3 = sum(varianza_pca[:3])\n",
    "        html.append(\"<p>PC1: <b>\"+str(round(varianza_pca[0],3))+\"</b> ¬∑ PC1+PC2: <b>\"+str(round(suma_2,3))+\"</b> ¬∑ PC1+PC2+PC3: <b>\"+str(round(suma_3,3))+\"</b></p>\")\n",
    "        html.append(\"<table><thead><tr><th>Componente</th><th>Varianza explicada</th></tr></thead><tbody>\")\n",
    "        indice = 0\n",
    "        while indice < len(varianza_pca):\n",
    "            html.append(\"<tr><td>PC\"+str(indice+1)+\"</td><td>\"+str(round(varianza_pca[indice],6))+\"</td></tr>\")\n",
    "            indice += 1\n",
    "        html.append(\"</tbody></table>\")\n",
    "    else:\n",
    "        html.append(\"<p>Insuficientes columnas num√©ricas para PCA.</p>\")\n",
    "\n",
    "    # Im√°genes\n",
    "    html.append(\"<h2>Figuras</h2>\")\n",
    "    for titulo, ruta in rutas_figuras:\n",
    "        rel = os.path.relpath(ruta, start=str(carpeta_resultados))\n",
    "        html.append(\"<h3>\"+titulo+\"</h3>\")\n",
    "        html.append(\"<img src='\"+rel+\"' style='max-width:100%;height:auto;border:1px solid #ddd;padding:4px;' />\")\n",
    "\n",
    "    html.append(\"</body></html>\")\n",
    "\n",
    "    with open(ruta_html_salida, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(html))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Orquestador principal\n",
    "# -------------------------------------------------------------------\n",
    "def generar_reporte_unificado(config_datasets, cargar_dataset, graficar_distribucion_clases):\n",
    "    ruta_index = carpeta_resultados / \"index.html\"\n",
    "    filas_index = []\n",
    "    filas_index.append(\"<html><head><meta charset='utf-8'><title>Reporte unificado</title>\")\n",
    "    filas_index.append(\"<style>body{font-family:Arial,Helvetica,sans-serif;max-width:900px;margin:24px auto;padding:0 12px;}ul{line-height:1.8}</style>\")\n",
    "    filas_index.append(\"</head><body>\")\n",
    "    filas_index.append(\"<h1>Reporte unificado ‚Äî An√°lisis exploratorio</h1>\")\n",
    "    filas_index.append(\"<ul>\")\n",
    "\n",
    "    # CSV de m√©tricas consolidado\n",
    "    ruta_metricas_csv = carpeta_resultados / \"metricas_eda_consolidado.csv\"\n",
    "    with open(ruta_metricas_csv, \"w\", newline=\"\", encoding=\"utf-8\") as fcsv:\n",
    "        escritor = csv.writer(fcsv)\n",
    "        escritor.writerow([\n",
    "            \"dataset\",\"cantidad_muestras\",\"cantidad_atributos\",\"cantidad_clases\",\n",
    "            \"clase_minima_real\",\"clase_mayoritaria_real\",\"n_min\",\"n_max\",\"ratio_desequilibrio_max\",\n",
    "            \"entropia_clases\",\"tamano_efectivo_clases\",\"porcentaje_faltantes\",\"porcentaje_duplicados\",\n",
    "            \"asimetria_mediana\",\"curtosis_mediana\",\"porcentaje_variables_con_normalidad_no_rechazada\",\n",
    "            \"correlacion_absoluta_maxima\",\"porcentaje_pares_correlacion_mayor_0_9\",\n",
    "            \"rango_mediano_variables\",\"rango_maximo_variables\",\"sugerencia_escalado\",\n",
    "            \"pca_pc1\",\"pca_pc1_pc2\",\"pca_pc1_pc2_pc3\"\n",
    "        ])\n",
    "\n",
    "    # Loop datasets\n",
    "    for nombre, cfg in config_datasets.items():\n",
    "        print(f\"\\nüîé Generando reporte: {nombre.upper()}\")\n",
    "        try:\n",
    "            names = cfg.get(\"esquema\") if cfg.get(\"header\", None) is None else None\n",
    "\n",
    "            X, y, _ = cargar_dataset(\n",
    "                path=cfg.get(\"path\"),\n",
    "                clase_minoria=cfg.get(\"clase_minoria\"),\n",
    "                col_features=cfg.get(\"col_features\"),\n",
    "                col_target=cfg.get(\"col_target\"),\n",
    "                sep=cfg.get(\"sep\", \",\"),\n",
    "                header=cfg.get(\"header\", None),\n",
    "                binarizar=False,\n",
    "                tipo=cfg.get(\"tipo\", \"tabular\"),\n",
    "                impute=\"median\",\n",
    "                names=names\n",
    "            )\n",
    "\n",
    "            # DataFrames seguros\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                X_df = X.copy()\n",
    "            else:\n",
    "                columnas_genericas = []\n",
    "                indice_col = 0\n",
    "                while indice_col < X.shape[1]:\n",
    "                    columnas_genericas.append(f\"col_{indice_col}\")\n",
    "                    indice_col += 1\n",
    "                X_df = pd.DataFrame(X, columns=columnas_genericas)\n",
    "            y_series = pd.Series(y)\n",
    "\n",
    "            # Resumen de clases para tabla HTML\n",
    "            conteo = pd.Series(y).value_counts()\n",
    "            total = int(conteo.sum())\n",
    "            resumen_clases = []\n",
    "            for clase, n in conteo.items():\n",
    "                porcentaje = float((n / max(1,total)) * 100.0)\n",
    "                resumen_clases.append((clase, int(n), porcentaje))\n",
    "\n",
    "            # M√©tricas\n",
    "            metricas = calcular_metricas_basicas_dataframe(X_df, y_series)\n",
    "\n",
    "            # PCA\n",
    "            var_pca = calcular_varianza_explicada_pca(X_df, k=5)\n",
    "            pc1 = float(var_pca[0]) if len(var_pca) > 0 else 0.0\n",
    "            pc12 = float(sum(var_pca[:2])) if len(var_pca) >= 2 else pc1\n",
    "            pc123 = float(sum(var_pca[:3])) if len(var_pca) >= 3 else pc12\n",
    "\n",
    "            # Guardar en CSV consolidado\n",
    "            with open(ruta_metricas_csv, \"a\", newline=\"\", encoding=\"utf-8\") as fcsv:\n",
    "                escritor = csv.writer(fcsv)\n",
    "                escritor.writerow([\n",
    "                    nombre,\n",
    "                    metricas[\"cantidad_muestras\"],\n",
    "                    metricas[\"cantidad_atributos\"],\n",
    "                    metricas[\"cantidad_clases\"],\n",
    "                    metricas[\"clase_minima_real\"],\n",
    "                    metricas[\"clase_mayoritaria_real\"],\n",
    "                    metricas[\"n_min\"],\n",
    "                    metricas[\"n_max\"],\n",
    "                    round(metricas[\"ratio_desequilibrio_max\"], 6) if math.isfinite(metricas[\"ratio_desequilibrio_max\"]) else \"inf\",\n",
    "                    round(metricas[\"entropia_clases\"], 6),\n",
    "                    round(metricas[\"tamano_efectivo_clases\"], 6),\n",
    "                    round(metricas[\"porcentaje_faltantes\"], 6),\n",
    "                    round(metricas[\"porcentaje_duplicados\"], 6),\n",
    "                    round(metricas[\"asimetria_mediana\"], 6),\n",
    "                    round(metricas[\"curtosis_mediana\"], 6),\n",
    "                    round(metricas[\"porcentaje_variables_con_normalidad_no_rechazada\"], 6),\n",
    "                    round(metricas[\"correlacion_absoluta_maxima\"], 6),\n",
    "                    round(metricas[\"porcentaje_pares_correlacion_mayor_0_9\"], 6),\n",
    "                    round(metricas[\"rango_mediano_variables\"], 6),\n",
    "                    round(metricas[\"rango_maximo_variables\"], 6),\n",
    "                    metricas[\"sugerencia_escalado\"],\n",
    "                    round(pc1, 6),\n",
    "                    round(pc12, 6),\n",
    "                    round(pc123, 6)\n",
    "                ])\n",
    "\n",
    "            # Carpetas y figuras\n",
    "            carpeta_dataset = carpeta_figuras / nombre.lower()\n",
    "            carpeta_dataset.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "            ruta_fig_clases = os.path.join(str(carpeta_dataset), f\"{nombre.lower()}_clases.png\")\n",
    "            graficar_distribucion_clases(y, nombre_dataset=nombre, guardar_en=ruta_fig_clases)\n",
    "\n",
    "            graficar_histograma_variables(X_df, nombre, str(carpeta_dataset), max_columnas=12)\n",
    "            graficar_boxplot_variables(X_df, nombre, str(carpeta_dataset), max_columnas=12)\n",
    "            graficar_mapa_correlaciones(X_df, nombre, str(carpeta_dataset))\n",
    "\n",
    "            # Recolectar rutas de im√°genes para el HTML\n",
    "            rutas_figuras = []\n",
    "            rutas_figuras.append((\"Distribuci√≥n de clases\", ruta_fig_clases))\n",
    "\n",
    "            # Agregar hist y box generados (primeras 12 num√©ricas)\n",
    "            columnas_numericas = [c for c in X_df.columns if es_numerica(X_df[c])]\n",
    "            limite = min(12, len(columnas_numericas))\n",
    "            indice = 0\n",
    "            while indice < limite:\n",
    "                col = columnas_numericas[indice]\n",
    "                rutas_figuras.append((f\"Histograma: {col}\", os.path.join(str(carpeta_dataset), f\"{nombre}_hist_{col}.png\")))\n",
    "                rutas_figuras.append((f\"Boxplot: {col}\", os.path.join(str(carpeta_dataset), f\"{nombre}_box_{col}.png\")))\n",
    "                indice += 1\n",
    "\n",
    "            # Correlaciones (si hubo)\n",
    "            if len(columnas_numericas) >= 2:\n",
    "                rutas_figuras.append((\"Matriz de correlaciones\", os.path.join(str(carpeta_dataset), f\"{nombre}_correlaciones.png\")))\n",
    "\n",
    "            # HTML por dataset\n",
    "            ruta_html_dataset = carpeta_resultados / f\"reporte_{nombre.lower()}.html\"\n",
    "            generar_html_dataset(\n",
    "                nombre_dataset=nombre,\n",
    "                resumen_clases=resumen_clases,\n",
    "                metricas=metricas,\n",
    "                varianza_pca=var_pca,\n",
    "                rutas_figuras=rutas_figuras,\n",
    "                ruta_html_salida=str(ruta_html_dataset)\n",
    "            )\n",
    "\n",
    "            # Agregar al √≠ndice\n",
    "            rel = os.path.relpath(str(ruta_html_dataset), start=str(carpeta_resultados))\n",
    "            filas_index.append(f\"<li><a href='{rel}'>{nombre}</a></li>\")\n",
    "            print(f\"   ¬∑ OK -> {ruta_html_dataset}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ¬∑ ERROR en {nombre}: {e}\")\n",
    "\n",
    "    filas_index.append(\"</ul>\")\n",
    "    filas_index.append(\"<hr/><p>Carpeta de figuras: <code>\" + str(carpeta_figuras) + \"</code></p>\")\n",
    "    filas_index.append(\"</body></html>\")\n",
    "    with open(ruta_index, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(filas_index))\n",
    "\n",
    "    print(\"\\n‚úÖ Reporte unificado generado.\")\n",
    "    print(\"   - √çndice:\", ruta_index)\n",
    "    print(\"   - M√©tricas consolidadas:\", ruta_metricas_csv)\n",
    "    print(\"   - Figuras por dataset en:\", carpeta_figuras)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# EJECUCI√ìN: llama con tus objetos reales\n",
    "# -------------------------------------------------------------------\n",
    "# Requiere que existan en tu notebook:\n",
    "#  - config_datasets\n",
    "#  - cargar_dataset(path, clase_minoria, col_features, col_target, sep, header, binarizar, tipo)\n",
    "#  - graficar_distribucion_clases(y, nombre_dataset, guardar_en)\n",
    "generar_reporte_unificado(config_datasets, cargar_dataset, graficar_distribucion_clases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31abe33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Asumo que ya ten√©s:\n",
    "# - config_datasets\n",
    "# - cargar_dataset(path, clase_minoria, col_features, col_target, sep, header, binarizar, tipo, impute, na_values)\n",
    "# Si tus funciones est√°n en m√≥dulos, importalas antes.\n",
    "\n",
    "def diagnosticar_outliers_shuttle(config_datasets, nombre_dataset=\"shuttle\", ruta_salida=\"diagnosticos\"):\n",
    "    Path(ruta_salida).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    cfg = config_datasets[nombre_dataset]\n",
    "    \n",
    "    names = cfg.get(\"esquema\") if cfg.get(\"header\", None) is None else None\n",
    "\n",
    "    X, y, _ = cargar_dataset(\n",
    "        path=cfg.get(\"path\"),\n",
    "        clase_minoria=cfg.get(\"clase_minoria\"),\n",
    "        col_features=cfg.get(\"col_features\"),\n",
    "        col_target=cfg.get(\"col_target\"),\n",
    "        sep=cfg.get(\"sep\", \",\"),\n",
    "        header=cfg.get(\"header\", None),\n",
    "        binarizar=False,\n",
    "        tipo=cfg.get(\"tipo\", \"tabular\"),\n",
    "        impute=\"median\",\n",
    "        names=names\n",
    "    )\n",
    "\n",
    "    # X viene como ndarray (seg√∫n tu cargar_dataset) -> armo DataFrame con nombres\n",
    "    nombres_columnas = cfg.get(\"col_features\")\n",
    "    df = pd.DataFrame(X, columns=nombres_columnas)\n",
    "    serie_clase = pd.Series(y, name=\"clase\")\n",
    "\n",
    "    # === 1) Resumen univariado: skew, kurtosis, % outliers por IQR ===\n",
    "    lista_filas_resumen = []\n",
    "    for nombre_columna in nombres_columnas:\n",
    "        serie = df[nombre_columna].astype(float)\n",
    "\n",
    "        q1 = float(np.percentile(serie, 25))\n",
    "        q3 = float(np.percentile(serie, 75))\n",
    "        iqr = q3 - q1\n",
    "        limite_inferior = q1 - 1.5 * iqr\n",
    "        limite_superior = q3 + 1.5 * iqr\n",
    "\n",
    "        cantidad_outliers = int(((serie < limite_inferior) | (serie > limite_superior)).sum())\n",
    "        porcentaje_outliers = 100.0 * cantidad_outliers / len(serie)\n",
    "\n",
    "        asimetria = float(pd.Series(serie).skew())\n",
    "        curtosis = float(pd.Series(serie).kurtosis())\n",
    "\n",
    "        fila = {\n",
    "            \"variable\": nombre_columna,\n",
    "            \"asimetria\": asimetria,\n",
    "            \"curtosis\": curtosis,\n",
    "            \"q1\": q1,\n",
    "            \"q3\": q3,\n",
    "            \"iqr\": iqr,\n",
    "            \"limite_inferior\": limite_inferior,\n",
    "            \"limite_superior\": limite_superior,\n",
    "            \"cantidad_outliers_IQR\": cantidad_outliers,\n",
    "            \"porcentaje_outliers_IQR\": porcentaje_outliers\n",
    "        }\n",
    "        lista_filas_resumen.append(fila)\n",
    "\n",
    "    resumen_univariado = pd.DataFrame(lista_filas_resumen).sort_values(\"porcentaje_outliers_IQR\", ascending=False)\n",
    "    resumen_univariado.to_csv(f\"{ruta_salida}/shuttle_resumen_univariado.csv\", index=False)\n",
    "\n",
    "    # === 2) Gr√°ficos univariados (histograma + boxplot) por variable ===\n",
    "    for nombre_columna in nombres_columnas:\n",
    "        serie = df[nombre_columna].astype(float)\n",
    "\n",
    "        plt.figure(figsize=(8, 3))\n",
    "        plt.hist(serie, bins=50)\n",
    "        plt.title(f\"Histograma - {nombre_columna}\")\n",
    "        plt.xlabel(nombre_columna)\n",
    "        plt.ylabel(\"Frecuencia\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{ruta_salida}/shuttle_hist_{nombre_columna}.png\", dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        plt.boxplot(serie, vert=True, showfliers=True)\n",
    "        plt.title(f\"Boxplot - {nombre_columna}\")\n",
    "        plt.ylabel(nombre_columna)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{ruta_salida}/shuttle_box_{nombre_columna}.png\", dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "    # === 3) Outliers por clase usando IQR (no borro, solo mido) ===\n",
    "    lista_resumen_por_clase = []\n",
    "    clases_unicas = np.unique(serie_clase.values)\n",
    "    for clase_actual in clases_unicas:\n",
    "        indice_clase = (serie_clase.values == clase_actual)\n",
    "        subdf = df.loc[indice_clase]\n",
    "\n",
    "        # porcentaje promedio de outliers IQR (promedio sobre columnas)\n",
    "        porcentaje_por_variable = []\n",
    "        for nombre_columna in nombres_columnas:\n",
    "            serie = subdf[nombre_columna].astype(float)\n",
    "            if len(serie) == 0:\n",
    "                continue\n",
    "            q1 = float(np.percentile(serie, 25))\n",
    "            q3 = float(np.percentile(serie, 75))\n",
    "            iqr = q3 - q1\n",
    "            limite_inferior = q1 - 1.5 * iqr\n",
    "            limite_superior = q3 + 1.5 * iqr\n",
    "            cant = int(((serie < limite_inferior) | (serie > limite_superior)).sum())\n",
    "            porcentaje = 100.0 * cant / len(serie)\n",
    "            porcentaje_por_variable.append(porcentaje)\n",
    "        promedio_porcentaje = float(np.mean(porcentaje_por_variable)) if len(porcentaje_por_variable) > 0 else 0.0\n",
    "\n",
    "        lista_resumen_por_clase.append({\n",
    "            \"clase\": clase_actual,\n",
    "            \"muestras_clase\": int(indice_clase.sum()),\n",
    "            \"promedio_%_outliers_IQR\": promedio_porcentaje\n",
    "        })\n",
    "\n",
    "    resumen_por_clase = pd.DataFrame(lista_resumen_por_clase).sort_values(\"muestras_clase\", ascending=False)\n",
    "    resumen_por_clase.to_csv(f\"{ruta_salida}/shuttle_outliers_por_clase_IQR.csv\", index=False)\n",
    "\n",
    "    # === 4) IsolationForest por clase (solo para medir en la mayoritaria, no para borrar) ===\n",
    "    # Nota: no uses este resultado para eliminar en clases minoritarias.\n",
    "    if len(clases_unicas) > 0:\n",
    "        # identifico clase mayoritaria\n",
    "        conteo = pd.Series(serie_clase).value_counts()\n",
    "        clase_mayoritaria = conteo.idxmax()\n",
    "        indice_mayoritaria = (serie_clase.values == clase_mayoritaria)\n",
    "        X_may = df.loc[indice_mayoritaria].values.astype(float)\n",
    "\n",
    "        if X_may.shape[0] > 100:\n",
    "            modelo_iso = IsolationForest(\n",
    "                n_estimators=200,\n",
    "                contamination=0.002,  # umbral MUY estricto: ~0.2%\n",
    "                random_state=42\n",
    "            )\n",
    "            etiquetas = modelo_iso.fit_predict(X_may)\n",
    "            porcentaje_anomalias = 100.0 * (etiquetas == -1).sum() / len(etiquetas)\n",
    "\n",
    "            with open(f\"{ruta_salida}/shuttle_isolationforest_mayoritaria.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(f\"Clase mayoritaria: {clase_mayoritaria}\\n\")\n",
    "                f.write(f\"Muestras en clase mayoritaria: {len(etiquetas)}\\n\")\n",
    "                f.write(f\"Porcentaje marcado como anomal√≠a (contamination=0.002): {porcentaje_anomalias:.4f}%\\n\")\n",
    "\n",
    "    # === 5) Recomendaci√≥n final en texto ===\n",
    "    with open(f\"{ruta_salida}/shuttle_recomendacion.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"Recomendaci√≥n Shuttle:\\n\")\n",
    "        f.write(\"- No eliminar outliers: los extremos reflejan transiciones reales del sistema.\\n\")\n",
    "        f.write(\"- Si el modelo lo requiere, usar RobustScaler o winsorizaci√≥n leve (p0.1‚Äìp99.9) y validar.\\n\")\n",
    "        f.write(\"- Evitar IsolationForest global. Si se limpia, hacerlo solo en clase mayoritaria y evaluar impacto.\\n\")\n",
    "\n",
    "    return resumen_univariado, resumen_por_clase\n",
    "\n",
    "# Ejecutar diagn√≥stico\n",
    "resumen_univariado, resumen_por_clase = diagnosticar_outliers_shuttle(config_datasets, \"shuttle\", \"diagnosticos\")\n",
    "resumen_univariado.head(), resumen_por_clase\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
